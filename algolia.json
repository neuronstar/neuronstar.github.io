[{"categories":null,"contents":" one of the most important concepts in all of probability theory — that of conditional probability.\n\u0026ndash; Sheldon M. Ross\n Topics  Association Rules Conditional Probability Naive Bayes  Association Rules Association Rules\n$$ \\text{Milk} \\Rightarrow \\text{Croissant} [ \\text{support} = 2/5, \\text{confidence} = 2/3 ] $$\n A measure of co-occurrence  Prize in Boxes Three boxes M, N, O:\n Only one of them refers to a prize. The participant claims one of them, e.g., M. The host removes one of the empty boxes (N) from the other two boxes (N, O). Now we have only two candidate boxes for the participants, M, O. The participant is asked to reclaim a box.  Question:\n Should the participant switch?  Frequentist vs Bayesian Frequentists:\n Probability is based on the repetition of events. Without repetition, the probability is unknown. Events are random. Make predictions based on probabilities. Relies on NHST to validate our models.  Bayesian:\n Probability is objective (educated guess). It is a conceptual tool to describe our degree of certainty. Probability is not necessarily a one-to-one map of occurrences of events. Data (such as a previous reoccurring event) is used to update our beliefs. Parameters of models are random.  Joint Probability Joint Probability $$ P(A\\cap B) $$ also denoted as $P(A, B)$\nExamples of joint probabilities:\n $P(A, B)=0$: Event $A$ and event $B$ are so different that they will never happen together. In this case, $P(A\\cup B) = P(A) + P(B)$. $P(A, B) = P(A)P(B)$: $A$ and $B$ are independent of each other. $P(A) + P(B) = 1$.  Conditional Probability and Bayes\u0026rsquo;s Theorem $P(A\\vert B)$: the probability of event $A$ if event $B$ occurred.\n It\u0026rsquo;s a rescaling/(re)normailization of $P(A)$: $P(A\\cap B) = P(A\\vert B)P(B)$; We didn\u0026rsquo;t specify $A$ and $B$: $P(B\\cap A) = P(B\\vert A)P(A)$ also holds; Apply $P(A\\cap B) = P(B \\cap A)$: $P(A\\vert B)P(B) = P(B\\vert A)P(A)$  In $$ \\begin{equation}P(A\\vert B) = \\frac{P(B\\vert A)P(A)}{P(B)}\\end{equation} $$\n $P(A)$: prior $P(A\\vert B)$: posterior $P(B\\vert A)$: likelihood  $A\\to H$ (A theory) \u0026amp; $B\\to D$ (some data points)\n $P(H\\vert D) = \\frac{P(D\\vert H)P(H)}{P(D)}$  Solutions to the Prize in Boxes Problem  $P(\\text{Win}\\vert \\text{Switch}, \\text{Wrong Box}) = 1$: The participant made a wrong choice at the first attempt, then switched. $P(\\text{Win}\\vert \\text{Switch}, \\text{Right Box}) = 0$ $P(\\text{Win}\\vert \\text{NoSwitch}, \\text{Wrong Box})=1$ $P(\\text{Wrong Box}) = 2/3$ $P(\\text{Right Box}) = 1/3$ $P(\\text{Win}\\vert \\text{Switch})$: We will solve this.  Applying Bayes\u0026rsquo; theorem,\n$$ \\begin{align} P(\\text{Win}\\vert \\text{Switch}) =\u0026 P(\\text{Win}\\vert \\text{Switch}, \\text{Wrong Box}) P(\\text{Wrong Box}) + P(\\text{Win}\\vert \\text{Switch}, \\text{Right Box}) P(\\text{Right Box}) \\\\ =\u0026 2/3 \\nonumber \\end{align} $$\n$$ \\begin{align} P(\\text{Win}\\vert \\text{NoSwitch}) =\u0026 P(\\text{Win}\\vert \\text{NoSwitch}, \\text{Wrong Box}) P(\\text{Wrong Box}) + P(\\text{Win}\\vert \\text{NoSwitch}, \\text{Right Box}) P(\\text{Right Box}) \\\\ =\u0026 1/3 \\nonumber \\end{align} $$\nWe update our probability perception when we have new data.\nRare Disease We are about to test for a rare disease:\n Prevalence: $P(D) = 0.001$ Test method: $P(+\\vert D) = 0.99$ Test method: $P(+\\vert H) = 0.005$  The positive rate for a random person is\n$$ P(+) = P(+\\vert D)P(D) + P(+\\vert H) P(H) = 0.006. $$\nIf a test is positive, the probability of the person being test has the disease is\n$$ P(D\\vert +) = \\frac{P(+\\vert D) P(D)}{P(+)} = 0.99\\times 0.001/0.006 = 0.165. $$\n$$ P(H\\vert +) = \\frac{P(+\\vert H) P(H)}{P(+)} = 0.005\\times (1-0.001)/0.006 = 0.8325. $$\nNaive Bayes  Chapter 16 of Introduction to Probability and Statistics for Engineers and Scientists (6th ed.) by Sheldon Ross. Naive Bayes  ","permalink":"/cpe/01.conditional-probability-and-bayes/","tags":["Bayesian","Bayes's Theorem","Conditional Probability"],"title":"Conditional Probability and Bayes"},{"categories":null,"contents":"Neuron biological properties Dendrites receive neurotransmitter and generate local post-synaptic potential (PSP), either excitatory by glutamate or inhibitory by GABA. Soma integrates all PSPs. Once membrane potential reaches the threshold, an action potential is generated. Action potentials are propagated via axon.\nSpike Response Model (SRM) In this model, membrane potential is denoted as u(t). When the neuron is at rest state, receiving no external stimulation, the membrane potential is called rest potential.\n$$u(t_0)=u_{\\mathrm{rest}}.$$\nWhen a presynaptic spike arrives, there is a local PSP, causing a small disturbance in membrane potential.\n$$u(t)-u_{\\mathrm{rest}} = e_{ij}.$$\nThroughout the notes, $i$ is always indicating the neuron we are measuring the potential, while $j$ is for the incoming signal.\nAccording to simplification rule, all PSPs are integrated linearly.\n$$u_i(t) =u_{\\mathrm{rest}}+ \\sum_{j} \\sum_f \\epsilon_{ij}(t - t_j^{(f)}) $$\nin which $(f)$ shows the $j$ neurons can fire multiple times.\nHowever, when an action potential is generated, there is a dramatic membrane potential change (depolarization), and due to the electrophysiological property of channels, hyperpolarization phenomenon is observed. So the complete but simple presentation of membrane potential is\n$$u_i(t) =u_{\\mathrm{rest}}+ \\sum_{j} \\sum_f \\epsilon_{ij}(t - t_j^{(f)}) + \\eta(t-\\hat t_i),$$\nwhere $\\hat t_i$ is the moment when the action potential is triggered.\nSpike train When simulating a neural network, we don\u0026rsquo;t care about the exact trajectory of membrane potential. so Dirac function is used to present an action potential. This provides the first step to computation, using firing times to present the spike train of a neuron.\nLimitation of SRM SRM cares about merely the most recent spike, however in real case, previous spikes could influence the reaction of the neuron to a later simulation. (for example, by changing membrane conductivity)\n","permalink":"/snm/01.single_neuron_model/","tags":null,"title":"01.Neuron Biological Properties"},{"categories":null,"contents":"Topics  Least squares Bootstrap Maximum Likelihood Maximum posterior Bayesian inference    ","permalink":"/cpe/02.least-squares-bootstrap-maximum-likelihood-and-bayesian/","tags":["Bayesian","Least Squares","Bootstrap","Maximum Likelihood","Bayesian"],"title":"Least Squares, Bootstrap, Maximum Likelihood, and Bayesian"},{"categories":null,"contents":"Review of Last Week\u0026rsquo;s Reading Keywords:\n Spiking train: a chain of action potentials (P3) Absolute refractory period: minimal distance between two spikes synapses: chemical, electrical (gap junctions) excitatory and inhibitory: change of potential due to an arrival spike, positive-\u0026gt; excitatory, negative -\u0026gt; inhibitory postsynaptic potential: voltage response  PSP: postsynaptic potential IPSP/EPSP: I-\u0026gt; Inhibitory, E-\u0026gt;Excitatory   depolarize/hyperpolarize: increase potential-\u0026gt;depolarize, decrease potential-\u0026gt;hyperpolarize $\\text{SRM}_0$: Each response to the incoming spikes are linearly summed up until a spike is triggered. (P7, Equation 1.3)  $$u_i=\\eta(t-\\hat t_i)+ \\sum_j \\sum_f \\epsilon_{ij}(t-t_j^{(f)}) + u_{\\mathrm{rest}} $$   adaptation: fig1.5  regular firing neurons/fast-spiking neurons/bursting neurons: fig 1.3. fast-spiking is what would SRM0 give us rebound spikes: fig 1.3 D    Limitations (cont\u0026rsquo;d) Saturating Excitation and Shunting Inhibition Facts: shape of PSP depends on\n level of potential, internal state of the neuron, like state of ion channels.   The PSP depends on the potential of the neuron itself when the presynaptic spike arrives. Interesting facts:  IPSP, usually leads to hyperpolarization. Amplitude is larger if the neuron has a higher potential when presynaptic spike arrives. However, the response is reversed, i.e., depolarizing, if the potential is already hyperpolarized a lot. Reason for a reversal response is that PSC switch to the other direction if the potential $u_0$ before spike is due to the wrong direction of the PSC.  EPSP is larger given lower potential, i.e., more depolarization.   PSC: postsynaptic current, which is proportional to the actual effective potential (reversal potential) on the neuron membrane. Shunting inhibition: A few inhibitory synapses shunt input of a few hundred excitatory synapses.  Neuronal Coding One of the ways to present neuron spikes is the spatio-temporal patterns of pulses. (Fig 1.8)\n mean firing rate: $\\nu= \\frac{n_{\\mathrm{sp}}(T)}{T}$, i.e., number of spikes during time $T$ divided by the time span $T$. History:  Adrian, 1926, 1928: firing rate of stretch receptor neurons -\u0026gt; forces on muscle   Temporal average: is an approximation by filtering a lot of information out. Objections:  brain activities are fast, too fast to allow coding in temporal average. 400ms for example. evidence of temporal correlations betwen pulses of different neurons. stimulus-dependent synchronization of activity in many neurons    Rate Codes Mean firing rate:\n  temporal averaging (fig 1.9) (Boltzmann?):\n $\\nu=\\frac{\\text{No. of spikes during time }T}{\\text{time }T}$. Usually $T\\sim 100\\mathrm{ms}$ or $T\\sim 500\\mathrm{ms}$. The general principle is to increase the time until no change in the average during a single stimulus. Example: leech touch receptor,stronger the stimulus -\u0026gt; more spikes during a $500\\mathrm{ms}$ average. Working if   mapping of one input variable to a single output result, i.e., $\\mathrm{single output}=f(\\mathrm{single input})$. Example: force on receptors -\u0026gt; average firing rate. stimulus is not fast-varying doesn\u0026rsquo;t require fast reaction   Evidence for other types of coding:   Fly react in 30-40ms Human react to some visual in a $\\sim 10^2$ms Human can detect images even the images was shown for 14-100ms.    averaging over repetitions of experiments (fig 1.10) (partial ensemble average, Gibbs?):\n many runs of experiment + PSTH (fig 1.10) PSTH: peri-stimulus-time histogram, peri means we count the spikes in a time interval $\\Delta t$. Spike density: $\\rho(t) = \\frac{n_K(\\text{from } t \\text{ to }t+\\Delta t)}{K}/\\Delta t$, where $K$ is the number of runs done, while $n_K(\\text{from } t \\text{ to }t+\\Delta t)$ is the sum of all spike of all runs from time $t$ to $t+\\Delta t$. If we have enough runs we could take $\\Delta t\\to 0$ to get the continuous density. Unit of spike density: Hz. Meaning of this average: not really what happens but only ensemble average. It reflects some properties of the pattern but not what is done in the brain. Ensemble average is more or less equivalent to time average if the stimulus is constant. Ensemble average is almost identical to the population average if a lot of identical neurons are firing independently.    averaging over populations of neurons (fig 1.11) (another ensemble average):\n Population activity: $A(t) = \\frac{\\text{No. of spikes during time } t\\sim t+\\Delta t \\text{ of all the }N \\text{ neurons that has the same input}}{N}/\\Delta t$ No. of spikes during time $t\\sim t+\\Delta t$ of all the $N$ neurons that has the same input $n_{act}=\\int_t^{t+\\Delta}dt \\text{ all spike written in delta function}=\\int_t^{t+\\Delta}dt \\sum_j\\sum_f \\delta(t-t_j^{(f)})$. However,   no actual homogeneous identical neurons   Inhomogeneous? Population vector coding. $i$th neuron takes input $x_i$.  Given a vector input, what is the average position (location on the vector index) of all neurons that gives output? $\\mathrm{Average Location of Stimulated Neurons}=\\frac{\\int_t^{t+\\Delta t} dt \\sum_i \\sum_f x_j \\delta(t-t_j^{(f)}) }{\\int_t^{t+\\Delta t} dt \\sum_i \\sum_f \\delta(t-t_j^{(f)}) }$. Used on neuronal activiy in primate motor cortex. \u0026lt;- Not sure how this is done.      Spike Codes  Time-to-first-spike, a Simple Scheme of Spiking Codes: fig 1.12. Three neurons which are responsible for reading a picture of three pixels of three different color for example, will spike sequentially if each of them are responsible for a different color.  Assuming each neuron is inactive after its spike \u0026lt;- due to some mysterious inhibition: this is called time-to-first-spike   Coding by phase (fig 1.13): periodic spikes (common in hippocampus etc), relative phase between neurons (or relative to oscillatioins of background periodic spikes) could carry information.  what is background oscillation? fig 1.14   Example of sychronization and information  Reverse correlation approach: investigate the averaged input instead of averaged PSTH (as we have done in Fig 1.10). So we find what input is required to generate a spike. Given spike train we can estimate the input by summing up the input of each spike! Fig 1.16B. Eqn 1.12.  Relation Between Spike Codes and Rate Codes  Define a rate $\\nu$ given a spike code, $\\nu(t)=\\frac{\\text{effective number of spikes during time T}}{T}=\\int d\\tau K(\\tau)S(t-\\tau)/\\int d\\tau K(\\tau)$, where $K(\\tau)$ is window function which gives us the time interval of measurement, and $S(\\tau)=\\sum^n_{f=1}\\delta(t-t^{(f)})$. So $\\int d\\tau K(\\tau)S(t-\\tau)$ is the number of spikes during the time window given by $K$. How to reconstruct rate code? If eqn 1.12 is true (linear reverse correlation approach is true) -\u0026gt; rate code, otherwise -\u0026gt; spike code.  As an example, use Volterra expansion and drop the information about the reconstructed input correlations between two spikes.    ","permalink":"/snm/02.limitations_srm_contd_and_coding/","tags":null,"title":"02.Review of Last Week's Reading"},{"categories":null,"contents":"Topics  EM for Gaussian mixtures General EM algorithm Why does it work? Decomposition of log-likelihood into KL divergence and Relation between EM and Gibbs sampling  ","permalink":"/cpe/03.em-methods/","tags":["Maximum Likelihood","Bayesian","Sampling","Gaussian Mixture"],"title":"EM Methods"},{"categories":null,"contents":"Equilibrium Potential and Hodgkin-Huxley Model  Equilibrium Potential Nerst Potential For ions with positive charge\n  high potential -\u0026gt; more energy -\u0026gt; low density\n  low potential -\u0026gt; less energy -\u0026gt; high density\n  The reverse,\n  low density -\u0026gt; more energy -\u0026gt; high potential\n  high density -\u0026gt; less energy -\u0026gt; low potential\n  Nerst Potential:\nThe voltage generates by concentration difference.\nReverse Potential In the cell membrane:\n  ion pumps(ions go single direction)\n  ion channels(ions go both direction)\n  Nernst potential $E_{Na}=+50mV$\n  $\\Delta{u}\u0026lt;E_{Na}$ : $Na^{+}$ flow into cell\n  $\\Delta{u}\u0026gt;E_{Na}$ : $Na^{+}$ flow out of cell\n  Rest Potential $u_{rest}\\approx-65mV$\n$E_{K} \u0026lt; u_{rest} \u0026lt; E_{Na}$\nat rest potential:\n  potassium flow out of cell\n  sodium flow into cell\n  ion pumps balance these flows\n  Hodgkin-Huxley Model Definition $C\\frac{du}{dt}=-\\Sigma I_{k}(t)+I(t)$\n$\\Sigma I_{k}=g_{Na}m^{3}h(u-E_{Na})+g_{K}n^{4}(u-E_{k})+g_{L}(u-E_{L})$\n$\\dot{m}=\\alpha_{m}(u)(1-m)-\\beta_{m}(u)m$\n$\\dot{n}=\\alpha_{n}(u)(1-n)-\\beta_{n}(u)n$\n$\\dot{h}=\\alpha_{h}(u)(1-h)-\\beta_{h}(u)h$\nsodium\nm:activate\nh:inactivate\nanother form: $\\dot{x}=-\\frac{1}{\\tau_{x}(u)}[x-x_{0}(u)]$\n$x_{0}(u)=\\frac{\\alpha_{x}(u)}{\\alpha_{x}(u)+\\beta_{x}(u)}$\n$\\tau_{x}(u)=\\frac{1}{\\alpha_{x}(u)+\\beta_{x}(u)}$\nDynamics spike generation external input -\u0026gt;\nmembrane voltage rise -\u0026gt;\nm increase -\u0026gt;\nsodium into cell -\u0026gt;\nmembrane potential rise -\u0026gt;\naction potential\nfig 2.3\n$\\tau(h)\u0026gt;\\tau(m)$\nh:channel close\nm:channel open\nclose is more slowly then open\nthen, potassium sets in-\u0026gt;\nlower potential\nmean firing rates and gain function $I_{0}\u0026gt;I_{\\theta}$spike train\nstep current input $\\Delta{I}$ -\u0026gt; generate single spike $I_{2}$ -\u0026gt; generate repeat spikes\ninhibitory rebound:\n  $I_{2}=0$\n  $\\Delta{I}$ is large enough\n  refractoriness  hyperpolarizing -\u0026gt; needing more stimulation more channel open -\u0026gt; resistance is lower, stimulation decay faster  The Zoo of ion channels Sodium channels $$I_{NaP}=\\bar{g}_{NaP}m(u-E_{Na})$$\ndoes not have h, Noninactivating\nresult: larger depolarization\n","permalink":"/snm/03.equilibrium_potential_and_hodgkin-huxley_model/","tags":null,"title":"03.Equilibrium Potential and Hodgkin-Huxley Model"},{"categories":null,"contents":"Topics  Variational Inference Normalizing Flow Variational Inference with Normalizing Flows  ","permalink":"/cpe/04.variational-inference-normalizing-flow/","tags":["Variational Inference","Normalizing Flow","Bayesian"],"title":"Variantional Inference Normalizing Flow"},{"categories":null,"contents":"The Zoo of ion channels Hodgkin-Huxley model captures the essence of spike generation by sodium and potassium ion channels. But cortical neurons in vertebrates exhibit a much richer repertoire of electrophysiological properties than the squid axon studied by Hodgkin and Huxley. This is mostly due to a large variety of different ion channels.\nSodium channels fast sodium ion channels $I_{Na}$\npersistent or non inactivating sodium current described by an activation variable m only and does not have a separate inactivation variable h -\u0026gt; $I_{NaP}=\\bar{g}_{NaP}m(u-E_{Na})$\nPotassium channels Thalamic relay neurons have two different types of potassium channels: a rapidly inactivating potassium current $I_{A}$ and a slowly inactivating potassium current $I_{K2}$.\n$I_{A}$ -\u0026gt; $I_{A1}$ and $I_{A2}$\n$I_{K2}$ -\u0026gt; $I_{K2a}$ and $I_{K2b}$\n$I_{A}$ -\u0026gt; slowly firing neurons that slow down the firing of action potentials\ntype I: neurons with continuous gain function and delayed pulse generation\ntype II: neurons with discontinuous gain function\nLow-threshold calcium current Neurons of the deep cerebellar nuclei contain two different types of calcium channels: a high-threshold calcium current ($I_{L}$) and a low-threshold calcium current ($I_{T}$)\n$I_{T}$ -\u0026gt; Postinhibitory rebound means that a hyperpolarizing current, which is suddenly switched off, results in an overshoot of the membrane potential or even in the triggering of one or more action potentials. Through this mechanism, action potentials can be triggered by inhibitory input.\nWhat is similar to the fast sodium current of the Hodgkin-Huxley model: $I_{T}$ is inactivating\nWhat is different: in the Hodgkin-Huxley model the sodium channels are not activated (m ≈ 0) whereas the T-type calcium current is inactivated (h ≈ 0)\nHigh-threshold calcium current and calcium-activated potassium channels some neurons contain $I_{T}$ current\nthe inactivation variable h is absent\n","permalink":"/snm/04.the_zoo_of_ion_channels/","tags":null,"title":"04.The Zoo of ion channels"},{"categories":null,"contents":"Topics  Normalizing flow Applications of normalizing flow Methods of normalizing flow Problems of normalizing flow  ","permalink":"/cpe/05.normalizing-flow-review/","tags":["Normalizing Flow"],"title":"Review of Normalizing Flow"},{"categories":null,"contents":"Synapse and receptor What is left from last section: calcium-activated potassium channels, low-threshold and high-threshold calcium-activated potassium channels, are responsible for post-inhibitory-rebound and adaptation respectively.\nCalcium dynamics Calcium concentration varies in different parts of neuron: soma, dendrite and spines, enabling neurotransmitter release, action potential generation and synapse stabilization.\n$$\\mathrm{\\frac{d[Ca^{2+}]i}{dt}}=\\mathrm{-\\tau{Ca}^{-1}[Ca^{2+}]i+\\phi{Ca}I_{Ca}}$$\nSynapse Synapses could be classified as inhibitory synapses (mainly using GABA as neurotransmitter) and excitatory synapses (adopting glutamate as neurotransmitter).\nThe activation of post-synaptic element is realized by binding of neurotransmitter and receptor, ion channel opening and post-synaptic current (or potential) generation, or other intracellular cascade. The amount of current pass through a specific synapse could be described as a function of synaptic conductance $\\mathrm{g_{syn}}$ and driving potential.\n$$\\mathrm{I_{syn}}(t)=\\mathrm{g_{syn}}(t)(u-\\mathrm{E_{syn})}$$\nThe conductance of synapse is an exponential decay function:\n$$\\mathrm{g_{syn}}(t)=\\sum_f\\bar{g}_\\mathrm{syn}e^{-(t-t^f)/\\tau}\\Theta(t-t^f).$$\nfor inhibitory synapse, which has two types of current components:\n$$\\mathrm{g_{syn}}(t)=\\sum_f(\\bar{g}_\\mathrm{fast}e^{-(t-t^f)/\\tau_\\mathrm{fast})}+\\bar{g}_\\mathrm{slow}e^{-(t-t^f)/\\tau_\\mathrm{slow})})\\Theta(t-t^f).$$\nfor excitatory synapse, which adapts two types of receptors: NMDA (slow) and AMPA (fast):\n$$\\mathrm{g_{AMPS}}(t)=\\bar{g}_\\mathrm{AMPA} N [e^{-(t-t^f)/\\tau_\\mathrm{decay}}-e^{-(t-t^f)/\\tau_\\mathrm{rise}}]\\Theta(t-t^f).$$\n$$\\mathrm{g_{NMDA}}(t)=\\bar{g}_\\mathrm{NMDA} N [e^{-(t-t^f)/\\tau_\\mathrm{decay}}-e^{-(t-t^f)/\\tau_\\mathrm{rise}}]g_\\mathrm{\\infty}\\Theta(t-t^f),$$ with $g_{\\infty} = (1+e^{\\alpha u}[\\mathrm{Mg}^{2+}]_o/\\beta)^{-1}.$\nReceptors See this reference for differences between AMPA and NMDA receptor. http://www.sumanasinc.com/webcontent/animations/content/receptors.html\nSpatial structure of dendritic tree Cable equations based on Kirchhoff\u0026rsquo;s law. See Green\u0026rsquo;s function solving the stationary solution of cable equations in next section.\n","permalink":"/snm/05.synapse_and_receptors/","tags":null,"title":"05.Synapse and receptor"},{"categories":null,"contents":"Topics Refer to references.\nNotes 1310.8499_notes.pdf\n","permalink":"/cpe/06.deep-autoregressive-networks/","tags":["AutoRegressive"],"title":"Deep AutoRegressive Networks"},{"categories":null,"contents":"Cable Equation and Its Solutions Checkout the attachment: 06.1.cable_equation_green_function.pdf\nStationary Equation The stationary equation is \\begin{equation} \\left(\\frac{d^2}{dx^2} -1\\right) u(x) = - i_e(x). \\end{equation}\nThe form solution using Green\u0026rsquo;s function method is \\begin{equation} u(x) = \\int G(x,x\u0026rsquo;) (-i_e(t,x\u0026rsquo;) ) dx\u0026rsquo;, \\end{equation} where \\begin{equation*} G(x,x\u0026rsquo;) = \\begin{cases} \\frac{1}{2}e^{x-x\u0026rsquo;}, \u0026amp; \\qquad x\u0026lt;x\u0026rsquo;;\\\n\\frac{1}{2}e^{x\u0026rsquo; - x}, \u0026amp; \\qquad x\u0026gt;x\u0026rsquo;. \\end{cases} \\end{equation*}\nSo far we have been dealing with math. What is the actual meaning of GF? To dive into this question we need to review the equation for GF, in this case, \\begin{equation*} \\left(\\frac{d^2}{dx^2} -1\\right) u(x) = \\delta(x\u0026rsquo;-x). \\end{equation*}\nIn a stimulation-response system, one of the most important properties is the resonance width, or reaction width, which means the deviation required for the amplitude to drop to $1/e$ of the peak value. In this stationary solution, the distance is $1$ in renormalized unit. To transform back to to SI unit, recall that the characteristic length is this problem is $\\lambda = \\sqrt{\\frac{r_T}{r_L}}$.\nJust to build a picture, this length is around\\footnote{Since opening of ion channesl can significantly change the transverse conductivity, this estimation can change significantly in different situations.}\n\\begin{equation*} \\lambda = \\sqrt{ \\frac{r_T}{r_L}} = \\sqrt{ \\frac{30\\mathrm{k\\Omega\\cdot cm^2}/(2\\pi \\rho)}{ 100 \\mathrm{k\\Omega\\cdot cm}/(2\\pi \\rho) } } = \\sqrt{ \\frac{5\\times 10^{11} \\mathrm{\\Omega \\cdot \\mu m} }{ 3\\times 10^{5} \\mathrm{\\Omega \\cdot \\mu m^{-1}} } } = 1.2\\mathrm{mm} \\end{equation*}\nTime-dependent Source To include the time-dependent source we need a two dimensional Dirac delta distribution, \\begin{equation} \\frac{\\partial}{\\partial t} G(t,t\u0026rsquo;;x,x\u0026rsquo;) - \\frac{\\partial^2}{\\partial x^2} G(t,t\u0026rsquo;;x,x\u0026rsquo;) + G(t,t\u0026rsquo;;x,x\u0026rsquo;) = \\delta(t\u0026rsquo;-t)\\delta(x\u0026rsquo;-x). \\end{equation}\nThe Green\u0026rsquo;s function is solved out \\begin{equation} G(t,0;x,0) = \\frac{\\Theta(t)}{\\sqrt{4\\pi t}} \\exp\\left( -t - \\frac{x^2}{4t} \\right). \\end{equation}\nFinite Cable The boundary condition is given by \\begin{equation} i(t,x=0) = i(t,x=L) = 0. \\end{equation}\nNon-linear Cable Equation The current for the ion channels is not simply proportional to the membrane potential which we used for the previous cable equations. Introducing such a time dependent conductivity for the transverse current will significantly increase the complexity of the equations.\n","permalink":"/snm/06.cable_equation_and_its_solutions/","tags":null,"title":"06.Cable Equation and Its Solutions"},{"categories":null,"contents":"Topics Refer to references.\nNotes 1310.8499_notes.pdf\n","permalink":"/cpe/07.made/","tags":["VAE","Flow"],"title":"MADE: Masked Autoencoder for Distribution Estimation"},{"categories":null,"contents":"Two dimensional neuron models Reduction to two dimensions General approach  Origin model: $\\Sigma I_{k}=g_{Na}m^{3}h(u-E_{Na})+g_{K}n^{4}(u-E_{k})+g_{L}(u-E_{L})$  It has 4arguments $m$ $h$ $n$ $u$\n In this chapter,we reduce it to 2 arguments: $u$ , $\\omega$ $C\\frac{du}{dt}=-g_{Na}[m_0(u)]^{3}(b-\\omega)(u-E_{Na})-g_{k}(\\frac{\\omega}{a})^{4}(u-E_{k})-g_{L}(u-E_{L})+I$  $\\omega = b - h = an$\n What changes? because of m changes fast: $m(u,t)$ -\u0026gt; $m_{0}(u)$ because of h and n seems to have linear relationship: $h$ -\u0026gt; $b-\\omega$ $n$ -\u0026gt; $\\frac{\\omega}{a}$  Morris–Lecar model It seems that this model just change exponents of arguments to get a linear equation. This section also give a approximate equation for $m_{0}(u)$and $\\omega_{0}(u)$\nFitzHugh–Nagumo model Phase plane analysis On phase plane, point $(u(t),\\omega(t))$ $\\Delta{u}=\\dot{u}\\Delta{t}$ $\\Delta{\\omega}=\\dot{\\omega}\\Delta{t}$ Use vector $(\\dot{u},\\dot{\\omega})$plot a vector field on phase plane: For each point on the plane， we draw an arrow, the lenght of the arrow is proportional to the length of the vector, and the direction is same as the vection\nNullclines  $u$-nullcline: -points with $\\dot{u} = 0$. -The direction of flow on the u-nullcline is in direction of -$(0,\\dot{\\omega})$ -vertical $\\omega$-nullcline: -points with $\\dot{\\omega} = 0$ -$(0,\\dot{u})$ -horizontal fix-point: -intersection of $u$-nullcline and $\\omega$-nullcline -on nullclines the direction of arrows change at fix points  Stability of fixed points   At fix point\n$$\\frac{d}{dt}\\vec{x}=\\left(\\begin{array}{cc} F_{u} \u0026amp; F_{\\omega} \\ G_{u} \u0026amp; G_{\\omega} \\end{array} \\right)\\vec{x}$$\n  Set $x(t) =\\vec{e}e^{λt}$,we get eigenfunction: $$\\lambda\\vec{x}=\\left(\\begin{array}{cc}F_{u}\u0026amp;F_{\\omega}\\ G_{u}\u0026amp;G_{\\omega}\\end{array}\\right)\\vec{x}$$\n$\\lambda_{1}+\\lambda_{2} = F_{u}+G_{\\omega}$\n$\\lambda_{1}\\lambda_{2} = F_{u}G_{\\omega}-F_{\\omega}G_{u}$\n  Saddle point:\n$\\lambda_{1}\u0026gt;0$\n$\\lambda_{2}\u0026lt;0$\n  Stable points:\n$\\lambda_{1}\u0026lt;0$\n$\\lambda_{2}\u0026lt;0$\n  Unstable points:\n$\\lambda_{1}\u0026gt;0$\n$\\lambda_{2}\u0026gt;0$\n  ","permalink":"/snm/07.reduction_to_two_dimensions_and_phase_plane_analysis/","tags":null,"title":"07.Two dimensional neuron models"},{"categories":null,"contents":"We discussed MAF (arXiv:1705.07057v4) last time: The paper did not explain how exactly is MADE being used to update the shift and logscale.\nWe will use the tensorflow implementation of MAF to probe the above question. Here is the link to the relevant documentation: https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow\nTopics Refer to references.\nNotes 1310.8499_notes.pdf\n","permalink":"/cpe/08.maf/","tags":["VAE","Flow"],"title":"MAF: how is MADE being used"},{"categories":null,"contents":"Integrate-and-fire Models Integrate-and-fire Model We call this integrate and fire because we assume that the presynaptic current will come in and superpose on each other and push the membrane potential to threshold, as then the neuron will fire.\nThis is explicitly demonstrated in the equations. We have a current as total current $$I$$.\nMost simple integrate-and-fire model is a soma constructed from a resistance and capacitor,\n$$ R C\\frac{du}{dt} = - u + R I(t). $$\nTo generalize it we can introduce non-linear forms\n$$ \\frac{du}{dt} = F(u) + G(u)I(t). $$\nLeaky Integrate-and-fire Model Nonlinear Integrate-and-fire Model The change of membrane potential doesn\u0026rsquo;t depend on the membrane potentials linearly anymore.\n$$ \\tau \\frac{d u}{dt} = F(u) + G(u) I. $$\nAs an example, //quadratic// model by Latham et al, 2000, Feng, 2001, Hansel and Mato, 2001, use a quadratic dependence\n$$ \\tau \\frac{du}{dt} = a_0 (u - u_{rest})(u - u_c) + I R. $$\nSpike Response Mdoel Just a nonlinear integrate-and-fire model.\n$$ u_i(t) = \\eta(t-\\hat t_i) + \\sum_j w_{ij} \\sum_j \\epsilon_{ij} (t-\\hat t_i, t-\\hat t_j^{(f)}) + \\int_0^\\infty \\kappa(t-\\hat t_i,s) I^{ext}(t-s) ds, $$\nwhere $$\\eta(t-\\hat t_i)$$ is the evolution of action potential for a firing, while $$\\epsilon_{ij} (t-\\hat t_i, t-\\hat t_j^{(f)})$$ is the response of neuron i with stimulation from neuron j, $$w_{ij}$$ is called efficacy.\nThere are many interesting properties about SRM model.\n threshold depends on $$t-\\hat t_i$$;  The biological interpretation of the //response kernels// ($$\\eta$$, $$\\epsilon$$, and $$\\kappa$$) are explained in Gerstner\u0026rsquo;s book, section 4.2.1.\nHere are the important questions to ask.\n What are the interpretation of kernels? How to model refractoriness using SRM? (In general, by tuning the three kernels)  FitzHugh-Nagumo model (refractoriness) How exactly does hyperpolarization help us to understand refractoriness?\nRelation to Leaky Integrate-and-fire Model SRM0 Removing some of the possible refractoriness from kernels $$\\kappa$$ and $$\\epsilon$$.\n","permalink":"/snm/08.integrate-and-fire-models-1/","tags":null,"title":"08.Integrate and Fire Models Part 1"},{"categories":null,"contents":"","permalink":"/cpe/09.summary-of-generative-models/","tags":["VAE","Flow"],"title":"Summary of Generative Models"},{"categories":null,"contents":"Reduction of the Hodgkin-Huxley model type II Another way of approximation, compare to two phase analysis\nReduction  Hodgkin and Huxley model:  $$C\\frac{du}{dt}=-\\Sigma I_{k}(t)+I(t)$$\n$$\\Sigma I_{k}=g_{Na}m^{3}h(u-E_{Na})+g_{K}n^{4}(u-E_{k})+g_{L}(u-E_{L})$$\n$$\\dot{m}=\\alpha_{m}(u)(1-m)-\\beta_{m}(u)m$$\n$$\\dot{n}=\\alpha_{n}(u)(1-n)-\\beta_{n}(u)n$$\n$$\\dot{h}=\\alpha_{h}(u)(1-h)-\\beta_{h}(u)h$$\n  SRM: $ u(t) = \\eta(t-\\hat t) + \\int_0^{t-\\hat{t}} \\kappa(t-\\hat t_i,s) I^{ext}(t-s) ds+u_{rest} $ we need to define $\\eta(t-\\hat{t})$, $\\kappa(t-\\hat{t})$, $\\vartheta$\n  $\\eta(t-\\hat{t})$ action potential is stereotyped when triggered the spike In Hodgkin-Huxley model, let: $$I(t)=c\\frac{q_0}{\\Delta}\\Theta(t)\\Theta(\\Delta-t)$$ we can get $u(t)$, then use $u(t)$ to get $\\eta(t-\\hat{t})$ $$\\eta(t-\\hat(t))=[u(t)-u_{rest}]\\Theta(t-\\hat{t})$$\n  $\\kappa(t-\\hat{t})$ weak input current, slight perturbed Input: strong plus at $\\hat{t}$, weak plus at $t$, $(t\u0026gt;\\hat{t})$ $$\\kappa(t-\\hat{t},t)=\\frac{1}{c}[u(t)-\\eta(t-\\hat{t})-u_{rest}]$$\n  $\\vartheta$ threshold for spike fixed use different value in different cases\n  Scenarios time-dependent input the metrics: $$\\Gamma=\\frac{1}{C}\\frac{N_{coinc}-{\\langle}{N_{coinc}}{\\rangle}}{\\frac{1}{2}(N_{SRM}+N_{full})}$$ $\\langle{N_{coinc}\\rangle}=2\\nu\\Delta{N_{full}}$ $C=1-2\\nu\\Delta$ if Possison process: $$\\Gamma=0$$ if two model fit perfect: $$\\Gamma=1$$ if $\\kappa$ does not depend on last firing time, $\\Gamma$ will be lower (lower accuracy)\nconstant input different $\\vartheta$ make big differences\nstep current input same three zones also show inhibitory rebound\nspike input use $\\epsilon $ to substitute external input: $u_i(t)=\\eta(t-\\hat{t_i})+\\sum\\limits_{j}w_{ij}\\sum\\limits_{f}\\epsilon(t-\\hat{t_i},t-t_{j}^{(f)})+u_{rest}$\nReduction of a cortical neuron  type I SRM can also be used as a quantitative model of cortical neurons. cortical neurons has continuous gain function\nReduction to a nonlinear integrate-and-fire model Reduction $$C\\frac{du}{dt}=-\\sum{I_{k}(t)}+I(t)$$\n$$\\sum{I_{k}}=g_{Na}m^{3}h(u-E_{Na})+g_{K_{slow}}n^{4}_{slow}(u-E_{K})+g_{K_{fast}}n^2_{fast}(u-E_{K})$$\nfirst step define:\n $\\vartheta$ $\\Delta_{abs}$ $u_{r}$ $m_{r}$ $h_{r}$ $n_{slow}$ $n_{fast}$  we get multi integrate and fire model\nsecond step  fast variables: replace with steady state values (function of u) slow variables: replace with constant $m \\rightarrow m(u)$ $n_{fast} \\rightarrow n_{0,fast}$ $n_{slow} \\rightarrow n_{slow, average}$ $h \\rightarrow h_{average}$  we get nonlinear integrate and fire model\nScenarios constant input fluctuating input Reduction to SRM Reduction aim: find $\\eta$, $\\kappa$, $\\vartheta$\nfirst step reduce the model to and integrate-and-fire model with spike-time-dependent time constant\nsecond step integrate the model, get $\\eta$ and $\\kappa$\nthird step choose appropriate spike-time-dependent threshold $\\vartheta$\nScenarios constant input better with dynamic threshold\nfluctuating input the accuracy is more stable than nonlinear integrate-and-fire model\nLimitations  even $\\Gamma$ of the multi-current integrate-and-fire model is far below 1 time-dependent threshold of SRM is import to achieve generalize over a broad range of different inputs time-dependent threshold seems to be more important for the random-input task than the nonlinearity of function $F(u)$ in the immediate neighborhood of the firing threshold, nonlinear integrate-and-fire model performs better than SRM  ","permalink":"/snm/09.from_detailed_models_to_formal_spiking_neurons/","tags":null,"title":"09.Reduction of the Hodgkin-Huxley model type II"},{"categories":null,"contents":"We will discuss energy-based learning in this session.\nReferences:\n Lecture notes: https://atcold.github.io/pytorch-Deep-Learning/[cid:90ae645c-415b-4c9f-8ea6-c78839a8e8d4] https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/ https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2/ https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view : if you can not access Google Drive, this file (007-ebm-01) has been attached to this calendar event too.  Supplementary:\n https://arxiv.org/pdf/1803.08823.pdf  ","permalink":"/cpe/10.energy-based-learning/","tags":["Hebb's Rule","Hopfield Network","Boltzmann Machine"],"title":"Energy-based Models"},{"categories":null,"contents":"Information coding time to first spike In a $\\mathrm{SRM_0}$ model, N presynaptic neurons target on the postsynaptic neuron. At $t^\\mathrm{pre}$, n presynaptic spikes arrive. $u_i(t)=nw\\epsilon(t-t^\\mathrm{pre})$ $u_i(t)$ is membran potential, $w$ is the constant value of all synaptic weights. When $u_i(t)$ cross threshold $\\theta$, a spike is generated. So the time difference (time to fist spike) codes the strength of inputs.\nphase coding Reference is necessary: periodic background signal Phase codes constant input strength?\ncorrelation coding sharing same inputs or connected.\nnoise in neuron activities facts: in vivo recording revealed noise phenomena in in-tact neuron fast moving stimuli: no noise slowly moving stimuli: changing\nnoise sources  thermal noise number of channels detected multi-inputs from background  Statistics of spike trains To reflect noisy neuron activity, input-dependent renewal process is required. ","permalink":"/snm/10.noise_and_renewal_process/","tags":null,"title":"10.Information Coding"},{"categories":null,"contents":"We will discuss energy-based learning in this session.\nReferences:\n Lecture notes: https://atcold.github.io/pytorch-Deep-Learning/[cid:90ae645c-415b-4c9f-8ea6-c78839a8e8d4] https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/ https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2/ https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view : if you can not access Google Drive, this file (007-ebm-01) has been attached to this calendar event too.  Supplementary:\n https://arxiv.org/pdf/1803.08823.pdf  ","permalink":"/cpe/11.energy-based-learning-2/","tags":["Self-supervised Learning","Energy-based model"],"title":"Energy-based Models 2"},{"categories":null,"contents":"Renewal Theory Review of three key functions:\n $P_I$: probability density of finding spikes. Also called hazard function. Thus $\\int_{\\hat t}^{t_f} P(t\\mid \\hat t)dt$ is the probability of finding spikes during $[\\hat t, t_f]$. $S_I$: survivor function. Defined as $S_I(t\\mid \\hat t) = 1 - \\int_{\\hat t}^t P_I(t\u0026rsquo;\\mid \\hat t) dt'$. The probability of staying quite during $[\\hat t, t]$. $\\rho_I$: rate of decay, defined as $\\rho_I(t\\mid \\hat t) = - \\frac{d}{dt} S_I(t\\mid \\hat t) \\big / S)I(t\\mid \\hat t)$.  Relations between the three: $$ P_I(t\\mid \\hat t) =\\rho_I(t\\mid \\hat t) S_I(t\\mid \\hat t) $$\n$$ S_I(t\\mid \\hat t) = \\exp\\left( - \\int_{\\hat t}^t \\rho_I(t\u0026rsquo;\\mid \\hat t) dt\u0026rsquo; \\right) $$\nStationary Renewal Theory Stationary input? Not easily realized in experiments (for in vivo experiments). Reasoning: in put to a neuron by other neurons in vivo is not necessarily constant.\nin vitro experiments: impose constant input current.\nThree important quantities:\n  mean firing rate, $\\nu = 1/\\langle s\\rangle$, where the mean interval $\\langle s\\rangle = \\int_0^\\infty s P_0(s) ds$. Since $P_0=-dS_0(s)/ds$, we have $P_0(s) ds= -dS_0(s)$, which leads to\n$$\\langle s\\rangle = -\\int_0^\\infty s \\cdot \\mathrm dS_0(s) =- \\int_0^\\infty s S_0(s)ds - \\left( -\\int_0^\\infty S_0(s)ds \\right) $$\n  autocorrelation function, $$\nC(s) = \\langle S_i(t) S_i(t+s) \\rangle_t = \\frac{1}{T} \\int_{-T/2}^{T/2} S_i(t) S_i(t+s)dt\n$$\n  power spectrum, which is defined as $$ \\mathscr P_T(\\omega) = \\frac{1}{T} \\left \\vert \\int_{-T/2}^{T/2} S_i(t) e^{-i\\omega t} \\right \\vert^2 $$ The importance of it, is that we could find out which frequency mode has the most important amplitude.\n​\nNote that Wiener-Khinchin theorem says $P(\\omega) = \\hat C(\\omega) = \\mathscr{F}(C(s))$.(Proof is straightforward.)\n  signal to noise ratio\nDerive Autocorrelation function of stationary renewal process Define normalized autocorrelation $$ C^0(s) = C(s) -\\nu_i^2 $$ Autocorrelation for $s\u0026gt;0$ $$ \\nu_i C_+(s) = C(s)\\Theta(s) $$\nSome Questions and Comments about The Book   On page 160, $\\nu \\Delta t$ should be the number of spike during $\\Delta t$ . IF we think of it as the probability of spikes, this is not normalized.\n  Dirac delta function has an integral form $$ \\delta(\\omega) = \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty e^{itx} dt $$ ​\n  ","permalink":"/snm/11.stationary_renewal_theory/","tags":null,"title":"11.Renewal Theory"},{"categories":null,"contents":"In the past two meetups, we have been discussing EBM from a computer scientist\u0026rsquo;s perspective.\nIn this discussion, we will discuss chapter XV of Mehta P, Bukov M, Wang C-HH, Day AGRR, Richardson C, Fisher CK, et al. A high-bias, low-variance introduction to Machine Learning for physicists. Phys Rep. 2018;810: 122. doi:10.1016/j.physrep.2019.03.001\n","permalink":"/cpe/12.energy-based-learning-3/","tags":["Energy-based Learning","Principle of Max Entropy","Boltzmann Machine"],"title":"Energy-based Models 3"},{"categories":null,"contents":"Escape noise Two ways to introduce noise in formal spiking neuron models:\n noisy threshold(escape model or hazard model) noisy integration(stochastic spike arrival model or diffusion model)  In the escape model, the neuron may fire when $u\u0026lt;\\vartheta$ the neuron may stay quiescent when $u\u0026gt;\\vartheta$\nEscape rate and hazard function In the escape model, spikes can occur at any time with a probability density,\n$$\\rho=f(u-\\vartheta)$$\nSince $u$ is a function of time,$\\rho$ is also time dependent,\n$$\\rho_{I}(t|\\hat{t})=f[u(t|\\hat{t})-\\vartheta]$$\nRequired condition of function $f$, when $u\\rightarrow-\\infty$, $f\\rightarrow0$\nExample $$f(u-\\vartheta)=\\begin{cases} 0 \u0026amp; for \u0026amp;u\u0026lt;\\vartheta\\\n\\Delta^{-1}\u0026amp; for \u0026amp;u\\ge\\vartheta \\end{cases}$$\n$$f(u-\\vartheta)=\\frac{1}{\\tau_0}$$\n$$f(u-\\vartheta)=\\beta[u-\\vartheta]_{+}=\\big{\\begin{array}{lcc} 0 \u0026amp;for\u0026amp;u\u0026lt;\\vartheta\\\n\\beta(u-\\vartheta)\u0026amp;for\u0026amp;u\\ge\\vartheta \\end{array}$$\n$$f(u-\\vartheta)=\\frac{1}{2\\Delta}[1+erf(\\frac{u-\\vartheta}{\\sqrt{2}\\sigma})]$$\n$$erf(x)=\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{x}\\exp(-y^2)dy$$\nInterval distribution and mean fire rate the expect value of interval distribution = $\\frac{1}{mean\\space fire\\space rate}$ = mean period\nuse $\\rho$ we can get interval distribution,\n$$P_{I}(t|\\hat{t})=\\rho\\space \\exp[-\\int_{\\hat{t}}^{t}\\rho dt]$$\n$$\\rho=f[u(t|\\hat{t})-\\vartheta]$$\nuse $SRM_{0}$,\n$$u(t|\\hat{t})=\\eta(t-\\hat{t})+h(t)$$\n$$h(t)=\\int_{0}^{\\infty}\\kappa(s)I(t-s)ds$$\nuse non-leaky integrate-and-fire,\n$$u(t|\\hat{t})=u_r+\\frac{1}{C}\\int_{\\hat{t}}^{t}I(t\u0026rsquo;)dt'$$\nuse leaky integrate-and-fire,\n$$u(t|\\hat{t})=RI_0[1-e^{(-t-\\hat{t})/\\tau_m}]$$\nuse $SRM_0$ with periodic input,we get periodic response,\n$$h(t)=h_0+h_1cos(\\Omega t+\\varphi_1)$$\n$$\\eta(s)=\\begin{cases} -\\infty \u0026amp; for \u0026amp; s\u0026lt;\\Delta^{abs}\\\n-\\eta_0 \\exp{\\big(-\\frac{s-\\Delta^{abs}}{\\tau}\\big)} \u0026amp; for \u0026amp; s\u0026gt;\\Delta^{abs} \\end{cases}$$\n","permalink":"/snm/12.escape_noise/","tags":null,"title":"09.Escape Noise"},{"categories":null,"contents":"In this discussion, we will discuss the Pytorch Deep Learning Lectures by LeCun.\n","permalink":"/cpe/13.energy-based-learning-4/","tags":["Energy-based Learning","Principle of Max Entropy","Boltzmann Machine"],"title":"Energy-based Models 4"},{"categories":null,"contents":"Single Neuron Models Hodgkin-Huxley Model Biological principles: ion channel openness\n$$ I(t)= I_\\mathrm{cap}(t)+\\sum_{k}I_k(t) $$\nThis equation summarize the current run over the all ion channels. Three types of ion channels are considered: sodium channels, potassium channels, and unspecific leaky channels.\n$$ C\\frac{du}{dt}=-\\sum_kI_k(t)+I(t) $$\n$C\\frac{du}{dt}$ denotes the current across the capacitor.\n$$ \\sum_kI_k(t) = g_{Na}m^3h(u-E_{Na})+g_kn^4(u-E_k)+g_L(u-E_L) $$\nThis equation denotes the ionic channel currency, where m , n, and h could be analyzed with master equation.\n$$ \\dot m= \\alpha_m(u)(1-m)-\\beta_m(u)m $$\n$$ \\dot n= \\alpha_n(u)(1-n)-\\beta_n(u)n $$\n$$ \\dot h= \\alpha_h(u)(1-h)-\\beta_h(u)h $$\nAdvantages: represent real biological process Disadvantage: computation-expensive\n2D Model Leaky Integrate-and-Fire Model $$ I(t)=\\frac{u(t)}{R}+C\\frac{du}{dt} $$\nwhere $\\tau_m=RC$ is the leaky integrator, so it yields this equation:\n$$ \\tau_m\\frac{du}{dt}=-u(t)+RI(t) $$\nNonlinear Integrate-and-Fire Model Spike Response Model no equations available\nIzhikevich Model ","permalink":"/snm/13.all_neuron_models/","tags":null,"title":"13.Comparison Between Neuron Models"},{"categories":null,"contents":"In this meetup, we will discuss Restricted Boltzmann Machine (RBM). We will cover the reason for introducing RBM and the training. At the end of the discussion, we will also cover some topics of Deep Boltzmann Machines.\n","permalink":"/cpe/14.energy-based-learning-5/","tags":["Energy-based Learning","Principle of Max Entropy","Boltzmann Machine"],"title":"Energy-based Models 5"},{"categories":null,"contents":"Noise Using Stochastic Parameter For a short review of refractory kernel, please refer to page 114 of the textbook (4.2.3 Simplified model SRM0), i.e.,\n$$ u_i(t) = \\sum_f \\eta_0 (t-t_i^{(f)}) + \\sum_j w_{ij} \\sum_f \\epsilon_0 (t-t_j^{(f)}) + \\int_0^\\infty \\kappa_0 (s) I_i ^{ext} (t-s) ds. $$\nFor noise models, we define refractory kernel\n$$ \\eta(s) = \\eta_0 e^{-s/\\tau}, $$\nwhere\n$$ \\eta_0 \\equiv \\eta_0(r) = \\tilde \\eta_0 e^{r/\\tau}, $$\nwhich in turn is plugged back into the refractory kernel,\n$$ \\eta(s) = \\tilde \\eta_0 e^{- (s-r)/\\tau}. $$\nWe require that $r$ to be a parameter with mean $\\langle r \\rangle = 0$.\nWe discussed threshold in 5.3.1 where we said spikes occur with probability density\n$$ \\rho = f(u-\\theta), $$\nin which $\\theta$ is the threshold. For noise model the threshold is a noisy function.\n\u0026ldquo;Noise reset\u0026rdquo; model\n$$ \\theta = u(t \\vert \\hat t, r) = \\eta_0(r) e^{-(t-\\hat t)/\\tau} + \\int_0^\\infty \\kappa (t-\\hat t,s) I(t-s) ds. $$\nSpike occur at $t$ when the potential reaches threshold $u(t\\vert \\hat t,r)=\\theta$, thus the interval of spike is given by\n$$ T(\\hat t,r) = \\mathrm{min}\\left[ t -\\hat t \\vert u(t\\vert \\hat t,r)=\\theta \\right]. $$\nInterval distribution:\n$$ P_I(t\\lvert \\hat t) = \\int dr \\delta ( t-\\hat t - T(\\hat t, r) ) \\mathscr{G}_0(r). $$\nSRM0 model:\n$$ \\begin{align} u(t\\lvert \\hat t,r) \u0026amp;= \\eta_r(t-\\hat t) + h(t)\\\n\u0026amp;= \\tilde \\eta_0 e^{-(t-\\hat t- r)/\\tau} + h(t). \\end{align} $$\nThe stochastic parameter $r$ work as a shift of the spikes on time axis.\nDiffusive Noise Integrate-and-fire model:\n$$ \\tau_m \\frac{d}{d t} u = - u + R I(t). $$\n Membrane time constant $\\tau_m$; Input resistance $R$; Input current $I$.  Introducing noise: add noise to the RHS,\n$$ \\tau_m \\frac{d}{d t} u = - u + R I(t) + \\xi (t), $$\nwhere $\\xi(t)$ is a stochastic term thus the equation becomes a stochastic differential equation.\nFigure 5.12 is a very nice plot showing the effect of $\\xi$ on threshold.\nFor a Gaussian white noise\n$$ \\langle \\xi(t) \\xi(t\u0026rsquo;) \\rangle = \\sigma^2 \\tau_m \\delta(t-t\u0026rsquo;). $$\n $\\sigma$ amplitude of noise; $\\tau_m$ membrane time constant.  c.f. Ornstein-Uhlenbeck process.\nStochastic Spike Arrival In a network, a integrate-and-fire neuron will take in\n input $I^{ext}(t)$, input spikes at $t^{(f)}_j$, where $j$ means the spike from neuron $j$, stochastic spikes (from the background of the brain that we are not really interested in for now) $t_k^{(f)}$,  so that\n$$ \\frac{d}{dt} u = - \\frac{u}{\\tau_m} + \\frac{1}{C}I^{ext}(t) + \\sum_j \\sum_{t_j^{(f)} \u0026gt; \\hat t} w_j \\delta(t- t_j^{(f)}) + \\sum_k \\sum_{t_k^{(f)} \u0026lt; \\hat t} w_k \\delta(t-t_k^{(f)}), $$\nwhich is called Stein\u0026rsquo;s model.\nThe stochastic spike arrivals are Poissonian.\nExample: Membrane Potential Fluctuations   Poisson process with rate $\\nu$\n  Input spike train $$ S(t) = \\sum_{k=1}^N \\sum_{t_k^{(f)}} \\delta(t-t_k^{(f)}), $$\nwhich has an average\n$$ \\langle S(t) \\rangle = \\nu_0, $$\nand autocorrelation\n$$ \\langle S(t) S(t\u0026rsquo;)\\rangle - \\nu_0^2 = N\\nu_0 \\delta(t-t\u0026rsquo;). $$\n$\\nu_0^2$ is from the constant hazard $\\rho_0(t-\\hat t) = \\nu$ and Poisson has autocorrelation $C_{ii}(s) = \\nu \\delta(s) + \\nu^2$.\n  Neglect both threshold and reset, which basically means weak input so that neuron doesn\u0026rsquo;t reach firing threshold. $$ u(t) = w_0 \\int_0^\\infty \\epsilon_0(s) S(t-s) ds $$\nAlso neglect the term $-u/\\tau_m$?\n  Average over time we have $$ u_0 \\equiv \\langle u(t) \\rangle = w_0 \\nu_0 \\int_0^\\infty \\epsilon_0(s)ds. $$\n  Variance of potential $$ \\begin{align} \\langle (u-u_0)^2 \\rangle \u0026amp;= w_0^2 \\left\\langle \\left( \\int_0^\\infty \\epsilon_0(s) S(t-s) - w_0 \\nu_0 \\int_0^\\infty \\epsilon_0(s)ds \\right)^2 \\right\\rangle \\\n\u0026amp; = \\left\\langle w_0^2 \\int_0^\\infty \\int_0^\\infty \\epsilon_0(s) \\epsilon_0(s\u0026rsquo;) S(t-s) S(t-s\u0026rsquo;) ds\u0026rsquo; ds - 2 u_0 w_0 \\nu_0 \\int_0^\\infty \\epsilon_0(s) S(t-s)ds + u_0^2 \\right\\rangle \\\n\u0026amp; = \\left\\langle w_0^2 \\int_0^\\infty \\int_0^\\infty \\epsilon_0(s) \\epsilon_0(s\u0026rsquo;) S(t-s) S(t-s\u0026rsquo;) ds\u0026rsquo; ds \\right\\rangle - 2 u_0 w_0 \\left\\langle \\nu_0 \\int_0^\\infty \\epsilon_0(s) S(t-s)ds \\right\\rangle + u_0^2 \\\n\u0026amp; = \\left\\langle w_0^2 \\int_0^\\infty \\int_0^\\infty \\epsilon_0(s) \\epsilon_0(s\u0026rsquo;) S(t-s) S(t-s\u0026rsquo;) ds\u0026rsquo; ds \\right\\rangle - 2 u_0^2 + u_0^2 \\\n\u0026amp; = w_0^2 \\nu_0 \\int_0^\\infty \\epsilon_0(s)^2ds \\end{align} $$\n  Figure 5.14: We have equation 5.83 $\\langle \\delta u^2 \\rangle = 0.5 \\tau_m \\sum_k w_k^2 \\nu_k$, larger $w_k$ will give us larger variance of potential so that the spikes are more probable.\nDiffusion Limit Stein model\n$$ \\frac{d}{dt} u = - \\frac{u}{\\tau_m} + \\frac{1}{C}I^{ext}(t) + \\sum_j \\sum_{t_j^{(f)} \u0026gt; \\hat t} w_j \\delta(t- t_j^{(f)}) + \\sum_k \\sum_{t_k^{(f)} \u0026lt; \\hat t} w_k \\delta(t-t_k^{(f)}), $$\nAfter each firing, probability density of membrane potential can be calculated.\nBetween $\\Delta t$, the probability of firing is $\\sum_k\\nu_k \\Delta t$. As a result, the probability of quite is\n$$ 1 - \\sum_k\\nu_k \\Delta t. $$\nDuring this time the membrane potential will decay\n$$ u(t+\\Delta t) = u(t) e^{-\\Delta t/\\tau_m}. $$\nIncoming spike at synapse $k$:\n$$ u(t+\\Delta t) = u(t) e^{-\\Delta t/\\tau_m} + w_k. $$\n","permalink":"/snm/14.slow-noise/","tags":null,"title":"14.Noise in Refractory Kernel and Diffusive Noise"},{"categories":null,"contents":"In this meetup, we will discuss this paper: https://arxiv.org/abs/2006.04182\nWhy? Feedforward-backprop usually has a loss function that involves all the parameters. Backprop means we need this huge global loss $\\mathcal L({w_{ij}})$. However, it is hard to imaging such global loss calculations in our brain. One of the alternatives is predictive coding, which only utilizes local connection information.\nIn this paper (2006.04182), the author proves the equivalence of backprop and predictive coding on arbitary graph.\n","permalink":"/cpe/15.predictive-coding/","tags":["Biological Neural Network"],"title":"Predictive Coding Approximates Backprop along Arbitrary Computation Graphs"},{"categories":null,"contents":"Diffusive Noise and The subthreshold Regime Diffusive noise Interval distribution Because the diffusive noise generated by stochastic spike arrival, we cannot predict the exact value of the neuronal membrane potential, but only the probability that the membrane potential is in a certain interval$[u_0,u_1]$\n$$Prob{u_0\u0026lt;u(t)\u0026lt;u_1|u(\\hat{t})-u_r}=\\int_{u_0}^{u_1}p(u,t)du$$\nthus ,we can get the survivor function,\n$$S_I(t|\\hat{t})=\\int_{-\\infty}^{\\vartheta}p(u,t)du$$\nthe interval distribution,\n$$P_I(t|\\hat{t})=-\\frac{d}{dt}\\int_{-\\infty}^{\\vartheta}p(u,t)du$$\nThe sub-threshold regime sub and super threshold stimulation Input $I(t)$ is sub-threshold, if it generates a membrane potential that stays below the firing threshold\nInput $I(t)$ is sup-threshold, if it generates a membrane potential higher than the threshold\nIn the super-threshold regime,\nnoise broadens the interspike interval distribution.\nIn the sub-threshold regime,\nspikes are generated by the fluctuations of the membrane potential, rather than by its mean.\ncoefficient of variation C $$C^2_V=\\frac{\\langle\\Delta s^2\\rangle}{\\langle s\\rangle^2}$$\n$C_v\u0026gt;1$,broader than that generate by Poisson distribution\n$C_v=1$,noise generate by Poisson distribution\n$C_v\u0026lt;1$,more regular\n$C_v=0$,regular firing with a fixed period $\\langle s \\rangle$\nPoisson neuron with absolute refractoriness,\n$$C_V=1-\\frac{\\Delta^{abs}}{\\langle s \\rangle}$$\nrefractoriness make the spikes more regularly than a Poisson neuron.\n","permalink":"/snm/15.diffusive_noise_and_the_subthreshold_regime/","tags":null,"title":"15.Diffusive Noise and The Subthreshold Regime"},{"categories":null,"contents":"In this meetup, we will discuss some key ideas related to biological neural network: LTP and LTD.\n","permalink":"/cpe/16.ltd-ltp/","tags":["Biological Neural Network"],"title":"LTD/LTP"},{"categories":null,"contents":"Noise types motivated in single neuron models Why noise?\nDuring simulation, simplified neuron models like RSM, leaky integrate and fire model synthesize regular firing activity. However, in biological reality, the inter spike intervals are exponentially distributed, according to a point process of spike generation.\nescape noise: focus on firing threshold, replace fixed firing threshold with firing probability based on the difference between firing threshold and membrane potential. $$ \\rho = f(u-\\theta) = \\frac{1}{\\Delta}\\int_{-\\infty}^{(u-\\theta)} \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{x^2}{2\\sigma^2}}\\mathrm{d}x $$\nslow noise: focus on refractory time. $\\eta$ is the exponential refractory kernel,\n$\\eta(s)=\\eta_0 e^{\\frac{-s}{\\tau}}$\nBut the amount $\\eta_0$ depends on a stochastic variable $r$:\n$\\eta_0(r)=\\tilde \\eta_0 e^{\\frac{r}{\\tau}}$\ndiffusive noise: focus on stochastic synaptic inputs to the neuron, which leads to fluctuations of membrane potential. Firing or not is based on the distribution of membrane potential.\nLet\u0026rsquo;s look at a integrate-and-fire model, $$ \\frac{\\mathrm{d}}{\\mathrm{d}t}u(t)=-\\frac{u(t)}{\\tau_m}+\\frac{1}{C}I^{ext}(t)+\\sum_j\\sum_{t_j^{(f)}\u0026gt;\\tilde t}w_j\\delta(t-t_j^{(f)})+\\sum_k\\sum_{t_k^{(k)}\u0026gt;\\tilde t} w_k\\delta(t-t_k^{(f)}) $$ $\\delta(t-t_k^{(f)})$ is the superposition of all pre-synaptic inputs, weighted by $w_k$. This input spike train is generated by Poisson Process.\nFrom diffusive noise to escape noise For a model of diffusive noise, the membrane potential is normally distributed, even at the threshold level, if the neuron doesn\u0026rsquo;t fire immediately. $$ P(u\\sim \\theta) = \\Delta t e^{-\\frac{[u_0(t)-\\theta]^2}{2\u0026lt;\\Delta u^2(t)\u0026gt;}} $$ According to Stochastic Process, $\\Delta u^2(t)$ approaches a constant value of $\\sigma^2/2$.\nSo, the probability density at $\\theta$ could be rewritten as $$ f(u_0-\\theta)=\\frac{c_1}{\\tau_m}e^{-\\frac{[u_0(t)-\\theta]^2}{\\sigma^2}} $$ which share the same unit of one over time with $P(u\\sim\\theta)$, known as a escape rate formula.\nTo define a shift of probability density function when $u$ crosses $\\theta$: $$ f(u_0,\\dot u_0) = (\\frac{c_1}{\\tau_m} + \\frac{c_2}{\\sigma}[\\dot u_0]_+) e^{-\\frac{[u_0(t)-\\theta]^2}{\\sigma^2}}, $$ with $x(t)=\\frac{u_0(t)-\\theta}{\\sigma}$.\nSo the similarity of diffusive noise and escape noise is that they all care about the difference between current membrane potential and threshold $u-\\theta$, and the firing probability based on this difference.\nStochastic resonance notice: noise could improve the signal transmission property of neuronal system, especially in sub-threshold regime.\nNoise makes the neuron fire: $$ |x|=|(u-\\theta)/\\sigma|. $$ If the normalized distance is small, the neuron has great probability to fire (exponentially dependence). If noise $\\sigma$ is very large, $x^2 \\sim 0$\nFind the optimal noise level: signal-to noise ratio(SNR)\n$$ \\sigma^{opt} \\approx \\frac{2}{3}(\\theta-u_\\infty) $$ since $\\sigma^2=2\u0026lt;\\Delta u^2\u0026gt;$, the transmission is optimal if the stochastic fluctuations of the membrane potential have an amplitude: $$ \\frac{3}{\\sqrt 2}\\sqrt{\u0026lt;\\Delta u^2\u0026gt;} \\approx \\theta-u_\\infty $$\nStochastic firing and rate models three rate models\nanalog neurons (averaging over time) $$ v=\\frac{n_{sp}(T)}{T} $$ $T$: time period\n$n$: spike counts over time $T$\nFor constant current input $I_0$, the firing rate is a function of $I_0$:\n$v=g(I_0)$, which is called the gain function of the neuron.\nstochastic rate model definition: the process of generation a spike is stochastic, rate of the underlying Poisson process that generate the spikes.\ninhomogeneous Poisson model:\n$v_i=g(h_i)$,\nwhere $h_i(t)=\\sum_j \\sum_f w_{ij} \\epsilon_0(t-t_j^{(f)})$\nstochastic model in discrete time(?)\npopulation rate model an average activity of a population of equivalent neurons.\n$$ A(t)=\\mathrm{lim}{\\Delta t \\to0} \\mathrm{lim}{N\\to\\infty} \\frac{1}{\\Delta t} \\frac{n_{act}(t;t+\\Delta t)}{N} $$\nThe interaction between two groups of neurons (group $l$ and group $k$) could be represented as: $$ A_k=g(\\sum_l J_{kl} A_l), $$\nwhere $g$ again is the gain function, but $J$ is not connection weights, but the effective interaction strength between two groups.\n","permalink":"/snm/16.stochastic_process/","tags":null,"title":"16.stochastic process"},{"categories":null,"contents":"We have discussed many topics of generative models. This paper serves as a summary of the current season of the discussions.\n","permalink":"/cpe/17.self-supervised-learning/","tags":["Self-supervised Learning"],"title":"Self-supervised Learning: Generative or Contrastive"},{"categories":null,"contents":"Review of Population Activity Information from spatio-temporal pattern:\n  Temporal average: mean firing rate of the network:\n$$ \\begin{equation} A = \\frac{\\text{total number of spikes } n_{\\mathrm{act}}(t;t+\\Delta t) \\text{ during a time window } \\Delta t}{\\text{time widow } \\Delta t \\times \\text{ number of neurons } N }, \\end{equation} $$\nwhere $\\Delta t$ is the time binning. We can take the limit $\\Delta t\\to 0$. For spiking neuron models,\n$$ \\begin{align} A =\u0026amp; \\lim_{\\Delta t\\to 0}\\frac{n_{\\mathrm{act}}(t;t+\\Delta t)}{\\Delta t} \\frac{1}{N} \\\n=\u0026amp; \\frac{1}{N} \\text{total number of active neurons at time } t. \\end{align} $$\nAssume the $j$th neuron fires at $t_j^{(f)}$ (where $f$ means neuron $j$ can fire at $t_j^{(1)}$, $t_j^{(2)}$, etc), we know which neuron would fire at any time $t$ by calculating\n$$ \\delta(t-t_j^{(f)}). $$\nIf the argument is zero for neuron $j$, we know this neuron fires at time $t$. Thus the total number of active neurons at time $t$ is\n$$ \\sum_{j=1}^N \\sum_{f} \\delta(t-t_j^{(f)}). $$\nThe summation of $f$ counts all the contributions from neuron $j$ since it can fire multiple times.\n  Temporal correlations between pulses of different neurons;\n  Define Homogeneous Network Homogeneous:\n Each and every neurons are the same; Neurons take the same external input; Weight between arbitrary neuron $i$ and arbitrary neuron $j$ are the same $w_{ij}=J_0/N$.  Modeling Using Leaky Integrate-and-fire Model $$ \\tau_m \\frac{d}{dt} u_i = - u_i + R I_i(t), $$\nwhere the input on neuron $i$ at time $t$ ($I_i(t)$) includes both external and internal signal\n$$ I_i(t) = \\sum_j \\sum_f w_{ij} \\alpha(t-t_j^{(f)}) + I^{\\mathrm{ext}}(t), $$\nwhere $\\alpha(t-t_j^{(f)})$ is the time course after a $f$th spike in neuron $j$.\n QUESTION HERE:\nAssuming that we start from a homogeneous initial condition that all neurons have the same input will the network evolve to an inhomogeneous state under tiny perturbation? This should be called the stability of the network.\nThe method I can think of to check the instability is to add a field of perturbation, i.e., a perturbation to each input and check the evolution of these perturbations. If all the perturbations increase with time, we obtain a hint that the system could be unstable. Then we can investigate using numerical methods or linear stability analysis to find out exactly how.\nCOMMENT: This perturbation method is similar to the one discussed in Section 6.2.1 as stochastic input.\n Regardless of the question above, we assume that no perturbation is included so that the system evolve homogeneously. Input current on arbitrary neuron is the same as each other\n$$ I(t) = \\frac{1}{N} \\sum_j \\sum_f J_0 \\alpha(t-t_j^{(f)}) + I^{\\mathrm{ext}}(t). $$\nDue to homogeneity, we can plug in the definition of population activity\n$$ A = \\frac{1}{N} \\sum_j \\sum_f \\delta(t-t_j^{(f)}) $$\nfor every neurons. At time $s$, we know the activity is calculated as $$A(s)$$, the input current at time $t$ which is later than $s$ depends on all the history of population activity, which means we include all the history to find the current input current, i.e.,\n$$ I(t) = \\frac{1}{N} \\int_0^\\infty ds \\alpha(s) \\left[N A(t-s)\\right] + I^{\\mathrm{ext}}(t). $$\nModeling Using SRM SRM\n$$ \\begin{equation} u_i(t) = \\eta(t-\\hat t_i) + h_{\\mathrm{PSP}}(t\\vert \\hat t_i)\\label{srm}. \\end{equation} $$\nUsing an argument similar to integrate-and-fire model, we can plug in the population activity to equation (\\ref{srm}).\nRecall that spikes occur as the potential $u_i$ reaches the threshold $\\theta$.\nDensity Equations Intergrate-and-fire and stochastic spike arrival Each neuron is described by its membrane potential. We can find the percentage of neurons within a range of membrane potential $[u,u+du]$, which is\n$$ \\begin{equation} \\frac{dN}{N} = p(u,t) du, \\end{equation} $$\nwhere $p(u,t)$ is the membrane potential (distribution) density.\n  Due to the normalization $1/N$, we expect integral over all the possible potentials gives us 1, i.e.,\n$$ \\begin{equation} \\int_{-\\infty}^\\theta p(u,t),du = 1. \\end{equation} $$\n  Define the flux of neutrons going through the threshold by inspecting the population increase at potential $u_{\\mathrm{reset}}$.\n  About the stochastic input, we assume that a total number of M types of synapses are introduced. For each type $k$ we have a input rate $\\nu_k$. The equation for the $p(u,t)$ is written down in Section 6.2.1 as equation (6.14). In general, we can define the flux going across a potential by checking the time derivative of $p(u,t)$,\n$$ \\begin{equation} \\partial_t p(u,t) ,du = dJ + dJ_{\\mathrm{reset}}, \\end{equation} $$\nwhere\n$$ dJ = dJ_{\\mathrm{drift}} + dJ_{\\mathrm{jump}}, $$\nwhere $dJ_{\\mathrm{drift}}$ rate of neurons that has a potential continuously evolved into $u$ during time $[t,t+dt]$, while $dJ_{\\mathrm{jump}}$ stands for the rate of neurons jumping from all different potential $u-w_k$'s into $u$ due to stochastic input. **READ Fig. 6.2**\n For a better understanding of $dJ$, please refer to Eq. (\\ref{continuity-equation}).\n Flux means the percentage of neurons that goes across a specific potential $u$ at time $t$ per unit potential. In other words, it\u0026rsquo;s the changing rate of the membrane potential density at potential $u$ during time interval $[t,t+dt]$.\nEquation 6.18 shows the population activity.\nContinuity Equation Continuity equation describes the rate of change. In this case, continuity is the rate of change in percentage of neurons at time $t$ within a range of potentials $[u_0,u_1]$, so that\n$$ \\begin{equation} \\partial_t \\int_{u_0}^{u_1} p(u,t),du = J(u_0,t) - J(u_1,t). \\label{continuity-equation} \\end{equation} $$\n An example of continuity equation is the conservation of charge. The change of charge equals to the charge coming in and the charge going out.\n The we know the change in the percentage of neurons within potential range $[u_0,u_1]$ is the percentage of neurons moving within the potential range $J(u_0,t)$ and the percentage of neurons going out $J(u_1,t)$. In the language of density, we have\n$$ \\begin{equation} \\partial_t p(u,t) = - \\partial_u J(u,t), \\end{equation} $$\nwhere $u\\neq u_{\\mathrm{reset}}$ and $u\\neq \\theta$.\nDiffusion Approximation Expand equation 6.14 in terms of $w_k$ and keep only $\\mathscr{O}(w_k^2)$,\n$$ \\begin{align} p(u-w_k,t) =\u0026amp; p(u,t) + \\partial_u p(u-w_k,t)\\vert_{u-w_k=u} (-w_k) + \\frac{1}{2}\\partial_u^2 p(u-w_k,t)\\vert_{u-w_k=u} w_k^2 + \\mathscr{O}(w_k^3) \\\n=\u0026amp; p(u,t) - \\partial_u p(u,t) w_k + \\frac{1}{2} \\partial_u^2 p(u,t) w_k^2 + \\mathscr{O}(w_k^3). \\end{align} $$\nPlug into equation 6.14\n$$ \\begin{align} \\tau_m \\partial_t p(u,t) =\u0026amp; - p(u,t) - \\left[ -u + R I^{\\mathrm{ext}}(t) + \\tau_m \\sum_k v_k(t) w_k \\right] \\partial_u p(u,t) \\\n\u0026amp;+ \\frac{1}{2} \\tau_m \\sum_k v_k(t) w_k^2 \\partial_u^2 p(u,t) + \\tau_m A(t) \\delta(u-u_r) + \\mathscr{O}(w_k^3). \\end{align} $$\nThe first two terms are perfect derivatives\n$$ \\begin{align} \\phantom{=}\u0026amp;- p(u,t) - \\left[ -u + R I^{\\mathrm{ext}}(t) + \\tau_m \\sum_k v_k(t) w_k \\right] \\partial_u p(u,t) \\\n=\u0026amp; -\\partial_u \\left[\\left(-u + R I^{\\mathrm{ext}}(t) + \\tau_m \\sum_k v_k(t) w_k \\right)p(u,t) \\right]. \\end{align} $$\nThe equation we need to deal with is\n$$ \\begin{align} \\tau_m \\partial_t p(u,t) =\u0026amp; -\\partial_u \\left[\\left(-u + R I^{\\mathrm{ext}}(t) + \\tau_m \\sum_k v_k(t) w_k \\right)p(u,t) \\right]\\\n\u0026amp; + \\frac{1}{2} \\tau_m \\sum_k v_k(t) w_k^2 \\partial_u^2 p(u,t) + \\tau_m A(t) \\delta(u-u_r) + \\mathscr{O}(w_k^3). \\end{align} $$\n Fokker–Planck equation\n  $A(t)$ is the flux at threshold $\\theta$. To solve it we simply set $u=\\theta$, $$ \\begin{equation} A(t) = \\frac{1}{\\tau} (- \\theta + R I^{\\mathrm{ext}}(t)) p(\\theta,t) + \\sum_k v_k \\int_{\\theta - w_k}^\\theta p(u,t) du. \\end{equation} $$\n To expand $A(t)$ around $u=\\theta$, we need\n$$ \\begin{equation} p(u,t) = p(\\theta,t) + \\partial_u p(u,t)\\vert_{u=\\theta} (u-\\theta) + \\frac{1}{2} \\partial_u^2 p(u,t) \\vert_{u=\\theta} (u-\\theta)^2 + \\mathscr{O}((u-\\theta)^3),\\label{eq-p-u-t-expand-theta} \\end{equation} $$\nso that $$ \\begin{equation} \\int_{theta-w_k}^\\theta(u,t) du = p(\\theta,t) w_k + \\frac{1}{2} \\partial_u p(u,t)\\vert_{u=\\theta} w_k^2,\\label{eq-int-p-u-t-expand-theta} \\end{equation} $$ where $p(\\theta,t)=0$ since this is the threshold and no neurons can stay at this potential. Eq.(\\ref{eq-p-u-t-expand-theta}) and Eq.(\\ref{eq-int-p-u-t-expand-theta}) are plugged back into the express for $A(t)$. The simplified threshold flux is\n$$ \\begin{equation} A(t) = \\frac{1}{2}\\sum_k v_k w_k^2\\partial_u p(u,t)\\vert_{u=\\theta}. \\end{equation} $$\n I obtained a different sign. In the text book $A$ is negative.\n Stationary Solution  Why stationary solution? Experiments.\n Define\n$$ \\begin{equation} \\sigma^2 = \\tau_m \\sum_k v_k w_k^2. \\end{equation} $$\nStationary state means $p(u,t)$ is time translation invariant, i.e., $p$ only depends on $u$. The Fokker-Planck equation becomes\n$$ \\begin{equation} 0= -\\partial_u \\left[ (-u + h)p - \\frac{1}{2}\\frac{\\sigma^2}{\\tau_m} \\partial_u p \\right] + A\\delta(u-u_r), \\end{equation} $$ where $h=R I^{\\mathrm{ext}}+\\tau_m \\sum_k v_k w_k$.\n$A$ is the change of percentage of neurons at threshold (some kind of flux), dimension analysis shows the terms left is of dimension $[\\partial_u J]$.\n Dimension of $\\delta(u-u_r)$ is $[1/u]$.\n Indeed we define a flux\n$$ \\begin{equation} J = \\frac{-u+h}{\\tau_m} p - \\frac{1}{2} \\frac{\\sigma^2}{\\tau_m} \\partial_u p. \\end{equation} $$\nEducated guess $$ \\begin{equation} p = \\begin{cases} \\frac{c_1}{\\sigma^2} \\exp \\left( - \\frac{(u-h)^2}{\\sigma^2} \\right) \u0026amp; \\qquad u\u0026lt;u_r\\\n\\frac{c_2}{\\sigma^2} \\exp \\left( - \\frac{(u-h)^2}{\\sigma^2} \\right) \\int_u^\\theta \\exp \\left( \\frac{(x-h)^2}{\\sigma^2} \\right) dx \u0026amp; \\qquad u_r\u0026lt;u\u0026lt;\\theta\\\n\\end{cases} \\end{equation} $$\n","permalink":"/snm/17.homogeneous-network/","tags":null,"title":"17.Homogeneous Network"},{"categories":null,"contents":"SRM with Escape Noise $$ \\begin{equation} u(t) = \\eta(t-\\hat t) + h_{\\mathrm{PSP}}(t\\lvert \\hat t). \\end{equation} $$\n Define a parameter $r=t-\\hat t$. Define density for $r$, i.e., fraction of neurons with parameter $[r_0,r_0+\\Delta r]$ is given by $\\int_{r_0}^{r_0+\\Delta r} q(r\u0026rsquo;,t)dr'$. Continuity equation: $$\\partial_t q(r,t) = -\\partial_r J_{\\mathrm{refr}}(r,t)$$. $J_{\\mathrm{refr}}=q(r,t)\\partial_t r=q(r,t)$ is the continuous flux. Hazard function $$\\rho(t\\vert t-r) =f(\\eta(r)+h_{\\mathrm{PSP}}(t\\vert t-r))$$ tells us about the firing rate of a neuron. Loss per unit time $$J_{\\mathrm{loss}}=- \\rho(t\\vert t-r)q(r,t)$$. At time $t$, total number of neurons that fire, which is also called population activity $$A(t)=\\int_0^\\infty (-J_{\\mathrm{loss}})dr$$.  The change in the fraction of neurons with parameter $r$ depends on\n continuous flow passing by $r$, the loss flux derivative, the population activity,  so that we obtain\n$$ \\begin{equation} \\partial_t q(r,t) = -\\partial_r q(r,t) - \\rho(r\\vert t-r) q(r,t) + A(t) delta(r) \\end{equation} $$\nPopulation activity is the quantity we would love to obtain. By rewriting the previous equation\n$$ \\begin{equation} A(t)= \\int_{-\\infty}^t P_I(t\\vert \\hat t) A(\\hat t)d\\hat t, \\end{equation} $$\nwhere\n$$ \\begin{equation} P_I(t\\vert \\hat t)= \\rho(t\\vert \\hat t) \\exp \\left( - \\int_{\\hat t}^t \\rho(t\u0026rsquo;\\vert \\hat t) dt\u0026rsquo; \\right) \\end{equation} $$\n","permalink":"/snm/18.population-srm-with-escape-noise/","tags":null,"title":"18.SRM with Escape Noise"},{"categories":null,"contents":"Review of Several Concepts Population activity $A(t)$   From equations\n$$ \\begin{equation}\\partial_t p(u,t)= \\cdots + A(t)\\delta(u-u_r)\\end{equation} $$\nWe integrate over a range of potential\n$$\\begin{equation} \\partial_t \\int_{u_1}^{u_2} p(u,t) = \\cdots + A(t), \\end{equation}$$\nprovided that $u_r$ is within $[u_1,u_2]$.\nSo it\u0026rsquo;s some kind of flux. It works as a source term of faction of neurons at $u=u_r$, which is identical to fraction of neurons that spiked per unit time.\n  In fact, we have\n$$ \\begin{equation} A(t) = J(\\theta,t) \\end{equation} $$\n  Spike interval distribution $P_I(t\\vert \\hat t)$ $P_I(t\\vert \\hat t)$ is the probability density of firing, i.e.,\n$$ \\begin{equation} \\int_{t_1}^{t_2} P_I(t\\vert \\hat t) dt \\end{equation} $$\ncalculates the probability of finding spikes during time interval $[t_1,t_2]$.\nMeanwhile we have this survival probability\n$$ \\begin{equation} S_I(t\\vert \\hat t) = 1 - \\int_{\\hat t}^t P_I(s\\vert \\hat t) ds \\end{equation} $$\nor identically\n$$ \\begin{equation} P_I(t\\vert \\hat t) = \\partial_t S_I(t\\vert \\hat t) \\end{equation} $$\nMotivation What we need for a complete description of network activities is to calculate $A(t+\\Delta t)$ given $A(t)$ as well as the external input at $t$.\nUsing whatever we have up to this point, the procedure should be\n  Apply $A(t)$ and $I^{\\mathrm{ext}}(t)$ to equation (6.8). Within a small time interval $\\Delta t$, we obtain the PSP potential, i.e.,\n$$ h_{\\mathrm{PSP}}(t + \\Delta t\\vert \\hat t) = J_0 \\int_0^\\infty \\epsilon(t - \\hat t, s) A(t - s) ds + \\int_0^\\infty \\kappa(t-\\hat t,s) I^{\\mathrm{ext}}(t-s) ds $$\nMy thought is, calculating next step is impossible, since we have an integral to infinity? I do not really get it.\n  The ultimate reason is that we have insufficient equations compared to the unknown quantities. Unknown: $A(t)$, $h_{\\mathrm{PSP}}$, but we have only one equation.\nQuestion: What about other equation? Eqn (6.21), one equation, two variables. Same fate.\n  So we need another equation. What we get is\n$$ A(t) = \\int_{-\\infty}^t P_I(t\\vert \\hat t) A(\\hat t) d\\hat t. $$\nOh wait, new function $P_I(t\\vert \\hat t)$ has been introduced to the function. How is that going to help? $P_I$ actually can be determined by $h$.\nEq. 6.78, 6.79, 6.80\nWith Absolute refractoriness Wilson-Cowan integral form\n$$ \\begin{equation} A(t) = f[h(t)]\\left( 1 - \\int_{t-\\Delta^{\\mathrm{abs}}}^t A(t\u0026rsquo;) dt\u0026rsquo; \\right), \\end{equation} $$\nwhere\n $f[h(t)]$ rate of firing for a neuron that is not refractory, given the togal input potential $h(t)$.  Constant input potential $h(t)=h_0$,\n$$ \\begin{equation} A_0 = \\frac{f(h_0)}{1-\\Delta^{\\mathrm{abs}} f(h_0)} \\equiv g(h_0). \\end{equation} $$\nTo solve the population activity for homogeneous, isotropic and stationary network, all we need is the property of single neuron.\nTime Coarse-Graining Remove the integral.\n","permalink":"/snm/19.population-activity/","tags":null,"title":"19.Population Activity"},{"categories":null,"contents":"What does the renewal process describe? Replacement of component.\nfailure time A population of components, the failure time of each component is characterized by a non-negative random variable X. The failure time is in fact the age of the component, defining when the failure occurs.\nThe distribution of X could be either discrete or continuous. Discrete: X~{0,h,2h,3h,\u0026hellip;} Continuous: the probability is determined by a probability density function (pdf) over the range of (0,$\\infty$).\nprobability density function of X $$f(x)=\\lim_{\\Delta x \\to 0_+} \\frac{\\mathrm{prob}(x\u0026lt;X\u0026lt;x+\\Delta x)}{\\Delta x}$$\nwith\n$$\\int_0^{\\infty} f(x) dx =1.$$\nAnd the failure times are independent.\nOther functions cumulative distribution function $F(x)$:\n$$F(x) = \\mathrm{prob}(X\u0026lt;=x) = \\int_-^x f(u) du.$$\nand $f(x)=F\u0026rsquo;(x)$\nsurvivor function $\\mathscr{F}(x)$: $$\\mathscr{F}(x)= \\mathrm{prob(X\u0026gt;x)}\\\n= 1-F(x)\\\n= \\int_x^{\\infty} f(u) du$$\n$f(x)=-\\mathscr{F}'(x)$\nhazard function: $\\phi (x)$ the probability of almost immediate lailure of a component at age $x$.\n$$\\mathrm{prob} (A|B) = \\frac{\\mathrm{prob}(A \\mathrm{and} B)}{\\mathrm{prob} (B)}$$\nso, $$\\phi (x) = \\lim_{\\Delta x\\to 0+} \\frac{\\mathrm{prob(x\u0026lt;X\u0026lt;=x+\\Delta x)}}{\\Delta x} \\frac{1}{\\mathrm{prob(x\u0026lt;X)}}\\\n= \\frac{f(x)}{\\mathscr{F}(x)}$$\nDiscrete time: Life table events A life table consists of a list of the number of individuals, usually from an initial group of 1000 individuals so that the numbers are effectively proportions, who survive to a given age in a given population.\nImportant parameters:\n$\\mathscr{l}_x$: surviving to age $x$\n$d_x$: dying between age x and x+1\n$d_x = \\mathscr{x}-\\mathscr{x+1}$\n$q_x$: those surviving to age $x$ who die before reaching age $x+1$ $q_x = d_x/\\mathscr{l}_x$\nneural spike train In the presence of noise, the spike train generation is a stochastic point process, not deterministic. Hence the probability of generating the next event (spike), depends only on the “age” $t−\\hat t$ of the system, i.e., the time that has passed since the last event (last spike).\nThe central assumption of renewal theory is that the state does not depend on earlier events .\n","permalink":"/snm/20.basics-of-renewal-theory/","tags":null,"title":"20.Basics of Renewal Theory"},{"categories":null,"contents":"Review of Concepts  Population activity $A(t)$: fraction of neurons fired per unit time at time $t$.\n  Interval distribution $P_I(t-\\hat t\\vert t)$: given external input $I$, the\n  Mean firing rate $\\nu$: reciprocal of mean firing interval $\\langle T\\rangle$\n$$ \\begin{equation} \\nu = \\frac{1}{\\langle T \\rangle} = \\int s P(s) ds, \\end{equation} $$\nwhere $s$ is the interval between firing.\n  Asynchronous Firing Asynchronous firing is a state when the population activity $A(t)=A_0$ is a constant of time. In this section we deal with homogeneous network and constant external input.\n Whether it is possible to have equilibrium if we apply variant external input is still up for discussion.\n Equilibrium means that we need only the parameter $s=t-\\hat t$ to describe each neuron. Hence we simplify the probabilities\n$$ \\begin{align} P_I(t-\\hat t\\vert \\hat t) \u0026amp;= P_0(s), \\\nS_I(t-\\hat t\\vert \\hat t) \u0026amp;= S_0(s). \\end{align} $$\nThis corresponds to equilibrium state of the whole system. In statistical physics, we have the concept of equal priori probability 1. The question is, does it hold for neural networks?\nThe first question, though, is how to describe a microstate of a neural network. To simplify the idea, we discuss equilibrium systems. We could easily deny the idea of using spiking states, since it obviously gives much less information we should have.\nMy idea is to use the time lapsed after the spike $s = t-\\hat t$. If we can know exactly the $s$ of each neuron for an equilibrium state or asynchronous firing, we can determine the firing of the system probabilistically or on average.\nHere is an example. Suppose we have 4 neurons in a network, and the system reaches equilibrium.\n| Neuron Number | 1 | 2 | 3 | 4 | | $s^{(1)}$ | 0.1 | 0.7 | 0.4 | 0.2 | | $s^{(2)}$ | 0.5 | 0.8 | 0.3 | 0.4 |\nThe indices ${}^{(i)}$'s are the encoding of ensembles. Do they have equal probability?\nNO! This is why we have the interval distribution!\nThis either means we do not have equal priori probability principle or I didn\u0026rsquo;t find the correct way to describe all the accessible states of the system.\nHowever, another notion that bothers me is that in statistical physics, we have Avogadro number of particles so that the fluctuations $1/\\sqrt{N}$ is small. What about neural networks? How do we justify the analogy of equilibrium state/ensemble?\nIs $A_0$ the same as mean firing rate? Naively, we would expect that $A_0$ equals the mean firing rate since we have only one time scale for the whole system which is the mean firing interval and $A_0$ has dimension $1/[T]$. Here is a handwaving argument.\n$$ \\begin{align} \u0026amp;\\text{Total number of neurons} \\cdot A_0 \\cdot \\Delta t \\\n\\equiv \u0026amp; \\text{Number of neurons firing during time interval $\\Delta t$} \\\n\\equiv \u0026amp; \\text{Total number of neurons} \\cdot \\frac{\\Delta t}{\\text{Mean firing interval of each neuron} } \\end{align} $$\nThe last step is an educated guess. We are pretty sure about this forumla because we can take the limit that $\\Delta t=\\text{Mean firing interval}$ and all neurons have constant firing interval.\nTo prove this conjecture, we need to investigate the equations for $A_0$. We consider the normalization equation\n$$ \\int_{-\\infty}^t A_0 S_0(t-\\hat t) d\\hat t = 1. $$\nChange integration variable from $\\hat t$ to $s$, which are related through $\\hat t=t-s$,\n$$ \\int_{-\\infty}^t A_0 S_0(t-\\hat t) d\\hat t = \\int_0^{\\infty}A_0 S_0(s)ds. $$\nWe integrate by parts\n$$ \\begin{align} \u0026amp;\\int_0^{\\infty}A_0 S_0(s)ds \\\n=\u0026amp; A_0 \\int_0^\\infty d(S_0 \\cdot s) - A_0 \\int_0^\\infty s d S_0 \\\n=\u0026amp; A_0 \\cdot s\\cdot S_0(s)\\vert_0^\\infty - A_0 \\int_0^\\infty s \\frac{dS_0}{ds} ds\\\n=\u0026amp; A_0 \\int_0^\\infty s P_0(s) ds \\end{align} $$\nIn the last step, we used\n$$ \\begin{equation} \\frac{dS_0(s)}{ds} = - P_0(s). \\end{equation} $$\nThe term $\\int_0^\\infty s P_0(s) ds$ is exactly the definition of mean interspike interval $\\langle T\\rangle$. So we have proved\n$$ \\begin{equation} A_0 \\langle T\\rangle = 1. \\end{equation} $$\nNumerically, they show that the averaged fluctuations $\\langle \\lvert \\bar A(t) - A_0 \\rvert^2 \\rangle$ decrease as the number of neurons increase. Fig. 6.9.\nGain function and fixed points Gain function: $1/\\langle T \\rangle$ as a function of input current.\n$$ \\begin{equation} A_0 = g(I_0), \\end{equation} $$\nwhere\n$$ I_0 = I^{\\mathrm{ext}}_0 + I^{\\mathrm{int}}_0. $$\nThe subscript $0$ indicates that we are talking about stationary cases. If we apply SRM0 model,\n$$ \\begin{equation} h(t) = J_0 A_0 \\int_0^\\infty \\epsilon_0(s) ds + I^{\\mathrm{ext}}_0 \\int_0^\\infty \\kappa_0 (s)ds \\end{equation} $$\nbecomes time independent.\nSince $\\int_0^\\infty \\epsilon_0(s) ds$ is time independent, we can redefine a new quantitity\n$$ J_0^{(1)} \\equiv J_0 \\int_0^\\infty \\epsilon_0(s) ds. $$\nMeanwhile, our observation tells us that $\\int_0^\\infty \\kappa_0 (s)ds$ is a effective resistance.\nThus we obtain\n$$ h = J_0^{(1)} A_0 + I^{\\mathrm{ext}}_0 R. $$\nTo obtain the current, we devided the equation by $R$,\n$$ I_0 = J_0^{(2)} A_0 + I^{\\mathrm{ext}}_0, $$\nwhere\n$$ J_0^{(2)} = J_0^{(1)} /R. $$\nFor convinience we call just drop all these superscripts and name $J_0^{(2)}$ as $J_0$, i.e.,\n$$ I_0 = J_0 A_0 + I^{\\mathrm{ext}}_0. $$\nThe gain function becomes\n$$ A_0 = g(J_0 A_0 + I^{\\mathrm{ext}}_0). $$\nThis equation seems to be complicated. A graphical method is provided in the textbook. An easy way to understand this method is to rewrite the equation into two,\n$$ \\begin{align} A_0 = \u0026amp; g(I_0) \\\nA_0 =\u0026amp; \\frac{I_0 - I^{\\mathrm{ext}}_0 }{J_0}, \\end{align} $$\nwhich is equivalently\n$$ \\begin{equation} \\frac{I_0 - I^{\\mathrm{ext}}_0 }{J_0} =g(I_0) \\label{eq-graphical-solution-illustration} \\end{equation} $$\nIf we plot $A_0$ as a function of $I_0$, the solution to the system of equations should be where the LHS = RHS in Eq.(\\ref{eq-graphical-solution-illustration}). More explicitly, we plot out LHS and RHS as a function of $I_0$ and the intersection is the solution.\nLow Connectivity Networks Same neurons but the connections are not full.\n Each neuron takes input from $C$ neurons. Sparse connectivity: $C/N \\gg 1$. Sparse also means any two neurons i and j hardly take input from the same neurons. IF the presynaptic signals are stochastic, the neurons take stochastic inputs, thus described by diffusive noise.  How can we be sure that the presynaptic signals are stochastic? This loophole is solved by proving that the system has such solutions.\nAssuming two types of populations, excitatory neurons $N_E$ and inhibitory neurons $N_I$, each neuron has connectivity $C_E$ from $N_E$ and $C_I$ from $N_I$.\n$$ \\begin{equation} h_0 = R I^{\\mathrm{ext}} + \\tau_m \\sum_j \\nu_j w_j \\end{equation} $$\nWe have\n$$ \\begin{align} w_j = \u0026amp; w_E C_E + w_I C_I \\\n=\u0026amp; w_E C_E (1 - g\\gamma), \\end{align} $$\nwhere\n$$ \\begin{align} g = \u0026amp; - \\frac{w_I}{w_E} \\\n\\gamma = \u0026amp; \\frac{C_I}{C_E}. \\end{align} $$\nSimilarly,\n$$ \\begin{align} \\sigma^2 = \u0026amp; \\tau_m \\sum_j v_j w_j^2 \\\n=\u0026amp; \\tau_m v (w_E^2 C_E + w_I^2 C_I) \\\n=\u0026amp; \\tau_m v w_E^2 C_E^2 (1+g^2\\gamma). \\end{align} $$\nThen we can calculate the population activity: Eq. 6.110, which is a function that depends on $h_0$ and $\\sigma$. Hence we can find population activity given all the parameters:\n Threshold $\\theta$ Life time $\\tau_m$ $w_E$ $C_E$ $g$ $\\gamma$ External $I^{\\mathrm{ext}}$ Resistance $R$  The book has two examples of verifying that in some conditions such randomly connected sparse network is equivalent to noise\nReferences and Notes  Weiss Theory of Ferromagnetism    For some explaination of this concept, please read The principle of equal a priori probabilities and Probability calculations. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"/snm/21.asynchronous-firing/","tags":null,"title":"21.Asynchronous Firing"},{"categories":null,"contents":"interacting populations Every heterogeneous network could be viewed as a combination of different homogeneous network, or the interaction among different populations. The simplest case is the balanced random network: with excitatory and inhibitory neurons.\nLet\u0026rsquo;s look at a fully connected network with excitatory and inhibitory neurons. Excitatory neurons belong to population $n$ and inhibitory neurons belong to population $m$.\nSo the activity of population $n$ or population $m$ is:\n$$ A_n(t)=\\frac{1}{N_n}\\sum_{j\\in \\Gamma_n}\\sum\\delta(t-t_j^{(f)}) $$\nSo for each single neuron in population $m$, which receives the inputs from all neurons in population $n$, the membrane potential is changing accordingly:\n$$ \\begin{align} h_i(t|\\hat{t_i}) \u0026amp; = \\sum_j\\sum_fw_{ij}\\eta(t-\\hat{t_i},t-t_j^{(f)})\\\n\u0026amp; = \\sum_mJ_{nm}\\int_0^{\\infty}\\eta(t-\\hat{t_i},s)\\sum_{j\\in \\Gamma_m}\\sum_f\\frac{\\delta(t-t_j^{f}-s)}{N_m}\\\n\u0026amp; = \\sum_mJ_{nm}\\int_0^{\\infty}\\eta(t-\\hat{t},s)A_m(t-s)ds\n\\end{align} $$\nThat is just an example population with SRM0 neuron model. For a general population, the population activity evolution could be rewrite as:\n$$ A_n(t) = \\int_{-\\infty}^t P_n(t|\\hat{t})A_n(\\hat{t})d\\hat{t} $$\n$P$ is the interval distribution, it could also be viewed as a kernel for population activity, which is determined by the post-synaptic potential (PSP).\nStationary states and gain function of interacting populations When the network reaches stationary state, the activity is stable, so the inputs from one sub-population ($N$ for example) to another sub-population is more or less constant.\nSo the inputs could be viewed as $I_m$ (to population $M$), and the activity of this population is: $$ A_m=g_m(I_m), $$\nwith\n$$ I_m = \\sum_nJ_{mn}A_n, $$\nwe have\n$$ A_m = g_m(\\sum_n J_{mn}A_n) $$\nspatial continuum network Features: the auditory cortex is a good example, with neurons organized along an axis with continuously changing preferred frequency.\nSo the principle of analyzing this network is \\emph{binning} the cortex with a binsize $d$. Each sub-population could be labeled according to its spatial location. For instance, population $N$ is the neurons between $[nd,(n+1)d]$, and in each bin, the neuron number is $N_n=\\rho d$, where $\\rho$ is the spatial density.\nThen this network is a special case of interacting populations, with $d$ approaching zero.\n","permalink":"/snm/22.interacting-populations-and-continuum-models/","tags":null,"title":"22.interacting populations and continuum models"},{"categories":null,"contents":"This is the first section of chapter 7, which is about signal transmission and neuronal coding, i.e., reaction of population of neurons to different inputs.\nLinearized Population Equation In this section, we linearize the population equation. More specifically, we develop a perturbation theory of the population equations.\n Input: $I(t)=I_0+\\Delta i(t)$. The input is restrained to be a small perturbation upon a constant input. Population activity: $A(t) = A_0 + \\Delta A(t)$. We expect the output to be also not changing dramatically. This might not be true if the equation is non-linear, especially for integro-differential equations. In the examples given in Sec. 6.3, this perturbative output assumption is always true. HOWEVER, WE SHOULD DEFINITELY TAKE NOTE ON THIS CAVEAT.  From Eq. 6.75, we can derive a \u0026ldquo;equation\u0026rdquo; that shows how this method works.\n$$ \\begin{align} \u0026amp; A(t) = \\int_{-\\infty}^t P_I(t|\\hat t) A(\\hat t) d\\hat t\\\n\\Rightarrow \u0026amp; A_0 + \\Delta A(t) = \\int_{-\\infty}^t P_I(t|\\hat t) A_0 d\\hat t + \\int_{-\\infty}^t P_I(t|\\hat t) \\Delta A(\\hat t) d\\hat t\\\n\\Rightarrow \u0026amp; \\Delta A(t) = A_0 ( -1 + \\int_{-\\infty}^t P_I(t|\\hat t) d\\hat t ) + \\int_{-\\infty}^t P_I(t|\\hat t) \\Delta A(\\hat t) d\\hat t. \\label{eqn-perturbation-1} \\end{align} $$\nWe would also expect the interval distribution is perturbative under small perturbations of input current, i.e.,\n$$ \\begin{equation} P_I(t\\vert \\hat t) = P_{I_0}(t\\vert \\hat t) + \\Delta P_{I_0}(t\\vert \\hat t). \\end{equation} $$\nFor a perturbation method, we drop all terms with high orders. Thus Eq. ($\\ref{eqn-perturbation-1}$) becomes\n$$ \\begin{equation} \\Delta A(t) = A_0 \\left( -1 + \\int_{-\\infty}^t P_I(t|\\hat t) d\\hat t \\right) + \\int_{-\\infty}^t P_{I_0}(t|\\hat t) \\Delta A(\\hat t) d\\hat t. \\label{eqn-perturbation-2} \\end{equation} $$\nSome Simplifications How to calculate the integral $\\int_{-\\infty}^t P_I(t\\vert\\hat t) d\\hat t$? For models without noise, we should have\n$$ P_I(t\\vert \\hat t) = P_I(t-\\hat t), $$\nwhich means the spikes depends on the time interval since last spike. The integral $\\int_{-\\infty}^t P_I(t\\vert\\hat t) d\\hat t$ becomes\n$$ \\begin{align} \u0026amp;\\int_{-\\infty}^t P_I(t\\vert\\hat t) d\\hat t\\\n=\u0026amp; - \\int_{\\hat t = -\\infty}^{\\hat t=t} P_I(t - \\hat t) d (t - \\hat t ) \\\n=\u0026amp; - \\int_{s = \\infty}^{s=0} P_I(s) ds\\\n=\u0026amp; \\int^{\\infty}_{0} P_I(s) ds. \\end{align} $$\nEq. ($\\ref{eqn-perturbation-2}$) reads\n$$ \\begin{equation} \\Delta A(t) = A_0 \\left( -1 + \\int^{\\infty}{0} P_I(s) ds \\right) + \\int{-\\infty}^t P_{I_0}(t|\\hat t) \\Delta A(\\hat t) d\\hat t. \\label{eqn-perturbation-3} \\end{equation} $$\nMore Simplifications The problem is reduced to the calculation of $P_I(t\\vert\\hat t)$. Recall that interval distribution $P_I$ is in fact the time derivative of the survivor function (Sec. 5.2.3). The subscript ${}_I$ indicates the current.\nSome review of the definitions.\n  Interval distribution is defined as\n$$ \\begin{equation} \\int_{t_1}^{t_2} dt P_I(t\\vert \\hat t) = \\text{Probability of firing during time interval $[t_1,t_2]$ when the neuron fired at $\\hat t$}. \\end{equation} $$\n  Survivor function is\n$$ \\begin{equation} S(t\\vert \\hat t) = 1 - \\int_{\\hat t}^t P_I(t\\vert \\hat t) dt. \\end{equation} $$\nWe also derived\n$$ \\begin{equation} P_I(t\\vert \\hat t) = - \\frac{d}{dt} S_I(t\\vert \\hat t). \\end{equation} $$\n  Hazard function is\n$$ \\begin{equation} \\rho(t\\vert\\hat t) = -\\left(\\frac{d}{dt} S(t\\vert \\hat t)\\right)/S(t\\vert \\hat t) = - P_I(t\\vert \\hat t)/S_I(t\\vert \\hat t). \\end{equation} $$\nMeanwhile\n$$ \\rho(t\\vert \\hat t) S(t\\vert \\hat t) = P_I(t\\vert \\hat t). $$\n   I have no idea how to proceed from the Eq. ($\\ref{eqn-perturbation-3}$).\n Anyways, the book postulated a solution with multiple integrals,\n$$ \\begin{equation} \\Delta A(t) = \\int_{-\\infty}^t P_{I_0}(t-\\hat t) \\Delta A(\\hat t)d\\hat t + A_0 \\frac{d}{dt}\\int_0^\\infty \\mathcal{L}(x) \\Delta h(t-x) dx, \\label{eqn-perturbation-final} \\end{equation} $$\nwith\n$$ \\Delta h(t) = \\int_0^\\infty \\kappa(s) \\Delta I(t-s)ds. $$\nInterpretations   The first term in Eq. ($\\ref{eqn-perturbation-final}$) describes the dependence on the previous population activity perturbation.\n  The second term is the dependence on the input variations.\n  Low noise limit: kernel $\\mathcal L(x) \\to \\delta(x)$ (Eq. 6.75 in the book)\n$$ \\begin{equation} \\Delta A(t) = \\cdots + A_0 \\frac{d}{dt} \\Delta h(t) = \\cdots + A_0 \\int_0^\\infty ds , \\kappa (s) \\frac{d}{dt} \\Delta I(t-s). \\end{equation} $$\nThis amazing result tells us that any fast change in the input current leads to a large jump in the population activity, regardless of the amplitude of the input current.\n  Large noise limit: noise is critical.\n Slow noise (adiabatic limit): the response of the system is fast enough to keep track on the noise at any time. This is similar to noise-free limit with noise as the effective input or something. Fast changing input means large change in population activity. Fast noise: the system is too slow to track the noise. The noise is ambient so that the kernel $\\mathcal{L}(x)$ becomes broad. In a limit that the kernel is flat, the population activity depends on the amplitude of potential $\\Delta h$.     Sec. 7.1.1 proves that the noise-free system indeed depends on the time derivative of the input. Sec. 7.1.2 proves that the escape noise, aka fast noise, indeed provides a broad kernel $\\mathcal{L}(x)$.   Rapid change in input might indicate danger for the host of the neural system. This feature might be useful for the survivability of animals.\n Transients The problem to solve in this section is the time course of population activity under a rapid change of input.\nConsider step-like input\n$$ \\begin{equation} I(t) = \\begin{cases} I_0 \u0026amp;\\qquad t\\leq t_0\\\nI_0+\\Delta I \u0026amp; \\qquad t\u0026gt;t_0, \\end{cases} \\end{equation} $$\nwhich generates step like input potential\n$$ \\begin{equation} \\Delta h(t) = \\begin{cases} h_0 \u0026amp; \\qquad t\\leq t_0 \\\nh_0 + \\Delta I \\int_0^{t-t_0} \\kappa_0(s),ds \u0026amp;\\qquad t\u0026gt; t_0. \\end{cases} \\end{equation} $$\nAs long as the kernel $\\kappa$ is defined, we can obtain the time course of every quantity.\nWe assume the population activity has reached equilibrium (asynchronous firing) before $t_0$. The population activity would also obtain a sudden change\n$$ \\begin{equation} A(t) = \\begin{cases} A_0 \u0026amp;\\qquad t\\leq t_0\\\nA_0+\\Delta A(t) \u0026amp;\\qquad t\u0026gt;t_0. \\end{cases} \\end{equation} $$\nThis change is called transient. During a short time interval $[t_0,t_0+\\Delta t]$, we expect the first term in Eq. ($\\ref{eqn-perturbation-final}$) doesn\u0026rsquo;t contribute because the network has not react to the change at time $t_0$, i.e., $\\Delta A(\\hat t)=0$. We have to solve\n$$ \\begin{equation} \\Delta A(t)\\sim A_0 \\frac{d}{dt}\\int_0^\\infty \\mathcal L(x) \\Delta h(t-x) dx, \\end{equation} $$\nfor $t-t_0\\ll T$, where $T=1/A_0$ is the mean interspike interval.\n As we expected, the short time interval $[t_0,t_0+\\Delta t]$ is short enough if $\\Delta t\\ll 1/A_0$.\n Noise-free Network For noise-free network, averaging over time is equivalent to averaging over populations (ensembles).\nWe can inspect a single neuron. A neuron fired at $t_0$ will fire again at $t_0+T$ with $T$ being determined by $u(t_0+T)=\\theta$. Meanwhile the theory of population gives us the population activity change\n$$ \\Delta A(t) = A_0 \\frac{d}{dt}\\left( \\Delta I \\int_0^{t-t_0} \\kappa_0(s),ds \\right) \\propto A_0\\Delta I \\kappa_0(t-t_0), $$\nfor $t_0\u0026lt; t\u0026lt;t_0+T$.\nThe solution is written as\n$$ \\Delta A(t) = a A_0 \\kappa_0(t-t_0), $$\nwhere $a$ can be derived for different models.\n Does it come from the differentiation of time? Not sure how it is derived.\n  SRM0: $a=R\\Delta I/\\eta'$, where $\\eta$ is the intrinsic response; Integrate-and-fire: $a=R\\Delta I/u'$.    Integrate and fire model:\n$$ \\kappa_0(s) = \\frac{1}{\\tau_m} \\exp\\left( -\\frac{s}{\\tau_m}\\right) \\Theta(s). $$\n  For integrate-and-fire model, we can show that the input potential is\n$$ h(t) = h_0 + R \\Delta I \\left( 1 - \\exp\\left( -\\frac{t-t_0}{\\tau_m} \\right) \\right), $$\nafter the input current change, i.e., $t\u0026gt;t_0$. Within a short period, $h(t)\\sim h_0$. However, as we have derived previously, the population activity change drastically,\n$$ \\Delta A(t) = a\\frac{A_0}{\\tau_m} \\exp\\left( - \\frac{t-t_0}{s}\\right)\\Theta(t-t_0), $$\nfor $t_0\u0026lt;t\u0026lt;t_0+T$.\nFig. 7.5\nTransients with Noise For slow noise, refer to Fig. 7.6A.\nSome random questions:\n How to explain the abrupt transient? Before the change of input, there are always neurons at sub-threshold in the population. Increase the input triggers these neurons immediately. Why does the population activity approach another asynchronous state after a long time? Donno.  For fast noise (standart rate mode, or Wilson-Cowan based model), refer to Fig. 7.6B.\nFor SRM0 with escape noise, refer to Fig. 7.7.\nFor diffusive noise, refer to Fig. 7.8.\n","permalink":"/snm/23.linearized-population-equation/","tags":null,"title":"23.Linearized Population Equation and Transients"},{"categories":null,"contents":"This is a short introduction and rationale to my PhD subject on organized behaviors in neuronal networks, focusing on collective bursting in neuronal cultures.\nContents:\n Introduction to the aEIF model to describe the neuronal dynamics Network topology in cultures Synchronization in homogeneous networks The mean-field model  1. Neuronal dynamics and the aEIF model Principle As described in the previous sessions on neuronal models, we use simplified equations to describe the behavior of neurons. How far should we simplify them? For physicists, the best way is to keep the model 2D, so that you can always represent the evolution of the neuron\u0026rsquo;s state on a piece of paper, but make it discontinuous so that it can also have a chaotic behavior.\nFor that reason, we will here use the adaptive Exponential Integrate-and-Fire model (a.k.a. aEIF or AdEx) [Brette2005]. In addition to being 2D, this model also recovers a large number of biologically relevant features compared to the IAF, the Izhikevitch, or GIF models.\nThe model describes the behavior of a neuron using only two variables: the membrane potential $\\tilde{V}$, and a slow current $\\tilde{w}$, which conveys the capacity of the neuron to adapt, i.e. to modulate its excitability depending on the input it receives.\nEquations The two variables are coupled through the following equations:\n$$ \\begin{equation} \\left\\lbrace \\begin{array}{l} \\tilde{C}_m \\frac{d\\tilde{V}}{d\\tilde{t}} = - \\tilde{g}_L(\\tilde{V}-\\tilde{E}_L) + \\tilde{g}_L \\tilde{\\Delta}T \\exp\\left(\\frac{\\tilde{V}-\\tilde{V}{th}}{\\tilde{\\Delta}_T}\\right) - \\tilde{w} + \\tilde{I}_e + \\tilde{I}_s\\\n\\tilde{\\tau}_w \\frac{d\\tilde{w}}{d\\tilde{t}} = \\tilde{a} (\\tilde{V} - \\tilde{E}_L) - \\tilde{w} \\end{array}\\right. \\end{equation} $$\nas in the integrate-and-fire model, the spike is terminated by a reset condition, which reads:\n$$ \\begin{equation} \\text{if } \\tilde{V} \u0026gt; \\tilde{V}_{peak} \\quad \\left\\lbrace \\begin{array}{r c l} \\tilde{V} \u0026amp;\\leftarrow \u0026amp; \\tilde{V}_r\\\n\\tilde{w} \u0026amp;\\leftarrow \u0026amp; \\tilde{w} + \\tilde{b} \\end{array}\\right. \\label{eq:AdEx_adim} \\end{equation} $$\nFor simplicity, I will use dimensionless variables in the rest of the session. The equations will thus become:\n$$ \\begin{equation} \\left\\lbrace \\begin{array}{l} \\dot{V} = - (V-E_L) + e^{V} - w + I_e + I_s\\\n\\tau_w \\dot{w} = a (V - E_L) - w \\end{array}\\right. \\end{equation} $$\nSee Appendices \u0026ndash; Variable change for details.\nBehavior This model is able to reproduce almost all behaviors observed in electrophysiological recordings, from fast-spiking to intrisically bursting or chattering neurons [Naud2008].\n2. Neuronal networks in cultures Now that we have the neuronal model, let us have a look at the structure that will underly the system.\\\nThe neurons are coupled by synapses, so this structure can be represented simply by a directed network, described by an adjacency matrix A, where: $$ A_{ij} = s_{ij} \\delta(i \\rightarrow j) $$ i.e. the matrix entry is non-zero only of neuron $i$ extends an axon towards $j$, in which case it\u0026rsquo;s value represents the synaptic strength of the connection.\nIn cultures, the connectivity is much simpler than in the brain, since the structure is planar (at least in the cultures we study). Though the network cannot be considered as fully homogeneous, as what was considered in previous sessions, some studies [Cohen2010] suggest that its heterogeneity is rather limited.\nIndeed, it seems that the in-degree connectivity can described by a Gaussian distribution $\\mathcal{N}(\\overline{k}, \\sigma_k)$, with an average degree of the order of 100.\nFor a given average connectivity $\\overline{k}$, the Gaussian in-degree (GID) network has the advantage of allowing us to test the influence of heterogeneity by varying $\\sigma_k$.\n3. Synchronization in homogeneous networks Let us try to understand simply how synchronization can occur in networks containing oscillators.\nSimulations: what does it look like? The Kuramoto paradigm One of the most well-known models for this is the Kuramoto model [Acebron2005].\nLet a complex oscillator follow any given trajectory in phase-space and let its phase along the trajectory be $\\theta(t)$.\nIn this model, all that we consider is the phase of the oscillator, and the oscillator will interact with the rest of the world only because of the phase delay that might exist between it and some other oscillator. This is denoted, for any oscillator $i$ among $N$, by the equation:\n$$ \\dot{\\theta}i(t) = \\omega_i + \\frac{K}{N}\\sum{j=1}^N sin(\\theta_j(t) - \\theta_i(t)) $$\nwhere $K$ is the non-normalized coupling constant.\nMany studies on the subject exist, so we will just look at how synchronization can occur on a simpler system, where one oscillator sees a large periodic stimulus (e.g. a population of synchronized oscillators). This exercise is detailed in [Strogatz1994] section 4.5.\nThe periodic stimulus has a period $\\Omega$ and a phase $\\Theta$, hence:\n$$ \\dot{\\theta}(t) = \\omega + K sin(\\Theta - \\theta(t)) $$\nlooking at the phase difference $\\varphi(t) = \\Theta - \\theta(t)$, and noting $\\Delta\\omega = \\Omega - \\omega$, we get:\n$$ \\dot{\\varphi}(t) = \\Delta\\omega - K sin(\\varphi(t)) $$\nThus we can attain a permanent phase lock solution for $\\dot{\\varphi} = 0$ if $\\Delta\\omega \\leq K$.\nRelaxation oscillators Though the Kuramoto model allows us to understand qualitatively many features of synchronization, it is quite different from what happens in neuronal assemblies, especially since these continuous phase models are:\n characterized by rather slow synchronizations, whereas the discrete coupling and almost discontinuous dynamics of neurons leads to much faster synchronization phenomena, not leading to zero phase-lag for finite coupling which is very different from the behavior observed in neuronal populations.  As can be seen on Figure 1, when looking at the adaptation variable $w$, bursting neurons display a behaviour which is very similar to that of a relaxation oscillator.\nContrary to the phase oscillator, the relaxation oscillator can have a discontinuous trajectory. This feature is what allows not only the fast transition from the asynchronous to the synchronous phase, but also the existence of a zero phase-lag state for finite couplings.\n4. The analytic mean-field model Our model is based on the hypothesis that many neuronal cultures contain intrisically oscillating neurons (the majority of hte pyramidal neurons according to [Penn2016]), and that the collective bursting behavior is observed even in the absence of inhibitory neurons.\nTherefore, we use the AdEx model to describe periodic spiking adaptive neurons and tried to understand how adaptation alone could explain the existence of the collective dynamics.\nTo that end, we set the persistent current $I_e$ to a positive value (mimicking biological persistent sodium currents), so that the neurons spike periodically even in the absence of external input.\nThe relaxation oscillations of $w$ can be understood as follows: the adaptation variable increases rapidly during the burst towards a peak value $w^∗$, then undergoes a quasi-exponential decrease until it reaches its minimum value $w_{min}$.\\\nSince the increase of $w$ is simply due to the spike-driven adaptation linked to the $b$ increment after a spike, all that\u0026rsquo;s left to understand is what determines the minimal and maximal values for $w$.\nFigure 2 shows the origin of the phenomena: as the persistent current $I_e$ progressively drives the depolarisation of the membrane, $w$ decreases while $V$ increases. Once $w$ goes below the $V$-nullcline, the potential can diverge and the first spike occurs: the burst begins.\nDuring the burst, the neuron receives an average input from the other neurons, which drives a subsequent series of burst. This phenomenon comes to an end when $w$ reaches a threshold value given by the position at which the spike trajectory (red line + arrow) will intersect the $V$-nullcline (red cross), thus preventing the spike divergence to occur and leading to the termination of the burst.\nUsing a detailed mean-field model, we could translate this condition to a dimensionless self-coherent equation:\n$$ \\begin{equation} w^* = w_{min} + b \\left[ \\overline{t_s}(w^*) - d \\right] + \\overline{k} Q_s. \\end{equation} $$\nwhere $\\overline{t_s}(w^*)$ is the average interspike during a burst, $d$ is the propagation delay of an action potential between the source and target neuron and $Q_s$ is the charge delivered by a spike on the post-synaptic neuron.\nAs $\\overline{t_s}(w)$ is an increasing function of $w$, we understand that the spike-driven adaptation, described by $b$, will have a significant importance on the properties of the collective dynamics.\nReferences **[Acebron2005]**J. A. Acebron, L. L. Bonilla, C. J. P. Vicente, F. Ritort, and R. Spigler, The Kuramoto model: A simple paradigm for synchronization phenomena\n**[Brette2005]**R. Brette and W. Gerstner, Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity\n**[Cohen2010]**O. Cohen, A. Keselman, E. Moses, M. R. Martínez, J. Soriano, and T. Tlusty, Quorum Percolation in Living Neural Networks\n**[Ladenbauer2013]**J. Ladenbauer, M. Augustin, L. Shiau, and K. Obermayer, Impact of adaptation currents on synchronization of coupled exponential integrate-and-fire neurons\n**[Naud2008]**R. Naud, N. Marcille, C. Clopath, and W. Gerstner, Firing patterns in the adaptive exponential integrate-and-fire model\n**[Penn2016]**Y. Penn, M. Segal, and E. Moses, Network synchronization in hippocampal neurons\n**[Strogatz1994]**S. Strogatz, Nonlinear Dynamics and Chaos](http://www.stevenstrogatz.com/books/nonlinear-dynamics-and-chaos-with-applications-to-physics-biology-chemistry-and-engineering)\nAppendices Variable changes for the neuronal model $$ \\begin{eqnarray*} V = \\frac{\\tilde{V}-\\tilde{V}_{th}}{\\tilde{\\Delta}_T}, \u0026amp; \\quad E_L = \\frac{\\tilde{E}_L - \\tilde{V}_{th}}{\\tilde{\\Delta}_T} \\quad \u0026amp; \\text{(general relation for voltages)}\\\nw = \\frac{w}{\\tilde{g}_L \\tilde{\\Delta}_T}, \u0026amp; I = \\frac{\\tilde{I}}{\\tilde{g}_L \\tilde{\\Delta}_T} \u0026amp; \\text{(general relation for currents)}\\\nt = \\frac{\\tilde{t}}{\\tilde{\\tau}_m}, \u0026amp; \\tau_w = \\frac{\\tilde{\\tau}_w}{\\tilde{\\tau}_m} \u0026amp; \\text{(general relation for times)}\\\ng_L = 1 \u0026amp; a = \\frac{\\tilde{a}}{\\tilde{g}_L} \u0026amp; \\text{(general relation for conductances)} \\end{eqnarray*} $$\n","permalink":"/snm/24.from_individual_neurons_to_collective_bursting/","tags":null,"title":"24. From individual neurons to collective bursting"},{"categories":null,"contents":"Terms\n PSTH: peri-stimulus-time histogram, meaning the probability density of firing as a function of time, after the stimulus.  One Input Spike  Some neuron takes constant input $I_0$ and noise $I_{\\mathrm{noise}}$. We inject an extra input on to this neuron.  The factors of importance are\n amount of noise time course of PSP caused by the injection.  Relation between PSP and PSTH. Basically all well expalained in Fig 7.12:\n For large noise, PSTH is similar to PSP, For small noise, PSTH is the derivatives of PSP.  Amazing but why?\nRead Fig 7.11. Consider two scenarios,\n with noise, basically noise will trigger a spike, without noise: related to the derivatives of psp because spike can only occur when the derivative is positive.  Understand the significance using homogeneous population model. Linearized equation is applied\n$$ f_{\\mathrm{PSTH}}(t) = \\frac{d}{dt} \\int_0^\\infty \\mathcal L (x) \\epsilon_0(t-x) dx, $$\nwhere\n$$ \\mathcal L(x) \\sim \\begin{cases} \\delta(x) \u0026amp; \\qquad \\text{low noise limit} \\\n\\text{broad} \u0026amp; \\qquad \\text{high noise} \\end{cases}. $$\nReverse Correlation Reverse correlation: Record the input of the neuron just before it spikes, then average many spikes.\n$$ C^{\\mathrm{rev}}(s) = \\langle \\Delta I(t^{(f)-s}) \\rangle_f, $$\nwhere $\\Delta I$ is the stimulus right before the spike at time $t^{(f)}$.\nReverse correlation is related to correlation function $C$ through\n$$ \\nu C^{rev}(s) = C(s), $$\nwhere $\\nu$ is the firing rate, $\\nu=A_0$.\nWe will find the relation between this reverse correlation and transfer properties of a single neuron, which is described by\n$$ \\hat A(\\omega) = \\hat G(\\omega) \\hat I(\\omega), $$\nWe derive the population activity using the transfer function $\\hat G(\\omega)$\n$$ A(t) = A_0 + \\int_0^\\infty G(s) \\Delta I(t-s) ds. $$\n Fourier transform of multiplications leads to a convolution.\n With the expression of $A(t)$, we could calculate reverse correlation\n$$ \\begin{align} C(s) =\u0026amp; \\lim_{T\\to\\infty} \\frac{1}{T} \\int_0^T A(t+s)\\Delta I(t) dt \\\n=\u0026amp; \\lim_{T\\to\\infty} \\frac{1}{T} \\int_0^T \\int_0^\\infty G(s\u0026rsquo;) \\Delta I(t+s-s\u0026rsquo;) ds\u0026rsquo; \\Delta I(t) dt\\\n=\u0026amp; \\int_0^\\infty ds\u0026rsquo; G(s\u0026rsquo;) \\lim_{T\\to\\infty}\\frac{1}{T}\\int_0^T \\Delta I(t+s-s\u0026rsquo;) \\Delta I(t) dt\\\n=\u0026amp; \\int_0^\\infty ds\u0026rsquo; G(s\u0026rsquo;) \\langle \\Delta I(t+s-s\u0026rsquo;)\\Delta I(t)\\rangle \\end{align} $$\n The reason we dropped the term $A_0$ is because we assume the input is stochastic i.e., $\\langle \\Delta I(t)\\rangle=0$.\n For white noise, we have\n$$ \\langle \\Delta I(t\u0026rsquo;)\\Delta I(t)\\rangle=\\sigma^2\\delta(t\u0026rsquo;-t). $$\nThen we find the relation between reverse correlation and transfer function,\n$$ C^{rev}(s) = \\frac{1}{\\nu}C(s) = \\frac{\\sigma^2}{\\nu} G(s). $$\n","permalink":"/snm/25.significance-of-single-spike/","tags":null,"title":"25. The Significance of Single Spike"},{"categories":null,"contents":"Terms:\n Locking: stable state when all neurons are firing.  Finding Locking in Noise-free populations Verify that locking is a stable solution of the population activity equation.\nIn an ideal model, the consequence of locking is a fully synchronized firing, which leads to a step-like population activity.\nSo we assume the population activity has the form of step-like functions, c.f. Eq. (8.11). The visualization of the expression is shown in Fig. 8.4.\nAt time $t=0$, we have a rectangle. If the rectangle becomes wider, the synchronization will eventually disappear. So for stable synchronization, we would have them becoming more synchronized and obtaining smaller width.\nTo understand Eq. (8.11), we can consider each mode.\nFor $k=0$ mode, we have\n$$ \\frac{1}{2\\delta_0} \\Theta(t-\\delta_0)\\Theta(\\delta_0 - t). $$\nThere are three regions:\n $t\u0026lt;\\delta_0$: it\u0026rsquo;s 0; $t\u0026gt;\\delta_0$: it\u0026rsquo;s 0; $-\\delta_0\u0026lt;t\u0026lt;\\delta_0$: it\u0026rsquo;s 1.  Stable locking of all neurons: related to the instantaneous slope of input potential $h$ at the moment of firing. Eq. (8.12)\n$$ h\u0026rsquo;(T)\u0026gt;0 , \\Rightarrow , A(T) \u0026gt; A(0). $$\nLocking theorem: kind of makes sense.\nExamples of Locking Perfect synchronization in noiseless SRM0. Consider the case where all neurons are fired at $t=0$. Fig. 8.5.\nTo derive Eq. (8.16) from Eq. (8.15), we Taylor expand both sides in terms of small deviations $\\delta_0$ and $\\delta_1$,\n$$ \\begin{align} \u0026amp;\\theta - \\eta(T+\\delta_1-\\delta_0) = J_0 \\epsilon(T+\\delta_1) \\\n\\Rightarrow \u0026amp; \\theta - [ \\eta(T) + \\eta\u0026rsquo;(T)(\\delta_1-\\delta_0) ] = J_0 [\\epsilon(T) + \\epsilon\u0026rsquo;(T)\\delta_1] \\\n\\Rightarrow \u0026amp; - \\eta\u0026rsquo;(T)(\\delta_1-\\delta_0) = \\epsilon\u0026rsquo;(T)\\delta_1 \\\n\\Rightarrow \u0026amp; \\epsilon\u0026rsquo;(T) = -\\frac{ \\eta\u0026rsquo;(T)(\\delta_1-\\delta_0)}{\\delta_1} \\end{align} $$\nFor stability, we need $\\delta_1\u0026lt;\\delta_0$, i.e., the delay for the next firing is smaller than the delay for the first firing.\nSRM0 with inhibitory coupling Fig. 8.6\nLocking with Noise Noise will smear out the spikes. So a system with synchronization have to be with small noise.\nOtherwise the condition should be no different from noiseless case.\nCluster states  Fig. 8.9 Conditions for stable locking for each clustering subgroup: Eq. (8.30) Asynchronous firing: smear out the peaks and distribute spikes evenly in time. Neurons that fire with a delay will be pulled back to the subgroup that just synchronously fired. But neurons that fired earlier than the synchronous firing will not be pulled back to this subgroup but rather the subgroup that fired before. Due to noise, some neurons will drift off one subgroup and be pulled into another subgroup.  ","permalink":"/snm/27.synchronized-oscillations-and-locking/","tags":null,"title":"27. Synchronized Oscillations and Locking"},{"categories":null,"contents":"Outlines  Biological neuron networks have reverberating loops; inferior olive (IO). Periodic large-amplitude oscillations can happen even with each individual neurons firing at a significantly smaller rate or of irregular spike trains. Nicely explained in Fig. 8.11 Strong oscillations with irregular spike trains is related to short-term memory and timing tasks. Binary neurons: potential of the ith neuron at time $t_{n+1}$ is determined by the states of other neurons at time $t_n$ $u_i(t_{n+1})=w_{ij}S_j(t_n)$. The state of neuron $S_i(t_n)$ is determined by the potential at time $t_{n}$, $S_i(t_n)=\\Theta(u_i(t_n)-\\theta)$, where $\\theta$ is threshold. Approximate SRM to McClulloch-Pitts neurons with \u0026ldquo;digitized\u0026rdquo; states. For sparsely connect we can approximate the time evolution using independent events and find the probability. Fig. 8.12; The interactions are shown on the top panels. We start from a value of $a_n$, the iteration gives us the result of $a_{n+1}$. Then the next step depends on the value of $a_{n+1}$ so we project it onto the dashed line $a_{n}=a_{n+1}$. Then we use the new $a_n$ value to find the new $a_{n+1}$.  Excitations and Inhibitions Random network with balanced excitations and inhibitions can generate broad interval distributions. Reverberating projections usually have both excitation and inhibitions. McClulloch-Pitts model with both excitations and inhibitions.  Microscopic Dynamics  The simplified model (SRM-\u0026gt;McClulloch-Pitts) doesn\u0026rsquo;t catch all the features, with inhibitory neurons in presence. The limit circle can grow substantially larger as size of the network increases. Information will drain away with noise. Fig. 8.15  ","permalink":"/snm/28.oscillations-in-reverberating-loops/","tags":null,"title":"28. Oscillations in Reverberating Loops"},{"categories":null,"contents":"Outlines General ideas about Hebbian learning rule  Hebbian learning: correlation-based learning Long-term potentiation: persistent increase in the efficacy. Strong high-frequency pulses which evokes spikes will increase the efficacy. The change in synaptic weight will persist many hours, thus long-term potentiation. Joint activity of pre- and post-synaptic, i.e., correlated activity, could increase the synaptic weight. Presynaptic has to precede the postsynaptic in order to increase the rate.  Rate-based Hebbian learning   Locality: change of synaptic efficacy depends only local variables, pre- and post- firing rates, and efficacy.\n$$ \\frac{d}{dt}w_{ij} = F(w_{ij};\\nu_i,\\nu_j). $$\n  Cooperativity: timing of the pre- and post-synaptic activity. Naively we would simply implement a correlation function into $F$. But the book used Taylor expansion. However, I think we should expand over some kind of contant activity that we could have instead of 0\u0026rsquo;s that the book is using. They should give us the same results. In Taylor expansion, the second order is bilinear which contains the property we need. By keeping only the second order expansion, we have\n$$ \\frac{d}{dt} w_{ij} = c^{\\text{corr}}(w_{ij}) \\nu_i \\nu_j. $$\n  We could choose $c^{\\text{corr}}(w_{ij}) = \\gamma (w_{ij}^{\\text{max}} - w_{ij})$. As $w_{ij}$ approaches its max, the learning will saturate.\n  Weigth decays, so we need the first term of Taylor expansion, $c_0(w_{ij})$. We could choose $c_0(w_{ij}) = - \\gamma_0 w_{ij}$.\n  Final form: Eqn. 10.6\n  Competition: growth of synaptic weight is at the expense of weakening of other weights.\n  Other learning rules are possible. Eqn. 10.7\n  Spike-time-dependent Plasticity  If spike trains are applied, we would expect the effect to be a summation of the single spike rules. Eqn. 10.13 and Eqn. 10 14  ","permalink":"/snm/29.hebbian-learning/","tags":null,"title":"29. Hebbian Learning"},{"categories":null,"contents":"Outlines Learning in Rate Models  PCA  Correlation matrix $C_{ij} = \\langle \\xi^\\mu_i \\xi^\\mu_j\\rangle_\\mu$, where $\\xi^\\mu_i$ is the $i$th component of a vector $\\boldsymbol{\\xi}^\\mu$. Covariance matrix $V_{ij} = \\left\\langle (\\xi^\\mu_i - \\langle \\xi^\\mu_i\\rangle)( \\xi^\\mu_j - \\langle \\xi^\\mu_j\\rangle ) \\right\\rangle_\\mu$. Principal components of the vectors are the eigenvectors of the covariance matrix $V$. The first principal component is the direction where the variance is maximal.   Evolution of synaptic weights  A neuron takes $\\mu$ inputs at each time step, which are either 0\u0026rsquo;s or 1\u0026rsquo;s. At each time step, the input forms a $N$ dimensional vector ($N$ input neurons). For a total time step of $p$, we have $p$ $N$ dimensional vectors. At each time step, the weight change according to Hebbian learning rule $\\Delta w = \\gamma \\nu^{\\text{p}} \\nu^{\\text{pre}}_i$, where $\\gamma$ is the learning rate. Using linear model of post synaptic rate, $\\nu^{\\text{post}} = \\sum_i w_i \\nu_i^{\\text{pre}}$. The author derived the relation between weight and correlation matrix, as well as the eigenvalues and eigenvectors of it. The growth of the expectation value of weight will be dominated by the first principal component.   Exponential growth of weight mean blowing up in biological systems, which should not happen for a working brain. Thus modified Hebbian learning rule should be used and tested. Here we talk about normalization of weight.  Three key ideas:  Normalize sum of weights or quadratic norm of weights; Multiplicative normalization or subtractive normalization (to make sure that $\\sum_i w_i$ is a constant). Might not be just local learning rules.   Subtractive normalization: $\\Delta w_i = \\Delta \\tilde w_i - \\sum_j \\Delta w_j /N$ so that $\\sum_i \\Delta w_i=0$. Multiplicative normalization: Oja\u0026rsquo;s learning rule as an example. The natural choice of normalization is to divide the weight at each step by its norm.   Neurons of visual system have their receptive fields.  ","permalink":"/snm/30.learning-equations/","tags":null,"title":"30. Learning Equations"},{"categories":null,"contents":"Outlines Learning to be fast The fundamental idea behind this simple model is that predictions of actions before it happens strengthens the weight.\nThe simple model in the book is a neuron that takes input from 20 neurons. The input from these neurons is arranges in time orders. The first neuron fires then the second then the third and so on. Due to the input the post neuron would first at some time. Interestingly, after many repetitions the firing time of the post neuron becomes earlier the the initial test.\nSuppose the post neuron fires around the input of the 9th pre neuron initially. The learning strengthens the weight of neuron 7,8,9 for example. Thus the post neuron will fire early in the next run of the experiment. This requires an asymmetric learning window.\nThis procedure is similar to human learning with experience.\nThe learning window is usually as short as 100ms. However, monkeys can have conditioning that span seconds. This could be related to reverberating oscillations.\nLearning to be precise  Nonlinear Poisson model, $\\nu(u) = \\nu_{\\mathrm{max}} \\Theta(u -\\theta)$, where $\\nu_{\\mathrm{max}}$ is the reliability not the maximum rate. $\\nu_{\\mathrm{max}}\\to\\infty $ means the neuron will first immediately when the threshold is reached, thus noiseless. Here by noise we mean how well synchronized are the inputs. Noiseless indicates that the inputs are all synchronized in time. Combined with a non-Hebbian learning rule. Eqn 12.5 Calculate joint probability of pre and post synaptic firings, by neglecting the correlation between post synaptic firing and a individual presynaptic firing if the postsynaptic neuron takes in a large number of inputs. Fig 12.6: small noise respect the asymmetric learning window we used while large noise smears out the learning windows.  I have an idea of a possible phenomenon. In these models, we have soft bounds that drives the plasticity less efficient when the weight approaches the bounds.\nHowever, imagine the weight was changed to some value larger than the upper bound by some brute force. The network would become unstable. The treatment for such a network is to suppress the weights until everyone is within the reasonable bounds.\nSequence learning  Fig 12.9: a simulation shows that this indeed exist in this artificial network. However, sequence memory requires a lot of neurons and connections. A limited network would break down if too many sequences are stored.  ","permalink":"/snm/31.plasticity-and-coding/","tags":null,"title":"31. Plasticity and Coding"},{"categories":null,"contents":"Basics  Trevor Hastie, Robert Tibshirani, J. F. (2004). The Elements of Statistical Learning (Vol. 99, Issue 466). Springer Science \u0026amp; Business Media. Christpher M. Bishop. (2006). Pattern Recognition and Machine Learning.  Normalizing Flow   Variational Inference with Normalizing Flows\n  Kobyzev, I., Prince, S., \u0026amp; Brubaker, M. (2020). Normalizing Flows: An Introduction and Review of Current Methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1–1.\n  Gregor, K., Danihelka, I., Mnih, A., Blundell, C., \u0026amp; Wierstra, D. (2014). Deep autoregressive networks. 31st International Conference on Machine Learning, ICML 2014, 4, 2991–3000.\n  Improving Variational Inference with Inverse Autoregressive Flow\n  MADE: Masked Autoencoder for Distribution Estimation\n  Masked Autoregressive Flow for Density Estimation\n  Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models\n  Energy-based Models  Pytorch Deep Learning Lectures Pytorch Deep Learning Slides A high-bias, low-variance introduction to Machine Learning for physicists  ML Connections to Biological Networks  Millidge B, Tschantz A, Buckley CL. Predictive Coding Approximates Backprop along Arbitrary Computation Graphs. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.04182  ","permalink":"/cpe/00.references/","tags":["Bayesian","Least Squares","Bootstrap","Maximum Likelihood","Bayesian","Normalizing Flow"],"title":"References for Probability Estimation Club"},{"categories":["Machine Learning","Statistics"],"contents":"Upcoming Events:\n Why this Topic Conditional probability estimation is one of the most fundamental problems in statistics.\n Conditional probability estimation is frequently used in solving both real life and academic problems. One is likely to encounter this problem at some point of their life. If you are inferring, you are probably using conditional probabilities. It is a perspective. There are many models and methods to estimate the conditional probability. We can learn about and from these models and methods. We need a universal model to solve this problem for productivity. A universal model for this task will save us a lot of time and energy. Many machine learning methods are based on conditional probabilities.  Many classifiers Bayesian networks \u0026hellip;    What is Our Approach  Read and Discuss Apply on toy problems  Reading List and References We will update this list on our way forward. Here is a partial list of references.\nAs a start this is an outline of what should be covered.\n What is the conditional probability?  Sampling theory Bayes Representation of a conditional probability   Statistical methods to estimate the conditional probability  The list is enormous. We will only concentrate on the basics.   Tree-based  Tree as \u0026ldquo;clustering\u0026rdquo; method Application on the bike-sharing problem   NN-based  NN as feature transformations Application on the bike-sharing problem   EM Methods Variational Methods Normalizing Flow To be added as we learn more about it  Toy Problems A dataset that can be used both for classification problems and regression problems.\n Bike-sharing data  When and How The discussion is online through Skype/Wechat/Lark.\nPlease join the community:\n the skype group: if you have any problems joining the group, please shoot an email to hi@leima.is  This is a bi-weekly meetup\n Calendar Page, Calendar ics url  Rules:\n Everyone will and shall get their chance to lead the discussion. Interrupt and ask any questions to make sure we all understand the content well. Do not be carried away by the time limit. The next host will pick up the topic on the next discussion.  Tools Timezone tool: World Clock\n","permalink":"/projects/conditional-probability-estimation/","tags":["Probability","Estimation Theory"],"title":"Conditional Probability Estimation"},{"categories":["Machine Learning"],"contents":"We will discuss the following topics.\n Statistical learning formalism of machine learning Neural networks under this formalism Energy-based learning Neural neworks and energy-based learning  Textbooks and References  Shalev-Shwartz, S., \u0026amp; Ben-David, S. (2013). Understanding Machine Learning: From Theory to Algorithms.: chap. 1-7, 20 Lecun, Y. (2016). A Tutorial on Energy-Based Learning.  What will be covered Chapter 1-6, 20 of the book Understanding Machine Learning: From Theory to Algorithms.\nWhen and How The discussion is online through Skype/Wechat. It is usually held during the weekend.\nThis is the so called Russian style seminar.\n Everyone will get their chance to lead the discussion. Interrupt and ask for any questions. Do not be carried away by the time schedule. The next host will pick up whatever has not yet been finished.  ","permalink":"/projects/ml-foundations/","tags":["Introduction","Machine Learning"],"title":"Foundations of Machine Learning"},{"categories":["Neuroscience"],"contents":"What books are we reading?  Spiking Neuron Models: Single Neurons, Populations,Plasticity by Wulfram Gerstner and Werner M. Kistler. An Introduction to the Theory of Point Processes by D.J. Daley and D. Vere-Jones.  When and How The discussion is online through Skype. It is usually held during the weekends.\nMore Please watch the GitHub repo: neuronstar/spiking-neuron-models\n","permalink":"/projects/snm/","tags":["Introduction","Neuroscience"],"title":"Spiking Neuron Models"},{"categories":["Statistical Learning"],"contents":"What books are we reading?  The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman.  When and How The discussion is online through Skype/Wechat. It is usually held during the weekend.\nThis is the so called Russian style seminar.\n Everyone will get their chance to lead the discussion. Interrupt and ask for any questions. Do not be carried away by the time schedule. The next host will pick up whatever has not yet been finished.  At this point, we have five different time zones.\nMore Please watch the GitHub repo: neuronstar/elements-of-statistical-learning for updates.\nAbout typesetting Basically markdown.\nThe easiest way of creating notes is to duplicate one of the previous .md files and make changes to it.\nCode of conduct:\n Create a markdown file with extension .md; Any file name works, however, file names begins with two-digit number would be a nice convention. The markdown file has to include a header session that specifies the meta data. Here is an example. The --- is used to enclose the meta data. The four fields in the example is required.   --- title: \u0026quot;01.Introductions\u0026quot; date: 2016-06-30 author: OctoMiao summary: Some basics of statistical learning ---  The contents of the notes should be written in markdown (kramdown). Figures can be included simply using the markdown syntax ![](../assets/yourfolder/your-image.png). We suggest creating a folder with the same name of your markdown file in the assets folder. Pay attention to the path of the image. Eventhough the assets folder is under the same path as the file, we have to add ../ to the path because jekyll will generate a folder out of this markdown file. For images, markdown syntax is what really works. However, an image with caption is best formatted by the following code. ![](../assets/misc/timezones-1.jpg) For other syntax, refer to typography of neuronstar.  ","permalink":"/projects/esl/","tags":["Introduction","Machine Learning"],"title":"The Elements of Statistical Learning Reading Club"},{"categories":null,"contents":"Least Angle Regression  Algorithm 3.2 We want to change $\\beta$ so that the prediction is closer to data $y$, i.e., we require the change of $\\beta$ decreases $X\\beta = y - r$. So the change should be $\\propto X^T r $. Why this works? It reduces the MSE. LAR is similar to lasso. Modified LAR Algorithm 3.2a leads to lasso result. LAR(lasso) is efficient. It takes $\\mathrm{min}(p,N-1)$ steps where lasso itself might take more than p steps. LAR and lasso are almost identical if we use the geometric meaning of the algorithms. But when some coefficient crosses 0, the differences pop up.  ","permalink":"/esl/05.least-angle-regression/","tags":null,"title":"05.Least Angle Regression"},{"categories":null,"contents":"Shrinkage methods  Why? Some variables might be redundant. Shrink the model.  Ridge Regression Lasso  Small constraint $t$ cause some of the coefficients reduce exactly to 0: this is variable selection, while producing sparse model. Convex optimization.  Why would lasso leads to exact 0 coefficients?\nWould spot the reason as long as you plot out the constraints and the RSS. Fig. 3.11.\nCompare Lasso and Ridge For sparse models, lasso is better. Otherwise, lasso can make the fitting worse than ridge.\nNo rule of thumb.\nGeneralization Ridge and lasso can be generalized. Replace the distance calculation with other definitions, i.e., $\\sum \\lvert \\beta_j \\rvert^q$.\n $q=0$: subset selection $q=1$: lasso $q=2$: ridge  Smaller $q$ leads to tighter selection.\n{% highlight text %} Plot[Evaluate@Table[(1 - x^(q))^(1/q), {q, 0.5, 4, 0.5}], {x, 0, 1}, AspectRatio -\u0026gt; 1, Frame -\u0026gt; True, PlotLegends -\u0026gt; Placed[Table[\u0026ldquo;q=\u0026rdquo; \u0026lt;\u0026gt; ToString@q, {q, 0.5, 4, 0.5}], {Left, Bottom}], PlotLabel -\u0026gt; \u0026ldquo;Shrinkage as function of L-q norm disance\u0026rdquo;, FrameLabel -\u0026gt; {\u0026quot;!(*SubscriptBox[([Beta]), (i)])\u0026quot;, \u0026ldquo;!(*SubscriptBox[([Beta]), (j)])\u0026quot;}] {% endhighlight %}\n","permalink":"/esl/04.shrinkage-methods/","tags":null,"title":"04.Shrinkage Methods"},{"categories":null,"contents":"Gauss-Markov  \u0026ldquo;Least squares estimates the parameters $\\beta$ have the smallest variance among all linear unbiased estimates.\u0026rdquo; Unbiased estimation is not always good. ridge regression  Proof of 1  Model: $\\theta = a^T \\beta$ Least square estimate of $\\theta$: $\\hat\\theta = a^T \\hat \\beta = a^T ( \\mathbf X^T \\mathbf X )^{-1} \\mathbf X^T \\mathbf y = \\mathbf c_0^T \\mathbf y$ This is unbiased: $E(a^T\\hat\\beta) = a^T\\beta$ Gauss-Markov theorem: If we have any other linear estimator $\\tilde \\theta = \\mathbf c^T \\mathbf y$ and $E(\\mathbf c^T \\mathbf y)=a^T \\beta$, then $Var(a^T\\hat \\beta)\\leq Var(\\mathbf c^T \\mathbf y)$. To prove it we first write down the general form of a linear estimator. Question: is the general form of a linear estimator $\\alpha (X^T X)^{-1} X^T + D$?  Useful functions for the proof:\n  Variance: $Var(X) = E[ (X - \\mu)^2 ]$.\n  On wikipedia\n  MSE: $MSE(\\tilde\\theta) = E( (\\tilde\\theta -\\theta)^2 ) = E( (\\tilde \\theta - E(\\theta) + E(\\theta) - \\theta)^2 ) = E( (\\tilde\\theta - E(\\theta))^2 ) + \\cdots$\n  MSE is $MSE(\\tilde \\theta) = Var(\\tilde theta) + (E(\\theta) -\\theta)^2$ the second term is bias.\n  Least square is good but we can trade some bias to get a smaller variance sometimes.\n  Choices are variable subset selection, ridge regression.\n  Suppose new data is biased from the original data by a value $\\epsilon_0$, the MSE using the original estimator is only the original MSE differed by a constant. Eq. 3.22\n  We always have a larger MSE????? I don\u0026rsquo;t get this.\n  Multiple Regression  Model: $f(X) = \\beta_0 + \\sum_{j=1}^p X_j \\beta$ For multiple dimensional inputs, the estimator has no correlations for different features.  ","permalink":"/esl/03.gauss-markov-theorem/","tags":null,"title":"03.Guass-Markov Theorem and Multiple Regression"},{"categories":null,"contents":"Some Interesting Points  Geometrically speaking, linear regression methods finds the closest path from the true data to a hypersuface spanned by the data vectors. By definition, each set of data is viewed as a basis vector. The so called closed path to the hypersuface is basically the path that is perpendicular to the surface. Thus we know the prediction we are looking for is a projection of true data onto the hypersuface. The argument above also indicates that degenerate data set, which contains data of the same direction, could cause problems since we have a redundant basis. Distribution of the parameters can be obtained for some categories of data. It might be a normal distribution. t-distribution, aka student\u0026rsquo;s t-distribution, is a category of distributions describing the deviation of estimated mean in a normal distribution from the true mean. The tail of the estimated distribution approaches the actual tail distribution as the sample size increases. Z score can be used to test the significance of the statistics.  \u0026ldquo;Roughly a Z score larger than two in absolute value is significantly nonzero at the p=0.05 level.\u0026rdquo; The author said in the caption of Table 3.2\n  F statistic  Confusion:\n Eqn 3.14: plug in the definition of z and read again.  ","permalink":"/esl/02.linear-methods-for-regresssion/","tags":null,"title":"02.Linear Methods for Regression"},{"categories":null,"contents":"Review Skeleton notes\nAbbreviations  MSE: mean squared error EPE: expected prediction error RSS: sum of squares  Notations Fonts:\n Vectors or scalars are denoted by italic math font $X$. Components of vectors are denoted by subscripts $X_i$. Matrix is denoted by math bold font $\\mathbf X$.  Symbols\n $X$ for input variables; $Y$ for quantitative output; $G$ for qualitative output; $\\hat {}$ for prediction.  Least Squares and Nearest Neighbors Least Squares Least square model:\n$$ \\hat Y = X^T \\hat \\beta. $$\nResidual sum of squares (RSS):\n$$ \\mathrm{RSS}(\\beta) = (\\mathbf y - \\mathbf X \\beta)^{\\mathrm T} (\\mathbf y - \\mathbf X \\beta). $$\nThe parameters we need is the set that minimizes RSS, which requires\n$$ \\frac{d}{d\\beta} \\mathrm{RSS} = 0. $$\nSo we can solve the parameters easily.\nNearest-Neighbor  For input data $x$, calculate the Euclidean distance between $x$ and other input data $x_j$. Choose the $k$ nearest neighbors based on the distance. Output prediction is determined by average of the corresponding outputs of the selected inputs. $$ \\hat Y = \\frac{1}{k} \\sum_{N_k} y_i. $$  For the calculation of distance, metric must be implemented. The book used examples of Euclidean metric. Another metric that can be inspiring is the hyperbolic space. I talked about this in our reading club. {: .notes\u0026ndash;info}\nFor Which Scenario  Least squares: Gaussian-like data set; Nearest-Neighbor: mixture of Gaussians.  Mixture of Gaussians can be described by generative model. I am not really sure what that is. It seems to me that the final data is basically generated from Gaussians of different parameters which are generated randomly. {: .notes\u0026ndash;warning}\nStatistical Decision Theory  Given input $X$ and output $Y$; Following a joint distribution $\\mathrm{Pr}(X,Y)$; Based on input and output, we look for a function that predicts the behavior, i.e., $\\hat Y = f(X)$; How well the prediction is is defined by squared error loss $L(Y,\\hat Y) = (Y-\\hat Y)^2$. With the distribution, we predict the expected prediction error (EPE) as $$ \\mathrm{EPE}(f) = E[ ( Y- \\hat Y )^2 ] = \\int (y - f(x))^2 \\mathrm{Pr}(dx, dy). $$ The book derived that the best prediction is $f(x) = E(Y\\vert X=x)$. Different loss functions lead to different EPE\u0026rsquo;s.  Question: Can we simply solve the probability distribution and find out the function of prediction? The conclusion says the best prediction of $Y$ is the conditional mean. Is it effectively solving $Y$ from the probability distribution? {: .notes\u0026ndash;warning}\nNearest-Neighbor  The best prediction based on EPE is conditional mean, Eq. 2.13; Both $k$ nearest neighbor and linear regression fits into this framework; Additive models: basically turn the linear $x^T\\beta$ into a function of $f_j(X_j)$. The summation still holds. The best prediction based on expectation only is conditional median. Categorical variable $G$ also follows the same paradigm but with different loss function. A choice of loss function for categorical case is a matrix. It has to be a matrix because we have to specify penalties a given prediction class compared to the output class. The dimension of this matrix should be the number of categories. It is rank 2.   0 neighbor indicates an exact classification for the sample data but without the implementation of expectation values at each point since there is only one value at that point in one set of sample data; $k$ nearest neighbor assumed that expectation around a small patch of a point is identical to expectation at the exact point with the corresponding distribution. In Monte Carlo method, calculation of volume in high dimension converges very slowly. The reason is that we need a very large number of sampling points since the dimension is high. The procedure is multiplicative. The same thing might happen here. $k$ nearest neighbor is basically some kind of averaging procedure of the volume density. It requires a large number of sample data points to perform an fairly accurate average. The linear regression is basically a first order Taylor expansion of the approximator $f(x)$. $f(x) = x^T\\beta$.  Local Methods in High Dimensions  Curse of high dimensions: edge length of a cube of volume $r$ is $e_p(r) = r^{1/p}$. An extreme example: $(10^{-10})^{1/10} =0.1$. Small volume leads to high variance. Homogeneous sampling doesn\u0026rsquo;t work in high dimensions. Since most points will fall near the edges.  Requires huge number of sample points in high dimensions.  Statistical Models, Supervised Learning and Function Approximation Joint Distribution I didn\u0026rsquo;t not get the point of this subsection. It seems that the authors are talking about whether it is proper to assume the relation between input and output is deterministic. {: .notes\u0026ndash;warning}\nSupervised Learning  learn by example.  Function Approximation  Linear model; Function as basis (Eq. 2.30): $f_\\theta(x) = \\sum h_k(x)\\theta_k$. Examples of function bases are Fourier expansions, sigmoid, etc. Learning through minimizing sum of squares (RSS), or maximum likelihood estimation, etc. Maximum likelihood estimation:  Likelihood: $L(\\theta) = \\sum_{i=1}^N \\log \\mathrm{Pr}_\\theta (y_i)$; Maximized it (\u0026ldquo;probability of the observed sample is largest\u0026rdquo;) Minimizing RSS is equivalent to maximum likelihood estimation. Eq. 2.35.    ","permalink":"/esl/01.statistical-learning-theory/","tags":null,"title":"01.Introductions (Review) and Several Preliminary Statistical Methods"},{"categories":null,"contents":"We host reading clubs and seminars on neuroscience, machine learning, complex networks and intelligence.\nLicense \u0026amp; Source Articles on this website are published under CC BY-NC-SA license if no specific license is designated.\n This website is hosted on GitHub and generated by GitHub Pages (hugo). Computational Neuroscience Map is written in TiddlyMap and hosted statically on GitHub Pages.  ","permalink":"/about/","tags":null,"title":"ABOUT"},{"categories":null,"contents":"What books are we reading?  Spiking Neuron Models: Single Neurons, Populations,Plasticity by Wulfram Gerstner and Werner M. Kistler. An Introduction to the Theory of Point Processes by D.J. Daley and D. Vere-Jones.  When and How The discussion is online through Skype. It is usually held during the weekends.\nMore Please watch the GitHub repo: neuronstar/spiking-neuron-models\n","permalink":"/snm/00.spiking_neuron_models_club/","tags":null,"title":"00.Spiking Neuron Models Reading Club"},{"categories":null,"contents":"What books are we reading?  The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman.  When and How The discussion is online through Skype/Wechat. It is usually held during the weekend.\nThis is the so called Russian style seminar.\n Everyone will get their chance to lead the discussion. Interrupt and ask for any questions. Do not be carried away by the time schedule. The next host will pick up whatever has not yet been finished.  At this point, we have five different time zones.\nMore Please watch the GitHub repo: neuronstar/elements-of-statistical-learning for updates.\nAbout typesetting Basically markdown.\nThe easiest way of creating notes is to duplicate one of the previous .md files and make changes to it.\nCode of conduct:\n Create a markdown file with extension .md; Any file name works, however, file names begins with two-digit number would be a nice convention.  The markdown file has to include a header session that specifies the meta data. Here is an example. {% highlight text %} title: \u0026ldquo;01.Introductions\u0026rdquo; date: 2016-06-30 author: OctoMiao summary: Some basics of statistical learning {% endhighlight %} The --- is used to enclose the meta data. The four fields in the example is required. The contents of the notes should be written in markdown (kramdown). Figures can be included simply using the markdown syntax ![](../assets/yourfolder/your-image.png). We suggest creating a folder with the same name of your markdown file in the assets folder. Pay attention to the path of the image. Eventhough the assets folder is under the same path as the file, we have to add ../ to the path because jekyll will generate a folder out of this markdown file. For images, markdown syntax is what really works. However, an image with caption is best formatted by the following code. {% highlight text %}  For other syntax, refer to typography of neuronstar.  ","permalink":"/esl/00.the-elements-of-statistical-learning/","tags":null,"title":"00.The Elements of Statistical Learning Reading Club"},{"categories":null,"contents":"elements-of-statistical-learning Reading club: The Elements of Statistical Learning\nOnline Course: StatLearning@Standford\n An Introductory Book: http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/  The easiest way of creating notes is to duplicate one of the previous .md files and make changes to it.\nCode of conduct:\n Create a markdown file with extension .md; Any file name works, however, file names begins with two-digit number would be a nice convention. The markdown file has to include a header session that specifies the meta data. Here is an example. --- title: \u0026quot;01.Introductions\u0026quot; date: 2016-06-30 author: OctoMiao summary: Some basics of statistical learning --- The --- is used to enclose the meta data. The four fields in the example is required.\n The contents of the notes should be written in markdown (kramdown). Inline math is enclosed by $, e.g. $\\beta$; Display math is enclosed by $$, e.g. $$\\alpha + \\beta = \\gamma$$. Figures can be included simply using the markdown syntax ![](../assets/yourfolder/your-image.png). We suggest creating a folder with the same name of your markdown file in the assets folder. Pay attention to the path of the image. Eventhough the assets folder is under the same path as the file, we have to add ../ to the path because jekyll will generate a folder out of this markdown file. For other syntax, refer to typography of neuronstar.  ","permalink":"/esl/readme/","tags":null,"title":""},{"categories":null,"contents":" ToC {:toc}  We use kramdown This website uses kramdown as the basic syntax. However, a lot of html/css/js has been applied to generate some certain contents or styles.\nMath also follows the kramdown syntax.\nFootnote Syntax for footnotes is elaborated more on the website of kramdown.\n{% highlight text %} Some text here some other text here.1\nTable of Contents {% highlight text %}\n ToC {:toc} {% endhighlight %}  is used to generate table of contents.\nFigure with Caption {% highlight html %}\n![]({{ site.url }}/assets/programming/chrome-dev-tools-inspect.png) where {{ site.url }} is the configured url of the site.\nNotes div {% highlight html %}\nAlternatively, we can use the set attributes syntax in kramdown.\n{% highlight md %} This is a paragraph with some class. The class is specified in the end of the paragraph. {: .notes\u0026ndash;warning} {% endhighlight %}\nThe results shows as a paragraph with the corresponding class. Notice that this only works for one paragraph.\nThis is a paragraph with some class. The class is specified in the end of the paragraph. {: .notes\u0026ndash;warning}\n  Footnote here {% endhighlight %} \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"/typography/","tags":null,"title":""}]