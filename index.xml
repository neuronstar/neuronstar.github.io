<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NeuronStar</title><link>/</link><description>Recent content on NeuronStar</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 24 Mar 2016 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml"/><item><title>Conditional Probability and Bayes</title><link>/cpe/01.conditional-probability-and-bayes/</link><pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate><guid>/cpe/01.conditional-probability-and-bayes/</guid><description>The Bayesian view of probability is quite objective and also more general than the frequentist&amp;rsquo;s view. It doesn&amp;rsquo;t rely on repeatition of events.</description></item><item><title>01.Neuron Biological Properties</title><link>/snm/01.single_neuron_model/</link><pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate><guid>/snm/01.single_neuron_model/</guid><description>Neuron biological properties</description></item><item><title>Least Squares, Bootstrap, Maximum Likelihood, and Bayesian</title><link>/cpe/02.least-squares-bootstrap-maximum-likelihood-and-bayesian/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>/cpe/02.least-squares-bootstrap-maximum-likelihood-and-bayesian/</guid><description>Least squares, bootstrap, maximum likelihood, and maximum posterior leads to the same results in many cases.</description></item><item><title>02.Review of Last Week's Reading</title><link>/snm/02.limitations_srm_contd_and_coding/</link><pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate><guid>/snm/02.limitations_srm_contd_and_coding/</guid><description>Review of Last Week&amp;rsquo;s Reading</description></item><item><title>EM Methods</title><link>/cpe/03.em-methods/</link><pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate><guid>/cpe/03.em-methods/</guid><description>Topics EM for Gaussian mixtures General EM algorithm Why does it work? Decomposition of log-likelihood into KL divergence and Relation between EM and Gibbs sampling</description></item><item><title>03.Equilibrium Potential and Hodgkin-Huxley Model</title><link>/snm/03.equilibrium_potential_and_hodgkin-huxley_model/</link><pubDate>Sun, 13 Mar 2016 00:00:00 +0000</pubDate><guid>/snm/03.equilibrium_potential_and_hodgkin-huxley_model/</guid><description>Equilibrium Potential and Hodgkin-Huxley Model</description></item><item><title>Variantional Inference Normalizing Flow</title><link>/cpe/04.variational-inference-normalizing-flow/</link><pubDate>Sat, 16 Jan 2021 00:00:00 +0000</pubDate><guid>/cpe/04.variational-inference-normalizing-flow/</guid><description>Topics Variational Inference Normalizing Flow Variational Inference with Normalizing Flows</description></item><item><title>04.The Zoo of ion channels</title><link>/snm/04.the_zoo_of_ion_channels/</link><pubDate>Wed, 23 Mar 2016 00:00:00 +0000</pubDate><guid>/snm/04.the_zoo_of_ion_channels/</guid><description>The Zoo of ion channels</description></item><item><title>Review of Normalizing Flow</title><link>/cpe/05.normalizing-flow-review/</link><pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate><guid>/cpe/05.normalizing-flow-review/</guid><description>Topics Normalizing flow Applications of normalizing flow Methods of normalizing flow Problems of normalizing flow</description></item><item><title>05.Synapse and receptor</title><link>/snm/05.synapse_and_receptors/</link><pubDate>Mon, 28 Mar 2016 00:00:00 +0000</pubDate><guid>/snm/05.synapse_and_receptors/</guid><description>Synapse and receptor</description></item><item><title>Deep AutoRegressive Networks</title><link>/cpe/06.deep-autoregressive-networks/</link><pubDate>Sat, 13 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/06.deep-autoregressive-networks/</guid><description>Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>06.Cable Equation and Its Solutions</title><link>/snm/06.cable_equation_and_its_solutions/</link><pubDate>Sat, 02 Apr 2016 00:00:00 +0000</pubDate><guid>/snm/06.cable_equation_and_its_solutions/</guid><description>Cable Equation and Its Solutions</description></item><item><title>MADE: Masked Autoencoder for Distribution Estimation</title><link>/cpe/07.made/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/07.made/</guid><description>Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>07.Two dimensional neuron models</title><link>/snm/07.reduction_to_two_dimensions_and_phase_plane_analysis/</link><pubDate>Thu, 21 Apr 2016 00:00:00 +0000</pubDate><guid>/snm/07.reduction_to_two_dimensions_and_phase_plane_analysis/</guid><description>Two dimensional neuron models</description></item><item><title>MAF: how is MADE being used</title><link>/cpe/08.maf/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/08.maf/</guid><description>We discussed MAF (arXiv:1705.07057v4) last time: The paper did not explain how exactly is MADE being used to update the shift and logscale.
We will use the tensorflow implementation of MAF to probe the above question. Here is the link to the relevant documentation: https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow
Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>08.Integrate and Fire Models Part 1</title><link>/snm/08.integrate-and-fire-models-1/</link><pubDate>Sat, 21 May 2016 00:00:00 +0000</pubDate><guid>/snm/08.integrate-and-fire-models-1/</guid><description>Integrate and Fire Model Part 1</description></item><item><title>Summary of Generative Models</title><link>/cpe/09.summary-of-generative-models/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/09.summary-of-generative-models/</guid><description/></item><item><title>09.Reduction of the Hodgkin-Huxley model type II</title><link>/snm/09.from_detailed_models_to_formal_spiking_neurons/</link><pubDate>Sat, 25 Jun 2016 00:00:00 +0000</pubDate><guid>/snm/09.from_detailed_models_to_formal_spiking_neurons/</guid><description>Reduction of the Hodgkin-Huxley model &amp;lsquo;type II&amp;rsquo;</description></item><item><title>Energy-based Models</title><link>/cpe/10.energy-based-learning/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/10.energy-based-learning/</guid><description>We will discuss energy-based learning in this session.
References:
Lecture notes: https://atcold.github.io/pytorch-Deep-Learning/[cid:90ae645c-415b-4c9f-8ea6-c78839a8e8d4] https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/ https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2/ https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view : if you can not access Google Drive, this file (007-ebm-01) has been attached to this calendar event too. Supplementary:
https://arxiv.org/pdf/1803.08823.pdf</description></item><item><title>10.Information Coding</title><link>/snm/10.noise_and_renewal_process/</link><pubDate>Sat, 25 Jun 2016 00:00:00 +0000</pubDate><guid>/snm/10.noise_and_renewal_process/</guid><description>Information coding</description></item><item><title>Energy-based Models 2</title><link>/cpe/11.energy-based-learning-2/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>/cpe/11.energy-based-learning-2/</guid><description>We will discuss energy-based learning in this session.
References:
Lecture notes: https://atcold.github.io/pytorch-Deep-Learning/[cid:90ae645c-415b-4c9f-8ea6-c78839a8e8d4] https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/ https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2/ https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view : if you can not access Google Drive, this file (007-ebm-01) has been attached to this calendar event too. Supplementary:
https://arxiv.org/pdf/1803.08823.pdf</description></item><item><title>11.Renewal Theory</title><link>/snm/11.stationary_renewal_theory/</link><pubDate>Tue, 05 Jul 2016 00:00:00 +0000</pubDate><guid>/snm/11.stationary_renewal_theory/</guid><description>Renewal Theory</description></item><item><title>Energy-based Models 3</title><link>/cpe/12.energy-based-learning-3/</link><pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate><guid>/cpe/12.energy-based-learning-3/</guid><description>In the past two meetups, we have been discussing EBM from a computer scientist&amp;rsquo;s perspective.
In this discussion, we will discuss chapter XV of Mehta P, Bukov M, Wang C-HH, Day AGRR, Richardson C, Fisher CK, et al. A high-bias, low-variance introduction to Machine Learning for physicists. Phys Rep. 2018;810: 122. doi:10.1016/j.physrep.2019.03.001</description></item><item><title>09.Escape Noise</title><link>/snm/12.escape_noise/</link><pubDate>Fri, 29 Jul 2016 00:00:00 +0000</pubDate><guid>/snm/12.escape_noise/</guid><description>Escape Noise</description></item><item><title>Energy-based Models 4</title><link>/cpe/13.energy-based-learning-4/</link><pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate><guid>/cpe/13.energy-based-learning-4/</guid><description>In this discussion, we will discuss the Pytorch Deep Learning Lectures by LeCun.</description></item><item><title>13.Comparison Between Neuron Models</title><link>/snm/13.all_neuron_models/</link><pubDate>Tue, 05 Jul 2016 00:00:00 +0000</pubDate><guid>/snm/13.all_neuron_models/</guid><description>Comparison Between Neuron Models</description></item><item><title>Energy-based Models 5</title><link>/cpe/14.energy-based-learning-5/</link><pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate><guid>/cpe/14.energy-based-learning-5/</guid><description>In this meetup, we will discuss Restricted Boltzmann Machine (RBM). We will cover the reason for introducing RBM and the training. At the end of the discussion, we will also cover some topics of Deep Boltzmann Machines.</description></item><item><title>14.Noise in Refractory Kernel and Diffusive Noise</title><link>/snm/14.slow-noise/</link><pubDate>Fri, 22 Jul 2016 00:00:00 +0000</pubDate><guid>/snm/14.slow-noise/</guid><description>Slow Noise in parameters and diffusive noise (Part 1)</description></item><item><title>Predictive Coding Approximates Backprop along Arbitrary Computation Graphs</title><link>/cpe/15.predictive-coding/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>/cpe/15.predictive-coding/</guid><description>In this meetup, we will discuss this paper: https://arxiv.org/abs/2006.04182
Why? Feedforward-backprop usually has a loss function that involves all the parameters. Backprop means we need this huge global loss $\mathcal L({w_{ij}})$. However, it is hard to imaging such global loss calculations in our brain. One of the alternatives is predictive coding, which only utilizes local connection information.
In this paper (2006.04182), the author proves the equivalence of backprop and predictive coding on arbitary graph.</description></item><item><title>15.Diffusive Noise and The Subthreshold Regime</title><link>/snm/15.diffusive_noise_and_the_subthreshold_regime/</link><pubDate>Fri, 29 Jul 2016 00:00:00 +0000</pubDate><guid>/snm/15.diffusive_noise_and_the_subthreshold_regime/</guid><description>Diffusive Noise and The subthreshold Regime</description></item><item><title>LTD/LTP</title><link>/cpe/16.ltd-ltp/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>/cpe/16.ltd-ltp/</guid><description>In this meetup, we will discuss some key ideas related to biological neural network: LTP and LTD.</description></item><item><title>16.stochastic process</title><link>/snm/16.stochastic_process/</link><pubDate>Fri, 05 Aug 2016 00:00:00 +0000</pubDate><guid>/snm/16.stochastic_process/</guid><description>stochastic process</description></item><item><title>Self-supervised Learning: Generative or Contrastive</title><link>/cpe/17.self-supervised-learning/</link><pubDate>Sat, 03 Jul 2021 00:00:00 +0000</pubDate><guid>/cpe/17.self-supervised-learning/</guid><description>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218
We have discussed many topics of generative models. This paper serves as a summary of the current season of the discussions.</description></item><item><title>17.Homogeneous Network</title><link>/snm/17.homogeneous-network/</link><pubDate>Fri, 02 Sep 2016 00:00:00 +0000</pubDate><guid>/snm/17.homogeneous-network/</guid><description>Review of population activity; Homogeneous network.</description></item><item><title>Self-supervised Learning: GAN</title><link>/cpe/18.self-supervised-learning-gan/</link><pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate><guid>/cpe/18.self-supervised-learning-gan/</guid><description>We will discuss the reset of the paper arXiv:2006.08218.</description></item><item><title>18.SRM with Escape Noise</title><link>/snm/18.population-srm-with-escape-noise/</link><pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate><guid>/snm/18.population-srm-with-escape-noise/</guid><description>SRM neurons with escape noise</description></item><item><title>Self-supervised Learning: Theories (Part 1)</title><link>/cpe/19.self-supervised-learning-theories-1/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>/cpe/19.self-supervised-learning-theories-1/</guid><description>We will discuss Section 6 of the paper arXiv:2006.08218.</description></item><item><title>19.Population Activity</title><link>/snm/19.population-activity/</link><pubDate>Fri, 30 Sep 2016 00:00:00 +0000</pubDate><guid>/snm/19.population-activity/</guid><description>Integral equations for population activity</description></item><item><title>Self-supervised Learning: Theories (Part 2)</title><link>/cpe/20.self-supervised-learning-theories-2/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>/cpe/20.self-supervised-learning-theories-2/</guid><description>We will dive deep into Section 6 of the paper arXiv:2006.08218.
How does positive and negative samples work exactly? Alignment and uniformity JS-divergence</description></item><item><title>20.Basics of Renewal Theory</title><link>/snm/20.basics-of-renewal-theory/</link><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid>/snm/20.basics-of-renewal-theory/</guid><description>from math to neuroscience</description></item><item><title>21.Asynchronous Firing</title><link>/snm/21.asynchronous-firing/</link><pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate><guid>/snm/21.asynchronous-firing/</guid><description>Asynchronous firing of homogeneous network</description></item><item><title>22.interacting populations and continuum models</title><link>/snm/22.interacting-populations-and-continuum-models/</link><pubDate>Sat, 29 Oct 2016 00:00:00 +0000</pubDate><guid>/snm/22.interacting-populations-and-continuum-models/</guid><description>network of networks and continuum network</description></item><item><title>23.Linearized Population Equation and Transients</title><link>/snm/23.linearized-population-equation/</link><pubDate>Fri, 11 Nov 2016 00:00:00 +0000</pubDate><guid>/snm/23.linearized-population-equation/</guid><description>The population equation is quite complicated to solve, hence we linearize it and inspect the perturbation theory.</description></item><item><title>24. From individual neurons to collective bursting</title><link>/snm/24.from_individual_neurons_to_collective_bursting/</link><pubDate>Wed, 16 Nov 2016 00:00:00 +0000</pubDate><guid>/snm/24.from_individual_neurons_to_collective_bursting/</guid><description>Predicting collective dynamics from individual neuron properties.</description></item><item><title>25. The Significance of Single Spike</title><link>/snm/25.significance-of-single-spike/</link><pubDate>Fri, 09 Dec 2016 00:00:00 +0000</pubDate><guid>/snm/25.significance-of-single-spike/</guid><description>Single spike can have dramatic consequences on population activity.</description></item><item><title>27. Synchronized Oscillations and Locking</title><link>/snm/27.synchronized-oscillations-and-locking/</link><pubDate>Fri, 03 Feb 2017 00:00:00 +0000</pubDate><guid>/snm/27.synchronized-oscillations-and-locking/</guid><description>Locking</description></item><item><title>28. Oscillations in Reverberating Loops</title><link>/snm/28.oscillations-in-reverberating-loops/</link><pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate><guid>/snm/28.oscillations-in-reverberating-loops/</guid><description>Oscillations in reverberating loops can be simplified and researched.</description></item><item><title>29. Hebbian Learning</title><link>/snm/29.hebbian-learning/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>/snm/29.hebbian-learning/</guid><description>Simplest learning rule, aka, correlation based learning</description></item><item><title>30. Learning Equations</title><link>/snm/30.learning-equations/</link><pubDate>Sat, 03 Jun 2017 00:00:00 +0000</pubDate><guid>/snm/30.learning-equations/</guid><description>Unsupervised learning</description></item><item><title>31. Plasticity and Coding</title><link>/snm/31.plasticity-and-coding/</link><pubDate>Sat, 10 Jun 2017 00:00:00 +0000</pubDate><guid>/snm/31.plasticity-and-coding/</guid><description>How is plasticity related to neuronal coding</description></item><item><title>References for Probability Estimation Club</title><link>/cpe/00.references/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>/cpe/00.references/</guid><description>A list of references for our online discussions.</description></item><item><title>Conditional Probability Estimation</title><link>/projects/conditional-probability-estimation/</link><pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate><guid>/projects/conditional-probability-estimation/</guid><description>Understand models to estimate conditional probabilities</description></item><item><title>Foundations of Machine Learning</title><link>/projects/ml-foundations/</link><pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate><guid>/projects/ml-foundations/</guid><description>Dive deep into the foundations of machine learning.</description></item><item><title>Spiking Neuron Models</title><link>/projects/snm/</link><pubDate>Mon, 27 Apr 2020 13:22:46 +0200</pubDate><guid>/projects/snm/</guid><description>Reading club for the book Spiking Neuron Models</description></item><item><title>The Elements of Statistical Learning Reading Club</title><link>/projects/esl/</link><pubDate>Mon, 27 Apr 2020 13:22:46 +0200</pubDate><guid>/projects/esl/</guid><description>Read the book</description></item><item><title>05.Least Angle Regression</title><link>/esl/05.least-angle-regression/</link><pubDate>Thu, 29 Sep 2016 00:00:00 +0000</pubDate><guid>/esl/05.least-angle-regression/</guid><description>Least angle regression, aka, LAR</description></item><item><title>04.Shrinkage Methods</title><link>/esl/04.shrinkage-methods/</link><pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate><guid>/esl/04.shrinkage-methods/</guid><description>Shrinkage methods</description></item><item><title>03.Guass-Markov Theorem and Multiple Regression</title><link>/esl/03.gauss-markov-theorem/</link><pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate><guid>/esl/03.gauss-markov-theorem/</guid><description>Gauss-Markov Theorem</description></item><item><title>02.Linear Methods for Regression</title><link>/esl/02.linear-methods-for-regresssion/</link><pubDate>Thu, 18 Aug 2016 00:00:00 +0000</pubDate><guid>/esl/02.linear-methods-for-regresssion/</guid><description>Linear regression, least squares</description></item><item><title>01.Introductions (Review) and Several Preliminary Statistical Methods</title><link>/esl/01.statistical-learning-theory/</link><pubDate>Wed, 06 Jul 2016 00:00:00 +0000</pubDate><guid>/esl/01.statistical-learning-theory/</guid><description>Some basics of statistical learning; least squares and k nearest neighbors; statistical decision theory; local methods in high dimensions</description></item><item><title>ABOUT</title><link>/about/</link><pubDate>Thu, 24 Mar 2016 00:00:00 +0000</pubDate><guid>/about/</guid><description>We host reading clubs and seminars on neuroscience, machine learning, complex networks and intelligence.
License &amp;amp; Source Articles on this website are published under CC BY-NC-SA license if no specific license is designated.
This website is hosted on GitHub and generated by GitHub Pages (hugo). Computational Neuroscience Map is written in TiddlyMap and hosted statically on GitHub Pages.</description></item><item><title>00.Spiking Neuron Models Reading Club</title><link>/snm/00.spiking_neuron_models_club/</link><pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate><guid>/snm/00.spiking_neuron_models_club/</guid><description>Introduction to reading club of spiking neuron models, schedule, and notice</description></item><item><title>00.The Elements of Statistical Learning Reading Club</title><link>/esl/00.the-elements-of-statistical-learning/</link><pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate><guid>/esl/00.the-elements-of-statistical-learning/</guid><description>Introduction to reading club of The Elements of Statistical Learning, schedule, and notice</description></item><item><title/><link>/esl/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/esl/readme/</guid><description>elements-of-statistical-learning Reading club: The Elements of Statistical Learning
Online Course: StatLearning@Standford
An Introductory Book: http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/ The easiest way of creating notes is to duplicate one of the previous .md files and make changes to it.
Code of conduct:
Create a markdown file with extension .md; Any file name works, however, file names begins with two-digit number would be a nice convention. The markdown file has to include a header session that specifies the meta data.</description></item><item><title/><link>/typography/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/typography/</guid><description>ToC {:toc} We use kramdown This website uses kramdown as the basic syntax. However, a lot of html/css/js has been applied to generate some certain contents or styles.
Math also follows the kramdown syntax.
Footnote Syntax for footnotes is elaborated more on the website of kramdown.
{% highlight text %} Some text here some other text here.1
Table of Contents {% highlight text %}
ToC {:toc} {% endhighlight %} is used to generate table of contents.</description></item></channel></rss>