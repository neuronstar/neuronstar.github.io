<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NeuronStar</title><link>https://neuronstar.github.io/</link><description>Recent content on NeuronStar</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 24 Mar 2016 00:00:00 +0000</lastBuildDate><atom:link href="https://neuronstar.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Conditional Probability and Bayes</title><link>https://neuronstar.github.io/cpe/01.conditional-probability-and-bayes/</link><pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/01.conditional-probability-and-bayes/</guid><description>The Bayesian view of probability is quite objective and also more general than the frequentist&amp;rsquo;s view. It doesn&amp;rsquo;t rely on repeatition of events.</description></item><item><title>01.Neuron Biological Properties</title><link>https://neuronstar.github.io/snm/01.single_neuron_model/</link><pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/01.single_neuron_model/</guid><description>Neuron biological properties</description></item><item><title>Least Squares, Bootstrap, Maximum Likelihood, and Bayesian</title><link>https://neuronstar.github.io/cpe/02.least-squares-bootstrap-maximum-likelihood-and-bayesian/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/02.least-squares-bootstrap-maximum-likelihood-and-bayesian/</guid><description>Least squares, bootstrap, maximum likelihood, and maximum posterior leads to the same results in many cases.</description></item><item><title>02.Review of Last Week's Reading</title><link>https://neuronstar.github.io/snm/02.limitations_srm_contd_and_coding/</link><pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/02.limitations_srm_contd_and_coding/</guid><description>Review of Last Week&amp;rsquo;s Reading</description></item><item><title>EM Methods</title><link>https://neuronstar.github.io/cpe/03.em-methods/</link><pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/03.em-methods/</guid><description>Topics EM for Gaussian mixtures General EM algorithm Why does it work? Decomposition of log-likelihood into KL divergence and Relation between EM and Gibbs sampling</description></item><item><title>03.Equilibrium Potential and Hodgkin-Huxley Model</title><link>https://neuronstar.github.io/snm/03.equilibrium_potential_and_hodgkin-huxley_model/</link><pubDate>Sun, 13 Mar 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/03.equilibrium_potential_and_hodgkin-huxley_model/</guid><description>Equilibrium Potential and Hodgkin-Huxley Model</description></item><item><title>Variantional Inference Normalizing Flow</title><link>https://neuronstar.github.io/cpe/04.variational-inference-normalizing-flow/</link><pubDate>Sat, 16 Jan 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/04.variational-inference-normalizing-flow/</guid><description>Topics Variational Inference Normalizing Flow Variational Inference with Normalizing Flows</description></item><item><title>04.The Zoo of ion channels</title><link>https://neuronstar.github.io/snm/04.the_zoo_of_ion_channels/</link><pubDate>Wed, 23 Mar 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/04.the_zoo_of_ion_channels/</guid><description>The Zoo of ion channels</description></item><item><title>Review of Normalizing Flow</title><link>https://neuronstar.github.io/cpe/05.normalizing-flow-review/</link><pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/05.normalizing-flow-review/</guid><description>Topics Normalizing flow Applications of normalizing flow Methods of normalizing flow Problems of normalizing flow</description></item><item><title>05.Synapse and receptor</title><link>https://neuronstar.github.io/snm/05.synapse_and_receptors/</link><pubDate>Mon, 28 Mar 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/05.synapse_and_receptors/</guid><description>Synapse and receptor</description></item><item><title>Deep AutoRegressive Networks</title><link>https://neuronstar.github.io/cpe/06.deep-autoregressive-networks/</link><pubDate>Sat, 13 Feb 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/06.deep-autoregressive-networks/</guid><description>Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>06.Cable Equation and Its Solutions</title><link>https://neuronstar.github.io/snm/06.cable_equation_and_its_solutions/</link><pubDate>Sat, 02 Apr 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/06.cable_equation_and_its_solutions/</guid><description>Cable Equation and Its Solutions</description></item><item><title>MADE: Masked Autoencoder for Distribution Estimation</title><link>https://neuronstar.github.io/cpe/07.made/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/07.made/</guid><description>Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>07.Two dimensional neuron models</title><link>https://neuronstar.github.io/snm/07.reduction_to_two_dimensions_and_phase_plane_analysis/</link><pubDate>Thu, 21 Apr 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/07.reduction_to_two_dimensions_and_phase_plane_analysis/</guid><description>Two dimensional neuron models</description></item><item><title>MAF: how is MADE being used</title><link>https://neuronstar.github.io/cpe/08.maf/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/08.maf/</guid><description>We discussed MAF (arXiv:1705.07057v4) last time: The paper did not explain how exactly is MADE being used to update the shift and logscale.
We will use the tensorflow implementation of MAF to probe the above question. Here is the link to the relevant documentation: https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow
Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>08.Integrate and Fire Models Part 1</title><link>https://neuronstar.github.io/snm/08.integrate-and-fire-models-1/</link><pubDate>Sat, 21 May 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/08.integrate-and-fire-models-1/</guid><description>Integrate and Fire Model Part 1</description></item><item><title>Summary of Generative Models</title><link>https://neuronstar.github.io/cpe/09.summary-of-generative-models/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/09.summary-of-generative-models/</guid><description/></item><item><title>09.Reduction of the Hodgkin-Huxley model type II</title><link>https://neuronstar.github.io/snm/09.from_detailed_models_to_formal_spiking_neurons/</link><pubDate>Sat, 25 Jun 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/09.from_detailed_models_to_formal_spiking_neurons/</guid><description>Reduction of the Hodgkin-Huxley model &amp;lsquo;type II&amp;rsquo;</description></item><item><title>Energy-based Models</title><link>https://neuronstar.github.io/cpe/10.energy-based-learning/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/10.energy-based-learning/</guid><description>We will discuss energy-based learning in this session.
References:
Lecture notes: https://atcold.github.io/pytorch-Deep-Learning/[cid:90ae645c-415b-4c9f-8ea6-c78839a8e8d4] https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/ https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2/ https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view : if you can not access Google Drive, this file (007-ebm-01) has been attached to this calendar event too. Supplementary:
https://arxiv.org/pdf/1803.08823.pdf</description></item><item><title>10.Information Coding</title><link>https://neuronstar.github.io/snm/10.noise_and_renewal_process/</link><pubDate>Sat, 25 Jun 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/10.noise_and_renewal_process/</guid><description>Information coding</description></item><item><title>Energy-based Models 2</title><link>https://neuronstar.github.io/cpe/11.energy-based-learning-2/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/11.energy-based-learning-2/</guid><description>We will discuss energy-based learning in this session.
References:
Lecture notes: https://atcold.github.io/pytorch-Deep-Learning/[cid:90ae645c-415b-4c9f-8ea6-c78839a8e8d4] https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/ https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2/ https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view : if you can not access Google Drive, this file (007-ebm-01) has been attached to this calendar event too. Supplementary:
https://arxiv.org/pdf/1803.08823.pdf</description></item><item><title>11.Renewal Theory</title><link>https://neuronstar.github.io/snm/11.stationary_renewal_theory/</link><pubDate>Tue, 05 Jul 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/11.stationary_renewal_theory/</guid><description>Renewal Theory</description></item><item><title>Energy-based Models 3</title><link>https://neuronstar.github.io/cpe/12.energy-based-learning-3/</link><pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/12.energy-based-learning-3/</guid><description>In the past two meetups, we have been discussing EBM from a computer scientist&amp;rsquo;s perspective.
In this discussion, we will discuss chapter XV of Mehta P, Bukov M, Wang C-HH, Day AGRR, Richardson C, Fisher CK, et al. A high-bias, low-variance introduction to Machine Learning for physicists. Phys Rep. 2018;810: 122. doi:10.1016/j.physrep.2019.03.001</description></item><item><title>09.Escape Noise</title><link>https://neuronstar.github.io/snm/12.escape_noise/</link><pubDate>Fri, 29 Jul 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/12.escape_noise/</guid><description>Escape Noise</description></item><item><title>Energy-based Models 4</title><link>https://neuronstar.github.io/cpe/13.energy-based-learning-4/</link><pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/13.energy-based-learning-4/</guid><description>In this discussion, we will discuss the Pytorch Deep Learning Lectures by LeCun.</description></item><item><title>13.Comparison Between Neuron Models</title><link>https://neuronstar.github.io/snm/13.all_neuron_models/</link><pubDate>Tue, 05 Jul 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/13.all_neuron_models/</guid><description>Comparison Between Neuron Models</description></item><item><title>Energy-based Models 5</title><link>https://neuronstar.github.io/cpe/14.energy-based-learning-5/</link><pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/14.energy-based-learning-5/</guid><description>In this meetup, we will discuss Restricted Boltzmann Machine (RBM). We will cover the reason for introducing RBM and the training. At the end of the discussion, we will also cover some topics of Deep Boltzmann Machines.</description></item><item><title>14.Noise in Refractory Kernel and Diffusive Noise</title><link>https://neuronstar.github.io/snm/14.slow-noise/</link><pubDate>Fri, 22 Jul 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/14.slow-noise/</guid><description>Slow Noise in parameters and diffusive noise (Part 1)</description></item><item><title>Predictive Coding Approximates Backprop along Arbitrary Computation Graphs</title><link>https://neuronstar.github.io/cpe/15.predictive-coding/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/15.predictive-coding/</guid><description>In this meetup, we will discuss this paper: https://arxiv.org/abs/2006.04182
Why? Feedforward-backprop usually has a loss function that involves all the parameters. Backprop means we need this huge global loss $\mathcal L({w_{ij}})$. However, it is hard to imaging such global loss calculations in our brain. One of the alternatives is predictive coding, which only utilizes local connection information.
In this paper (2006.04182), the author proves the equivalence of backprop and predictive coding on arbitary graph.</description></item><item><title>15.Diffusive Noise and The Subthreshold Regime</title><link>https://neuronstar.github.io/snm/15.diffusive_noise_and_the_subthreshold_regime/</link><pubDate>Fri, 29 Jul 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/15.diffusive_noise_and_the_subthreshold_regime/</guid><description>Diffusive Noise and The subthreshold Regime</description></item><item><title>LTD/LTP</title><link>https://neuronstar.github.io/cpe/16.ltd-ltp/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/16.ltd-ltp/</guid><description>In this meetup, we will discuss some key ideas related to biological neural network: LTP and LTD.</description></item><item><title>16.stochastic process</title><link>https://neuronstar.github.io/snm/16.stochastic_process/</link><pubDate>Fri, 05 Aug 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/16.stochastic_process/</guid><description>stochastic process</description></item><item><title>Self-supervised Learning: Generative or Contrastive</title><link>https://neuronstar.github.io/cpe/17.self-supervised-learning/</link><pubDate>Sat, 03 Jul 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/17.self-supervised-learning/</guid><description>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218
We have discussed many topics of generative models. This paper serves as a summary of the current season of the discussions.</description></item><item><title>17.Homogeneous Network</title><link>https://neuronstar.github.io/snm/17.homogeneous-network/</link><pubDate>Fri, 02 Sep 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/17.homogeneous-network/</guid><description>Review of population activity; Homogeneous network.</description></item><item><title>Self-supervised Learning: GAN</title><link>https://neuronstar.github.io/cpe/18.self-supervised-learning-gan/</link><pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/18.self-supervised-learning-gan/</guid><description>We will discuss the reset of the paper arXiv:2006.08218.</description></item><item><title>18.SRM with Escape Noise</title><link>https://neuronstar.github.io/snm/18.population-srm-with-escape-noise/</link><pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/18.population-srm-with-escape-noise/</guid><description>SRM neurons with escape noise</description></item><item><title>Self-supervised Learning: Theories (Part 1)</title><link>https://neuronstar.github.io/cpe/19.self-supervised-learning-theories-1/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/19.self-supervised-learning-theories-1/</guid><description>We will discuss Section 6 of the paper arXiv:2006.08218.</description></item><item><title>19.Population Activity</title><link>https://neuronstar.github.io/snm/19.population-activity/</link><pubDate>Fri, 30 Sep 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/19.population-activity/</guid><description>Integral equations for population activity</description></item><item><title>Self-supervised Learning: Theories (Part 2)</title><link>https://neuronstar.github.io/cpe/20.self-supervised-learning-theories-2/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/20.self-supervised-learning-theories-2/</guid><description>We will dive deep into Section 6 of the paper arXiv:2006.08218. Here are a few topics to be explored.
InfoGAN objective; Positive and negative samples in loss function (InfoNCE); Uniformity in constrastive loss; JS-divergence.</description></item><item><title>20.Basics of Renewal Theory</title><link>https://neuronstar.github.io/snm/20.basics-of-renewal-theory/</link><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/20.basics-of-renewal-theory/</guid><description>from math to neuroscience</description></item><item><title>Graph Neural Networks: Basics</title><link>https://neuronstar.github.io/cpe/21.gnn-basics/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/21.gnn-basics/</guid><description>This will be the beginning of a new topic: Graph Neural Networks. In this new series, we will use the textbook by Hamilton1. For the first episode, we will discuss some basics about graphs to make sure we are all on the same page.
@Steven will lead the discussion.
Hamilton2020 Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046 &amp;#x21a9;&amp;#xfe0e;</description></item><item><title>21.Asynchronous Firing</title><link>https://neuronstar.github.io/snm/21.asynchronous-firing/</link><pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/21.asynchronous-firing/</guid><description>Asynchronous firing of homogeneous network</description></item><item><title>Graph Neural Networks: Basics (2)</title><link>https://neuronstar.github.io/cpe/22.gnn-basics-2/</link><pubDate>Sun, 03 Oct 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/22.gnn-basics-2/</guid><description>We will continue the discussion on Graph Neural Networks.
Problems of using Graphs Graph Neural Networks Textbook: Hamilton1
Hamilton2020 Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046 &amp;#x21a9;&amp;#xfe0e;</description></item><item><title>22.interacting populations and continuum models</title><link>https://neuronstar.github.io/snm/22.interacting-populations-and-continuum-models/</link><pubDate>Sat, 29 Oct 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/22.interacting-populations-and-continuum-models/</guid><description>network of networks and continuum network</description></item><item><title>Graph Neural Networks</title><link>https://neuronstar.github.io/cpe/23.gnn/</link><pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/23.gnn/</guid><description>Chapter 5 of Hamilton1.
Hamilton2020 Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046 &amp;#x21a9;&amp;#xfe0e;</description></item><item><title>23.Linearized Population Equation and Transients</title><link>https://neuronstar.github.io/snm/23.linearized-population-equation/</link><pubDate>Fri, 11 Nov 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/23.linearized-population-equation/</guid><description>The population equation is quite complicated to solve, hence we linearize it and inspect the perturbation theory.</description></item><item><title>Graph Neural Networks: PyTorch</title><link>https://neuronstar.github.io/cpe/24.gnn-pytorch/</link><pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/24.gnn-pytorch/</guid><description>We will go through the GNN tutorial by Phillip Lippe.</description></item><item><title>24. From individual neurons to collective bursting</title><link>https://neuronstar.github.io/snm/24.from_individual_neurons_to_collective_bursting/</link><pubDate>Wed, 16 Nov 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/24.from_individual_neurons_to_collective_bursting/</guid><description>Predicting collective dynamics from individual neuron properties.</description></item><item><title>Graph Neural Networks: Theoretical Motivations</title><link>https://neuronstar.github.io/cpe/25.gnn-2/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/25.gnn-2/</guid><description>We have changed the time!</description></item><item><title>25. The Significance of Single Spike</title><link>https://neuronstar.github.io/snm/25.significance-of-single-spike/</link><pubDate>Fri, 09 Dec 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/25.significance-of-single-spike/</guid><description>Single spike can have dramatic consequences on population activity.</description></item><item><title>Graph Neural Networks: Theoretical Motivations (Part 2)</title><link>https://neuronstar.github.io/cpe/26.gnn-3/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/26.gnn-3/</guid><description>We discussed the first section of Chapter 7. In this event, we will continue discussing chapter 7 of Hamilton1.
In this chapter, we will visit some of the theoretical underpinnings of graph neu- ral networks (GNNs). One of the most intriguing aspects of GNNs is that they were independently developed from distinct theoretical motivations.
Click here for an interactive widget.
Hamilton2020 Hamilton WL.</description></item><item><title>Graph Convolutional Matrix Completion</title><link>https://neuronstar.github.io/cpe/27.graph-convolutional-matrix-completion/</link><pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/27.graph-convolutional-matrix-completion/</guid><description>Our topic for this session is Graph Convolutional Matrix Completion (arXiv:1706.02263).
Abstract
Abstract of Graph Convolutional Matrix Completion (arXiv:1706.02263):
We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph.</description></item><item><title>27. Synchronized Oscillations and Locking</title><link>https://neuronstar.github.io/snm/27.synchronized-oscillations-and-locking/</link><pubDate>Fri, 03 Feb 2017 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/27.synchronized-oscillations-and-locking/</guid><description>Locking</description></item><item><title>Multivariate Time-series Forecasting Using GNN</title><link>https://neuronstar.github.io/cpe/28.gnn-time-series-forecasting/</link><pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/28.gnn-time-series-forecasting/</guid><description>Our topic for this session is Spectral Temporal Graph Neural Network for multivariate time-series forecasting (arXiv:2103.07719).
Abstract
Abstract of Spectral Temporal Graph Neural Network for multivariate time-series forecasting (arXiv:2103.07719):
Multivariate time-series forecasting plays a crucial rolein many real-world ap-plications. It is a challenging problem as one needs to consider both intra-seriestemporal correlations and inter-series correlations simultaneously. Recently, there have been multiple works trying to capture both correlations, but most, if not allof them only capture temporal correlations in the time domain and resort to pre-defined priors as inter-series relationships.</description></item><item><title>28. Oscillations in Reverberating Loops</title><link>https://neuronstar.github.io/snm/28.oscillations-in-reverberating-loops/</link><pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/28.oscillations-in-reverberating-loops/</guid><description>Oscillations in reverberating loops can be simplified and researched.</description></item><item><title>Hamilton WL. Graph Representation Learning. Chapter 8</title><link>https://neuronstar.github.io/cpe/29.hamilton-traditional-graph-generation-approaches/</link><pubDate>Sat, 05 Feb 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/29.hamilton-traditional-graph-generation-approaches/</guid><description>Our topic for this session is Chapter 8 of Hamilton WL. Graph Representation Learning: Traditional GraphGeneration Approaches.
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>29. Hebbian Learning</title><link>https://neuronstar.github.io/snm/29.hebbian-learning/</link><pubDate>Sat, 06 May 2017 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/29.hebbian-learning/</guid><description>Simplest learning rule, aka, correlation based learning</description></item><item><title>Hamilton WL. Graph Representation Learning. Chapter 8 (2)</title><link>https://neuronstar.github.io/cpe/30.hamilton-traditional-graph-generation-approaches-2/</link><pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/30.hamilton-traditional-graph-generation-approaches-2/</guid><description>We will wrap up Chapter 8 of Hamilton WL. Graph Representation Learning: Graph Generation.
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>30. Learning Equations</title><link>https://neuronstar.github.io/snm/30.learning-equations/</link><pubDate>Sat, 03 Jun 2017 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/30.learning-equations/</guid><description>Unsupervised learning</description></item><item><title>Uncertainty in Deep Learning</title><link>https://neuronstar.github.io/cpe/31.uncertaintyt-in-deep-learning/</link><pubDate>Sat, 26 Feb 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/31.uncertaintyt-in-deep-learning/</guid><description>Topic: uncertainty in deep learning
References:
Gawlikowski, J. et al. A Survey of Uncertainty in Deep Neural Networks. Arxiv (2021). Jospin, L. V., Buntine, W., Boussaid, F., Laga, H. &amp;amp; Bennamoun, M. Hands-on Bayesian Neural Networks &amp;ndash; a Tutorial for Deep Learning Users. Arxiv (2020). Gal, Yarin. &amp;ldquo;Uncertainty in deep learning.&amp;rdquo; (2016): 3. Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.</description></item><item><title>31. Plasticity and Coding</title><link>https://neuronstar.github.io/snm/31.plasticity-and-coding/</link><pubDate>Sat, 10 Jun 2017 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/31.plasticity-and-coding/</guid><description>How is plasticity related to neuronal coding</description></item><item><title>Causal Inference</title><link>https://neuronstar.github.io/cpe/35.causal-inference/</link><pubDate>Mon, 09 May 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/35.causal-inference/</guid><description>Alexa will lead a discussion on causal inference.
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Counterfactual Explanation in Multivariate Time Series</title><link>https://neuronstar.github.io/cpe/34.counterfactual-prediction-multivariate-time-series/</link><pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/34.counterfactual-prediction-multivariate-time-series/</guid><description>Ates E, Aksar B, Leung VJ, Coskun AK. Counterfactual Explanations for Machine Learning on Multivariate Time Series Data. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2008.10781
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Review of Time Series Forecasting</title><link>https://neuronstar.github.io/cpe/33.review-of-timeseries-2/</link><pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/33.review-of-timeseries-2/</guid><description>Lim B, Zohren S. Time Series Forecasting With Deep Learning: A Survey. arXiv [stat.ML]. 2020. Available: http://arxiv.org/abs/2004.13408
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Conformal Time Series Forecasting</title><link>https://neuronstar.github.io/cpe/32.review-of-timeseries/</link><pubDate>Thu, 10 Mar 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/32.review-of-timeseries/</guid><description>We start our new journey on time series by sharing and discussing two review papers:
Lim B, Zohren S. Time Series Forecasting With Deep Learning: A Survey. arXiv [stat.ML]. 2020. Available: http://arxiv.org/abs/2004.13408 Gneiting T, Katzfuss M. Probabilistic Forecasting. Annu Rev Stat Appl. 2014;1: 125–151. doi:10.1146/annurev-statistics-062713-085831 (pdf) Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Evaluating time series forecasting models</title><link>https://neuronstar.github.io/cpe/36.evaluating-forecasting-models/</link><pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/36.evaluating-forecasting-models/</guid><description>Our topic for this session is
Cerqueira V, Torgo L, Mozetic I. Evaluating time series forecasting models: An empirical study on performance estimation methods. arXiv [cs.LG]. 2019. Available: http://arxiv.org/abs/1905.11744
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>DeepAR</title><link>https://neuronstar.github.io/cpe/37.deepar/</link><pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/37.deepar/</guid><description>Topic: DeepAR.
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Temporal Fusion Transformer</title><link>https://neuronstar.github.io/cpe/38.tft/</link><pubDate>Sat, 09 Jul 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/38.tft/</guid><description>Lim B, Arik SO, Loeff N, Pfister T. Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting. In: arXiv.org [Internet]. 19 Dec 2019 [cited 9 Jul 2022]. Available: https://arxiv.org/abs/1912.09363
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Data Augmentation for Time Series</title><link>https://neuronstar.github.io/cpe/39.data-augmentation-ts/</link><pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/39.data-augmentation-ts/</guid><description>Wen Q, Sun L, Yang F, Song X, Gao J, Wang X, et al. Time Series Data Augmentation for Deep Learning: A Survey. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2002.12478
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>M Competition</title><link>https://neuronstar.github.io/cpe/40.m-competition/</link><pubDate>Sun, 07 Aug 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/40.m-competition/</guid><description>We will discuss the M competition.
@小紫花:
M5: 2020 年的一个比赛，预测沃尔玛在米国 3 个州、 10 个店、3000 多个产品的销售，要求预测 28 天。两个比赛：预测一个中值，或者预测一个分布（9 个数）。今年有 M6 官网，指引 PDF https://mofc.unic.ac.cy/m5-competition/ 中值 https://www.kaggle.com/competitions/m5-forecasting-accuracy/ 分布 https://www.kaggle.com/competitions/m5-forecasting-uncertainty 比赛背景、组织、运营总结 https://www.sciencedirect.com/science/article/pii/S0169207021001187 中值预测总结 https://www.sciencedirect.com/science/article/pii/S0169207021001874 分布预测总结（我比较感兴趣） https://www.sciencedirect.com/science/article/pii/S0169207021001722 一篇评论文章 https://www.sciencedirect.com/science/article/abs/pii/S016920702100128X 对讨论的回复 https://www.sciencedirect.com/science/article/abs/pii/S0169207022000644
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Neural ODE</title><link>https://neuronstar.github.io/cpe/41.neural-ode/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/41.neural-ode/</guid><description>Topic:
Chen RTQ, Rubanova Y, Bettencourt J, Duvenaud D. Neural Ordinary Differential Equations. arXiv [cs.LG]. 2018. Available: http://arxiv.org/abs/1806.07366
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Gradient Boosted Decision Trees (I)</title><link>https://neuronstar.github.io/cpe/42.gbdt-1/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/42.gbdt-1/</guid><description>Topic: XGBoost, LightGBM and Trees (I)
References: https://xgboost.readthedocs.io/en/stable/tutorials/model.html
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Gradient Boosted Decision Trees (II)</title><link>https://neuronstar.github.io/cpe/43.gbdt-2/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/43.gbdt-2/</guid><description>Topic: XGBoost, LightGBM and Trees (II)
References:
https://lightgbm.readthedocs.io/en/v3.3.2/ https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Forecasting with Trees</title><link>https://neuronstar.github.io/cpe/44.forecasting-with-trees/</link><pubDate>Sun, 16 Oct 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/44.forecasting-with-trees/</guid><description>Topic: Forecasting with Trees
References:
https://www.sciencedirect.com/science/article/pii/S0169207021001679 Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Probabilistic Forecasting: A Level-Set Approach</title><link>https://neuronstar.github.io/cpe/45.level-set-forecaster/</link><pubDate>Fri, 11 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/45.level-set-forecaster/</guid><description>Topic:
Hasson H, Wang Y, Januschowski T, Gasthaus J. Probabilistic forecasting: A level-set approach. [cited 25 Jan 2022]. Available: https://assets.amazon.science/a7/2b/29e00a5e429b8f2e708091ecb53e/probabilistic-forecasting-a-level-set-approach.pdf
Code: https://github.com/awslabs/gluonts/blob/fcc50e8be222bcf3b3da47ed1ed50b467e03f7e8/src/gluonts/ext/rotbaum/_model.py
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Diffusion Models: A Comprehensive Survey of Methods and Applications</title><link>https://neuronstar.github.io/cpe/46.difussion-model/</link><pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/46.difussion-model/</guid><description>Topic:
Yang L, Zhang Z, Song Y, Hong S, Xu R, Zhao Y, et al. Diffusion Models: A Comprehensive Survey of Methods and Applications. arXiv [cs.LG]. 2022. Available: http://arxiv.org/abs/2209.00796
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting</title><link>https://neuronstar.github.io/cpe/47.difussion-model-ar-denoising-diffusion-model-forecasting/</link><pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/47.difussion-model-ar-denoising-diffusion-model-forecasting/</guid><description>Topic:
Rasul K, Seward C, Schuster I, Vollgraf R. Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting. arXiv [cs.LG]. 2021. Available: http://arxiv.org/abs/2101.12072
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>End of 2022 Fireside Chat</title><link>https://neuronstar.github.io/cpe/48.end-of-2022-fireside-chat/</link><pubDate>Sun, 25 Dec 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/48.end-of-2022-fireside-chat/</guid><description>Fireside chat:
data statistics machine learning engineering Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>GitHub Actions for Data Scientists</title><link>https://neuronstar.github.io/cpe/49.github-actions-for-data-scientists/</link><pubDate>Sun, 08 Jan 2023 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/49.github-actions-for-data-scientists/</guid><description>Automate tasks using GitHub Actions:
Test code Build docs Scrape data from websites Build LaTeX resume Demos here: https://github.com/emptymalei/github-actions-for-data-scientists
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Diffusion Models for Time Series: Session 1</title><link>https://neuronstar.github.io/cpe/50.bootstrap-diffusion-models-for-ts-project/</link><pubDate>Sun, 08 Jan 2023 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/50.bootstrap-diffusion-models-for-ts-project/</guid><description>Get ready to write our own version of diffusion model for time series forecasting.
https://github.com/neuronstar/ts-diffusion
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Diffusion Models for Time Series: the Paper</title><link>https://neuronstar.github.io/cpe/51.diffusion-models-for-ts-paper/</link><pubDate>Sun, 08 Jan 2023 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/51.diffusion-models-for-ts-paper/</guid><description>Let&amp;rsquo;s discuss the idea behind the project.
Rasul K, Seward C, Schuster I, Vollgraf R. Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting. arXiv [cs.LG]. 2021. Available: http://arxiv.org/abs/2101.12072.
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Diffusion Models for Time Series: Initiation</title><link>https://neuronstar.github.io/cpe/52.diffusion-models-for-ts-initiation/</link><pubDate>Sat, 18 Feb 2023 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/52.diffusion-models-for-ts-initiation/</guid><description>Discuss how to proceed:
Data format, PyTorch Lightning. Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Diffusion Models for Time Series: Data</title><link>https://neuronstar.github.io/cpe/53.diffusion-models-data/</link><pubDate>Sun, 26 Feb 2023 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/53.diffusion-models-data/</guid><description>Topics:
data explorations,
dataloader construction
https://github.com/orgs/neuronstar/projects/2/views/2?pane=issue&amp;amp;itemId=21411846
https://github.com/orgs/neuronstar/projects/2/views/2?pane=issue&amp;amp;itemId=19112919
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Diffusion Models for Time Series: Dataloader</title><link>https://neuronstar.github.io/cpe/54.diffusion-models-dataloader/</link><pubDate>Fri, 14 Apr 2023 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/54.diffusion-models-dataloader/</guid><description>Topics:
dataloader construction Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Diffusion Models for Time Series: Dataloader and Collation</title><link>https://neuronstar.github.io/cpe/55.diffusion-models-dataloader-2/</link><pubDate>Fri, 14 Apr 2023 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/55.diffusion-models-dataloader-2/</guid><description>Topics:
dataloader construction Input pandas dataframe and output batched, transformed tensors Necessary transformations including fixed length input and moving-slicing. Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Diffusion Models for Time Series: Dataloader Discussions and Next Steps</title><link>https://neuronstar.github.io/cpe/56.diffusion-models-dataloader-discussionis/</link><pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/56.diffusion-models-dataloader-discussionis/</guid><description>Topics:
我们上周几个人写了一个 dataloader ，大概实现了 从 pandas dataframe -&amp;gt; pytorch dataloader apply transformations, e.g., moving slicing to produce fixed length input+output 这周我们来讨论一下这个写法，然后讨论一下接下来如何分工。
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Diffusion Models for Time Series: Review the Model</title><link>https://neuronstar.github.io/cpe/57.diffusion-models-model-discussions/</link><pubDate>Wed, 31 May 2023 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/57.diffusion-models-model-discussions/</guid><description>Topics:
Review the model
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Inferring causal impact using Bayesian structural time-series models</title><link>https://neuronstar.github.io/cpe/tbd.causal-impact-bayesian-structural-ts-models/</link><pubDate>Sat, 21 May 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/tbd.causal-impact-bayesian-structural-ts-models/</guid><description>Our topic for this session is Inferring causal impact using Bayesian structural time-series models (arXiv:1506.00356).
Abstract
Abstract of Inferring causal impact using Bayesian structural time-series models (arXiv:1506.00356):
An important problem in econometrics and marketing is to infer the causal impact that a designed market intervention has exerted on an outcome metric over time. This paper proposes to infer causal impact on the basis of a diffusion-regression state-space model that predicts the counterfactual market response in a synthetic control that would have occurred had no intervention taken place.</description></item><item><title>References for Probability Estimation Club</title><link>https://neuronstar.github.io/cpe/00.references/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/00.references/</guid><description>A list of references for our online discussions.</description></item><item><title>Conditional Probability Estimation</title><link>https://neuronstar.github.io/projects/conditional-probability-estimation/</link><pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/projects/conditional-probability-estimation/</guid><description>Understand models to estimate conditional probabilities</description></item><item><title>Foundations of Machine Learning</title><link>https://neuronstar.github.io/projects/ml-foundations/</link><pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/projects/ml-foundations/</guid><description>Dive deep into the foundations of machine learning.</description></item><item><title>Reading Club for Complex Systems</title><link>https://neuronstar.github.io/projects/complex-systems/</link><pubDate>Mon, 27 Apr 2020 13:22:46 +0200</pubDate><guid>https://neuronstar.github.io/projects/complex-systems/</guid><description>Some discussions on complex systems</description></item><item><title>Spiking Neuron Models</title><link>https://neuronstar.github.io/projects/snm/</link><pubDate>Mon, 27 Apr 2020 13:22:46 +0200</pubDate><guid>https://neuronstar.github.io/projects/snm/</guid><description>Reading club for the book Spiking Neuron Models</description></item><item><title>The Elements of Statistical Learning Reading Club</title><link>https://neuronstar.github.io/projects/esl/</link><pubDate>Mon, 27 Apr 2020 13:22:46 +0200</pubDate><guid>https://neuronstar.github.io/projects/esl/</guid><description>Read the book</description></item><item><title>05.Least Angle Regression</title><link>https://neuronstar.github.io/esl/05.least-angle-regression/</link><pubDate>Thu, 29 Sep 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/esl/05.least-angle-regression/</guid><description>Least angle regression, aka, LAR</description></item><item><title>04.Shrinkage Methods</title><link>https://neuronstar.github.io/esl/04.shrinkage-methods/</link><pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/esl/04.shrinkage-methods/</guid><description>Shrinkage methods</description></item><item><title>03.Guass-Markov Theorem and Multiple Regression</title><link>https://neuronstar.github.io/esl/03.gauss-markov-theorem/</link><pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/esl/03.gauss-markov-theorem/</guid><description>Gauss-Markov Theorem</description></item><item><title>02.Linear Methods for Regression</title><link>https://neuronstar.github.io/esl/02.linear-methods-for-regresssion/</link><pubDate>Thu, 18 Aug 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/esl/02.linear-methods-for-regresssion/</guid><description>Linear regression, least squares</description></item><item><title>01.Introductions (Review) and Several Preliminary Statistical Methods</title><link>https://neuronstar.github.io/esl/01.statistical-learning-theory/</link><pubDate>Wed, 06 Jul 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/esl/01.statistical-learning-theory/</guid><description>Some basics of statistical learning; least squares and k nearest neighbors; statistical decision theory; local methods in high dimensions</description></item><item><title>ABOUT</title><link>https://neuronstar.github.io/about/</link><pubDate>Thu, 24 Mar 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/about/</guid><description>We host reading clubs and seminars on neuroscience, machine learning, complex networks and intelligence.
License &amp;amp; Source Articles on this website are published under CC BY-NC-SA license if no specific license is designated.
This website is hosted on GitHub and generated by GitHub Pages (hugo). Computational Neuroscience Map is written in TiddlyMap and hosted statically on GitHub Pages.</description></item><item><title>00.Spiking Neuron Models Reading Club</title><link>https://neuronstar.github.io/snm/00.spiking_neuron_models_club/</link><pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/snm/00.spiking_neuron_models_club/</guid><description>Introduction to reading club of spiking neuron models, schedule, and notice</description></item><item><title>00.The Elements of Statistical Learning Reading Club</title><link>https://neuronstar.github.io/esl/00.the-elements-of-statistical-learning/</link><pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/esl/00.the-elements-of-statistical-learning/</guid><description>Introduction to reading club of The Elements of Statistical Learning, schedule, and notice</description></item><item><title/><link>https://neuronstar.github.io/esl/readme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/esl/readme/</guid><description>elements-of-statistical-learning Reading club: The Elements of Statistical Learning
Online Course: StatLearning@Standford
An Introductory Book: http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/ The easiest way of creating notes is to duplicate one of the previous .md files and make changes to it.
Code of conduct:
Create a markdown file with extension .md; Any file name works, however, file names begins with two-digit number would be a nice convention. The markdown file has to include a header session that specifies the meta data.</description></item><item><title/><link>https://neuronstar.github.io/typography/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/typography/</guid><description>ToC {:toc} We use kramdown This website uses kramdown as the basic syntax. However, a lot of html/css/js has been applied to generate some certain contents or styles.
Math also follows the kramdown syntax.
Footnote Syntax for footnotes is elaborated more on the website of kramdown.
{% highlight text %} Some text here some other text here.1
Table of Contents {% highlight text %}
ToC {:toc} {% endhighlight %} is used to generate table of contents.</description></item></channel></rss>