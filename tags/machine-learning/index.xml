<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Machine Learning on NeuronStar</title><link>https://neuronstar.github.io/tags/machine-learning/</link><description>Recent content in Machine Learning on NeuronStar</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 10 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://neuronstar.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Graph Neural Networks: Theoretical Motivations</title><link>https://neuronstar.github.io/cpe/25.gnn-2/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/25.gnn-2/</guid><description>We have changed the time!</description></item><item><title>Graph Neural Networks: Theoretical Motivations (Part 2)</title><link>https://neuronstar.github.io/cpe/26.gnn-3/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/26.gnn-3/</guid><description>We discussed the first section of Chapter 7. In this event, we will continue discussing chapter 7 of Hamilton1.
In this chapter, we will visit some of the theoretical underpinnings of graph neu- ral networks (GNNs). One of the most intriguing aspects of GNNs is that they were independently developed from distinct theoretical motivations.
Click here for an interactive widget.
Hamilton2020 Hamilton WL.</description></item><item><title>Graph Convolutional Matrix Completion</title><link>https://neuronstar.github.io/cpe/27.graph-convolutional-matrix-completion/</link><pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/27.graph-convolutional-matrix-completion/</guid><description>Our topic for this session is Graph Convolutional Matrix Completion (arXiv:1706.02263).
Abstract
Abstract of Graph Convolutional Matrix Completion (arXiv:1706.02263):
We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph.</description></item><item><title>Multivariate Time-series Forecasting Using GNN</title><link>https://neuronstar.github.io/cpe/28.gnn-time-series-forecasting/</link><pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/28.gnn-time-series-forecasting/</guid><description>Our topic for this session is Spectral Temporal Graph Neural Network for multivariate time-series forecasting (arXiv:2103.07719).
Abstract
Abstract of Spectral Temporal Graph Neural Network for multivariate time-series forecasting (arXiv:2103.07719):
Multivariate time-series forecasting plays a crucial rolein many real-world ap-plications. It is a challenging problem as one needs to consider both intra-seriestemporal correlations and inter-series correlations simultaneously. Recently, there have been multiple works trying to capture both correlations, but most, if not allof them only capture temporal correlations in the time domain and resort to pre-defined priors as inter-series relationships.</description></item><item><title>Hamilton WL. Graph Representation Learning. Chapter 8</title><link>https://neuronstar.github.io/cpe/29.hamilton-traditional-graph-generation-approaches/</link><pubDate>Sat, 05 Feb 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/29.hamilton-traditional-graph-generation-approaches/</guid><description>Our topic for this session is Chapter 8 of Hamilton WL. Graph Representation Learning: Traditional GraphGeneration Approaches.
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Hamilton WL. Graph Representation Learning. Chapter 8 (2)</title><link>https://neuronstar.github.io/cpe/30.hamilton-traditional-graph-generation-approaches-2/</link><pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/30.hamilton-traditional-graph-generation-approaches-2/</guid><description>We will wrap up Chapter 8 of Hamilton WL. Graph Representation Learning: Graph Generation.
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Uncertainty in Deep Learning</title><link>https://neuronstar.github.io/cpe/31.uncertaintyt-in-deep-learning/</link><pubDate>Sat, 26 Feb 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/31.uncertaintyt-in-deep-learning/</guid><description>Topic: uncertainty in deep learning
References:
Gawlikowski, J. et al. A Survey of Uncertainty in Deep Neural Networks. Arxiv (2021). Jospin, L. V., Buntine, W., Boussaid, F., Laga, H. &amp;amp; Bennamoun, M. Hands-on Bayesian Neural Networks &amp;ndash; a Tutorial for Deep Learning Users. Arxiv (2020). Gal, Yarin. &amp;ldquo;Uncertainty in deep learning.&amp;rdquo; (2016): 3. Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.</description></item><item><title>Basics of Time Series</title><link>https://neuronstar.github.io/cpe/32.basics-of-timeseries/</link><pubDate>Thu, 10 Mar 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/32.basics-of-timeseries/</guid><description>TBA
Use the following timezone tool or click on the &amp;ldquo;Add to Calendar&amp;rdquo; button on the sidebar.
Click here for an interactive widget.</description></item><item><title>Inferring causal impact using Bayesian structural time-series models</title><link>https://neuronstar.github.io/cpe/tbd.causal-impact-bayesian-structural-ts-models/</link><pubDate>Sat, 15 Jan 2022 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/tbd.causal-impact-bayesian-structural-ts-models/</guid><description>Our topic for this session is Inferring causal impact using Bayesian structural time-series models (arXiv:1506.00356).
Abstract
Abstract of Inferring causal impact using Bayesian structural time-series models (arXiv:1506.00356):
An important problem in econometrics and marketing is to infer the causal impact that a designed market intervention has exerted on an outcome metric over time. This paper proposes to infer causal impact on the basis of a diffusion-regression state-space model that predicts the counterfactual market response in a synthetic control that would have occurred had no intervention taken place.</description></item><item><title>Foundations of Machine Learning</title><link>https://neuronstar.github.io/projects/ml-foundations/</link><pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/projects/ml-foundations/</guid><description>Dive deep into the foundations of machine learning.</description></item><item><title>The Elements of Statistical Learning Reading Club</title><link>https://neuronstar.github.io/projects/esl/</link><pubDate>Mon, 27 Apr 2020 13:22:46 +0200</pubDate><guid>https://neuronstar.github.io/projects/esl/</guid><description>Read the book</description></item></channel></rss>