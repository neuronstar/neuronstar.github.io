<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Graph on NeuronStar</title><link>https://neuronstar.github.io/tags/graph/</link><description>Recent content in Graph on NeuronStar</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sat, 11 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://neuronstar.github.io/tags/graph/index.xml" rel="self" type="application/rss+xml"/><item><title>Graph Neural Networks: Basics</title><link>https://neuronstar.github.io/cpe/21.gnn-basics/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/21.gnn-basics/</guid><description>This will be the beginning of a new topic: Graph Neural Networks. In this new series, we will use the textbook by Hamilton1. For the first episode, we will discuss some basics about graphs to make sure we are all on the same page.
@Steven will lead the discussion.
Hamilton2020 Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046 &amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Graph Neural Networks: Basics (2)</title><link>https://neuronstar.github.io/cpe/22.gnn-basics-2/</link><pubDate>Sun, 03 Oct 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/22.gnn-basics-2/</guid><description>We will continue the discussion on Graph Neural Networks.
Problems of using Graphs Graph Neural Networks Textbook: Hamilton1
Hamilton2020 Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046 &amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Graph Neural Networks</title><link>https://neuronstar.github.io/cpe/23.gnn/</link><pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/23.gnn/</guid><description>Chapter 5 of Hamilton1.
Hamilton2020 Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046 &amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Graph Neural Networks: PyTorch</title><link>https://neuronstar.github.io/cpe/24.gnn-pytorch/</link><pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/24.gnn-pytorch/</guid><description>We will go through the GNN tutorial by Phillip Lippe.</description></item><item><title>Graph Neural Networks: Theoretical Motivations</title><link>https://neuronstar.github.io/cpe/25.gnn-2/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/25.gnn-2/</guid><description>We have changed the time!</description></item><item><title>Graph Neural Networks: Theoretical Motivations (Part 2)</title><link>https://neuronstar.github.io/cpe/26.gnn-3/</link><pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/26.gnn-3/</guid><description>We discussed the first section of Chapter 7. In this event, we will continue discussing chapter 7 of Hamilton1.
In this chapter, we will visit some of the theoretical underpinnings of graph neu- ral networks (GNNs). One of the most intriguing aspects of GNNs is that they were independently developed from distinct theoretical motivations.
Click here for an interactive widget.
Hamilton2020 Hamilton WL.</description></item><item><title>Graph Convolutional Matrix Completion</title><link>https://neuronstar.github.io/cpe/27.graph-convolutional-matrix-completion/</link><pubDate>Sat, 11 Dec 2021 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/27.graph-convolutional-matrix-completion/</guid><description>Our topic for this session is Graph Convolutional Matrix Completion (arXiv:1706.02263).
Abstract
Abstract of Graph Convolutional Matrix Completion (arXiv:1706.02263):
We consider matrix completion for recommender systems from the point of view of link prediction on graphs. Interaction data such as movie ratings can be represented by a bipartite user-item graph with labeled edges denoting observed ratings. Building on recent progress in deep learning on graph-structured data, we propose a graph auto-encoder framework based on differentiable message passing on the bipartite interaction graph.</description></item></channel></rss>