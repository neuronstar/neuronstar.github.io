{"data":[{"title":"Conditional Probability and Bayes","authors":null,"summary":"The Bayesian view of probability is quite objective and also more general than the frequentist's view. It doesn't rely on repeatition of events.","date":"2020-11-18T00:00:00Z","references":[{"link":"https://web.stanford.edu/~hastie/ElemStatLearn/","name":"Trevor Hastie, Robert Tibshirani, J. F. (2004). The Elements of Statistical Learning (Vol. 99, Issue 466). Springer Science \u0026 Business Media."},{"link":"https://datumorphism.leima.is/wiki/pattern-mining/association-rules/","name":"Association Rules"},{"link":"https://en.wikipedia.org/wiki/Bayes%27_theorem#/media/File:Bayes%27_Theorem_2D.svg","name":"Bayes' theorem @ Wikipedia"},{"link":"https://doi.org/10.1016/C2013-0-19397-X","name":"Ross, S. M. (2014). Introduction to Probability and Statistics for Engineers and Scientists. Elsevier."},{"link":"https://datumorphism.leima.is/wiki/machine-learning/bayesian/naive-bayesian/","name":"Naive Bayes"}]},{"title":"01.Neuron Biological Properties","authors":null,"summary":"Neuron biological properties","date":"2016-03-18T00:00:00Z","references":null},{"title":"Least Squares, Bootstrap, Maximum Likelihood, and Bayesian","authors":null,"summary":"Least squares, bootstrap, maximum likelihood, and maximum posterior leads to the same results in many cases.","date":"2020-12-12T00:00:00Z","references":[{"link":"https://web.stanford.edu/~hastie/ElemStatLearn/","name":"Trevor Hastie, Robert Tibshirani, J. F. (2004). The Elements of Statistical Learning (Vol. 99, Issue 466). Springer Science \u0026 Business Media."}]},{"title":"02.Review of Last Week's Reading","authors":null,"summary":"Review of Last Week's Reading","date":"2016-03-18T00:00:00Z","references":null},{"title":"EM Methods","authors":null,"summary":"","date":"2021-01-02T00:00:00Z","references":[{"link":"https://web.stanford.edu/~hastie/ElemStatLearn/","name":"Trevor Hastie, Robert Tibshirani, J. F. (2004). The Elements of Statistical Learning (Vol. 99, Issue 466). Springer Science \u0026 Business Media."},{"link":"https://doi.org/978-0387-31073-2","name":"Christpher M. Bishop. (2006). Pattern Recognition and Machine Learning."}]},{"title":"03.Equilibrium Potential and Hodgkin-Huxley Model","authors":null,"summary":"Equilibrium Potential and Hodgkin-Huxley Model","date":"2016-03-13T00:00:00Z","references":null},{"title":"Variantional Inference Normalizing Flow","authors":null,"summary":"","date":"2021-01-16T00:00:00Z","references":[{"link":"https://doi.org/978-0387-31073-2","name":"Christpher M. Bishop. (2006). Pattern Recognition and Machine Learning."},{"link":"http://arxiv.org/abs/1505.05770","name":"Rezende, D. J., \u0026 Mohamed, S. (2015). Variational Inference with Normalizing Flows."}]},{"title":"04.The Zoo of ion channels","authors":null,"summary":"The Zoo of ion channels","date":"2016-03-23T00:00:00Z","references":null},{"title":"Review of Normalizing Flow","authors":null,"summary":"","date":"2021-01-30T00:00:00Z","references":[{"link":"https://doi.org/10.1109/TPAMI.2020.2992934","name":"Kobyzev, I., Prince, S., \u0026 Brubaker, M. (2020). Normalizing Flows: An Introduction and Review of Current Methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1–1."}]},{"title":"05.Synapse and receptor","authors":null,"summary":"Synapse and receptor","date":"2016-03-28T00:00:00Z","references":null},{"title":"Deep AutoRegressive Networks","authors":null,"summary":"","date":"2021-02-13T00:00:00Z","references":[{"link":"https://arxiv.org/abs/1310.8499","name":"Gregor, K., Danihelka, I., Mnih, A., Blundell, C., \u0026 Wierstra, D. (2014). Deep autoregressive networks. 31st International Conference on Machine Learning, ICML 2014, 4, 2991–3000."},{"link":"https://deepgenerativemodels.github.io/notes/autoregressive/","name":"Autoregressive models"}]},{"title":"06.Cable Equation and Its Solutions","authors":null,"summary":"Cable Equation and Its Solutions","date":"2016-04-02T00:00:00Z","references":null},{"title":"MADE: Masked Autoencoder for Distribution Estimation","authors":null,"summary":"","date":"2021-02-27T00:00:00Z","references":[{"link":"https://arxiv.org/abs/1502.03509","name":"MADE: Masked Autoencoder for Distribution Estimation"}]},{"title":"07.Two dimensional neuron models","authors":null,"summary":"Two dimensional neuron models","date":"2016-04-21T00:00:00Z","references":null},{"title":"MAF: how is MADE being used","authors":null,"summary":"","date":"2021-02-27T00:00:00Z","references":[{"link":"https://arxiv.org/abs/1705.07057","name":"Masked Autoregressive Flow for Density Estimation"},{"link":"https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow","name":"Tensorflow Documentation"}]},{"title":"08.Integrate and Fire Models Part 1","authors":null,"summary":"Integrate and Fire Model Part 1","date":"2016-05-21T00:00:00Z","references":null},{"title":"Summary of Generative Models","authors":null,"summary":"","date":"2021-02-27T00:00:00Z","references":[{"link":"https://arxiv.org/abs/2103.04922","name":"Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models"}]},{"title":"09.Reduction of the Hodgkin-Huxley model type II","authors":null,"summary":"Reduction of the Hodgkin-Huxley model 'type II'","date":"2016-06-25T00:00:00Z","references":null},{"title":"Energy-based Models","authors":null,"summary":"","date":"2021-02-27T00:00:00Z","references":[{"link":"https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/","name":"Pytorch Deep Learning Lectures"},{"link":"https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view","name":"Pytorch Deep Learning Slides"},{"link":"https://arxiv.org/abs/1803.08823","name":"A high-bias, low-variance introduction to Machine Learning for physicists"}]},{"title":"10.Information Coding","authors":null,"summary":"Information coding","date":"2016-06-25T00:00:00Z","references":null},{"title":"Energy-based Models 2","authors":null,"summary":"","date":"2021-02-27T00:00:00Z","references":[{"link":"https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/","name":"Pytorch Deep Learning Lectures"},{"link":"https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view","name":"Pytorch Deep Learning Slides"},{"link":"https://arxiv.org/abs/1803.08823","name":"A high-bias, low-variance introduction to Machine Learning for physicists"}]},{"title":"11.Renewal Theory","authors":null,"summary":"Renewal Theory","date":"2016-07-05T00:00:00Z","references":null},{"title":"Energy-based Models 3","authors":null,"summary":"","date":"2021-04-24T00:00:00Z","references":[{"link":"https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/","name":"Pytorch Deep Learning Lectures"},{"link":"https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view","name":"Pytorch Deep Learning Slides"},{"link":"https://arxiv.org/abs/1803.08823","name":"A high-bias, low-variance introduction to Machine Learning for physicists"}]},{"title":"09.Escape Noise","authors":null,"summary":"Escape Noise","date":"2016-07-29T00:00:00Z","references":null},{"title":"Energy-based Models 4","authors":null,"summary":"","date":"2021-05-26T00:00:00Z","references":[{"link":"https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/","name":"Pytorch Deep Learning Lectures"},{"link":"https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view","name":"Pytorch Deep Learning Slides"},{"link":"https://arxiv.org/abs/1803.08823","name":"A high-bias, low-variance introduction to Machine Learning for physicists"}]},{"title":"13.Comparison Between Neuron Models","authors":null,"summary":"Comparison Between Neuron Models","date":"2016-07-05T00:00:00Z","references":null},{"title":"Energy-based Models 5","authors":null,"summary":"","date":"2021-06-02T00:00:00Z","references":[{"link":"https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/","name":"Pytorch Deep Learning Lectures"},{"link":"https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view","name":"Pytorch Deep Learning Slides"},{"key":"Mehta2008","link":"https://arxiv.org/abs/1803.08823","name":"A high-bias, low-variance introduction to Machine Learning for physicists"}]},{"title":"14.Noise in Refractory Kernel and Diffusive Noise","authors":null,"summary":"Slow Noise in parameters and diffusive noise (Part 1)","date":"2016-07-22T00:00:00Z","references":null},{"title":"Predictive Coding Approximates Backprop along Arbitrary Computation Graphs","authors":null,"summary":"","date":"2021-06-21T00:00:00Z","references":[{"link":"https://arxiv.org/abs/2006.04182","name":"Millidge B, Tschantz A, Buckley CL. Predictive Coding Approximates Backprop along Arbitrary Computation Graphs. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.04182"}]},{"title":"15.Diffusive Noise and The Subthreshold Regime","authors":null,"summary":"Diffusive Noise and The subthreshold Regime","date":"2016-07-29T00:00:00Z","references":null},{"title":"LTD/LTP","authors":null,"summary":"","date":"2021-06-21T00:00:00Z","references":[{"link":"https://www.nature.com/articles/s41539-019-0048-y","name":"Abraham WC, Jones OD, Glanzman DL. Is plasticity of synapses the mechanism of long-term memory storage? NPJ Sci Learn. 2019;4: 9. doi:10.1038/s41539-019-0048-y"}]},{"title":"16.stochastic process","authors":null,"summary":"stochastic process","date":"2016-08-05T00:00:00Z","references":null},{"title":"Self-supervised Learning: Generative or Contrastive","authors":null,"summary":"","date":"2021-07-03T00:00:00Z","references":[{"link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"}]},{"title":"17.Homogeneous Network","authors":null,"summary":"Review of population activity; Homogeneous network.","date":"2016-09-02T00:00:00Z","references":[{"name":"Section 6.1, Section 6.2.1"}]},{"title":"Self-supervised Learning: GAN","authors":null,"summary":"","date":"2021-08-01T00:00:00Z","references":[{"link":"http://arxiv.org/abs/1406.2661","name":"Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, et al. Generative Adversarial Networks. arXiv [stat.ML]. 2014. Available: http://arxiv.org/abs/1406.2661"},{"link":"http://arxiv.org/abs/1411.1784","name":"Mirza M, Osindero S. Conditional Generative Adversarial Nets. arXiv [cs.LG]. 2014. Available: http://arxiv.org/abs/1411.1784"},{"link":"http://arxiv.org/abs/1701.00160","name":"Goodfellow I. NIPS 2016 Tutorial: Generative Adversarial Networks. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1701.00160"},{"link":"https://deeplizard.com/lesson/gaa1ilrazd","name":"GAN Course Introduction - Intuitive Intro To Generative Adversarial Networks. [cited 1 Aug 2021]. Available: https://deeplizard.com/lesson/gaa1ilrazd"},{"link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"}]},{"title":"18.SRM with Escape Noise","authors":null,"summary":"SRM neurons with escape noise","date":"2016-09-23T00:00:00Z","references":[{"name":"Section 6.2.2"}]},{"title":"Self-supervised Learning: Theories (Part 1)","authors":null,"summary":"","date":"2021-08-26T00:00:00Z","references":[{"link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"}]},{"title":"19.Population Activity","authors":null,"summary":"Integral equations for population activity","date":"2016-09-30T00:00:00Z","references":[{"name":"Spiking Neuron Models, Section 6.3"}]},{"title":"Self-supervised Learning: Theories (Part 2)","authors":null,"summary":"","date":"2021-08-26T00:00:00Z","references":[{"link":"http://arxiv.org/abs/2006.08218","name":"Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218"},{"link":"http://arxiv.org/abs/2005.10242","name":"Wang T, Isola P. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.10242"},{"link":"http://arxiv.org/abs/2003.14323","name":"Newell A, Deng J. How Useful is Self-Supervised Pretraining for Visual Tasks? arXiv [cs.CV]. 2020. Available: http://arxiv.org/abs/2003.14323"},{"link":"http://arxiv.org/abs/1907.13625","name":"Tschannen M, Djolonga J, Rubenstein PK, Gelly S, Lucic M. On Mutual Information Maximization for Representation Learning. arXiv [cs.LG]. 2019. Available: http://arxiv.org/abs/1907.13625"},{"link":"http://arxiv.org/abs/1807.03748","name":"van den Oord A, Li Y, Vinyals O. Representation learning with Contrastive Predictive Coding. arXiv [cs.LG]. 2018. Available: http://arxiv.org/abs/1807.03748"},{"link":"http://arxiv.org/abs/1606.03657","name":"Chen X, Duan Y, Houthooft R, Schulman J, Sutskever I, Abbeel P. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1606.03657"},{"link":"http://arxiv.org/abs/1606.00709","name":"Nowozin S, Cseke B, Tomioka R. f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. arXiv [stat.ML]. 2016. Available: http://arxiv.org/abs/1606.00709"}]},{"title":"20.Basics of Renewal Theory","authors":null,"summary":"from math to neuroscience","date":"2016-10-08T00:00:00Z","references":null},{"title":"Graph Neural Networks: Basics","authors":null,"summary":"","date":"2021-09-11T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046"}]},{"title":"21.Asynchronous Firing","authors":null,"summary":"Asynchronous firing of homogeneous network","date":"2016-10-14T00:00:00Z","references":[{"name":"Spiking Neuron Models, Section 6.4"}]},{"title":"Graph Neural Networks: Basics (2)","authors":null,"summary":"","date":"2021-10-03T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046"}]},{"title":"22.interacting populations and continuum models","authors":null,"summary":"network of networks and continuum network","date":"2016-10-29T00:00:00Z","references":[{"name":"Spiking Neuron Models, Section 6.5"}]},{"title":"Graph Neural Networks","authors":null,"summary":"","date":"2021-10-19T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046"}]},{"title":"23.Linearized Population Equation and Transients","authors":null,"summary":"The population equation is quite complicated to solve, hence we linearize it and inspect the perturbation theory.","date":"2016-11-11T00:00:00Z","references":[{"name":"Spiking Neuron Models, Section 7.1, 7.2"}]},{"title":"Graph Neural Networks: PyTorch","authors":null,"summary":"","date":"2021-11-02T00:00:00Z","references":[{"key":"uva-dlc","link":"https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html","name":"Tutorial 7: Graph Neural Networks — UvA DL Notebooks v1.1 documentation. [cited 2 Nov 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html"},{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046"}]},{"title":"24. From individual neurons to collective bursting","authors":null,"summary":"Predicting collective dynamics from individual neuron properties.","date":"2016-11-16T00:00:00Z","references":[{"name":"Two dimensional neuron models; Integrate and Fire Models Part 1;"},{"name":"Comparison Between Neuron Models; Homogeneous Network"},{"name":"Asynchronous Firing"}]},{"title":"Graph Neural Networks: Theoretical Motivations","authors":null,"summary":"We have changed the time!","date":"2021-11-12T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046"}]},{"title":"25. The Significance of Single Spike","authors":null,"summary":"Single spike can have dramatic consequences on population activity.","date":"2016-12-09T00:00:00Z","references":[{"name":"Spiking Neuron Models, Section 7.4"}]},{"title":"Graph Neural Networks: Theoretical Motivations (Part 2)","authors":null,"summary":"","date":"2021-11-12T00:00:00Z","references":[{"key":"Hamilton2020","link":"https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046","name":"Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046"}]},{"title":"Graph Convolutional Matrix Completion","authors":null,"summary":"","date":"2021-12-11T00:00:00Z","references":[{"key":"vandenBergR2017","link":"http://arxiv.org/abs/1706.02263","name":"van den Berg R, Kipf TN, Welling M. Graph Convolutional Matrix Completion. arXiv [stat.ML]. 2017. Available: http://arxiv.org/abs/1706.02263"},{"key":"matrix_completion_wiki","link":"https://en.wikipedia.org/wiki/Matrix_completion","name":"Contributors to Wikimedia projects. Matrix completion. In: Wikipedia [Internet]. 9 Nov 2021 [cited 12 Dec 2021]. Available: https://en.wikipedia.org/wiki/Matrix_completion"}]},{"title":"27. Synchronized Oscillations and Locking","authors":null,"summary":"Locking","date":"2017-02-03T00:00:00Z","references":[{"name":"Spiking Neuron Models, Section 8.2"}]},{"title":"Inferring causal impact using Bayesian structural time-series models","authors":null,"summary":"","date":"2022-01-15T00:00:00Z","references":[{"key":"Cao2021","link":"https://arxiv.org/abs/1506.00356","name":"Brodersen KH, Gallusser F, Koehler J, Remy N, Scott SL. Inferring causal impact using Bayesian structural time-series models. aoas. 2015;9: 247–274. doi:10.1214/14-AOAS788"}]},{"title":"Multivariate Time-series Forecasting Using GNN","authors":null,"summary":"","date":"2021-12-11T00:00:00Z","references":[{"key":"Cao2021","link":"http://arxiv.org/abs/2103.07719","name":"Cao D, Wang Y, Duan J, Zhang C, Zhu X, Huang C, et al. Spectral Temporal Graph Neural Network for multivariate time-series forecasting. arXiv [cs.LG]. 2021. Available: http://arxiv.org/abs/2103.07719"}]},{"title":"28. Oscillations in Reverberating Loops","authors":null,"summary":"Oscillations in reverberating loops can be simplified and researched.","date":"2017-02-17T00:00:00Z","references":[{"name":"Spiking Neuron Models, Section 8.3"}]},{"title":"29. Hebbian Learning","authors":null,"summary":"Simplest learning rule, aka, correlation based learning","date":"2017-05-06T00:00:00Z","references":[{"name":"Spiking Neuron Models, Chapter 10"}]},{"title":"30. Learning Equations","authors":null,"summary":"Unsupervised learning","date":"2017-06-03T00:00:00Z","references":[{"name":"Spiking Neuron Models, Chapter 11"}]},{"title":"31. Plasticity and Coding","authors":null,"summary":"How is plasticity related to neuronal coding","date":"2017-06-10T00:00:00Z","references":[{"name":"Spiking Neuron Models, Chapter 12"}]},{"title":"Bayesian","authors":null,"summary":null,"date":null,"references":null},{"title":"Conditional Probability Estimation","authors":null,"summary":null,"date":null,"references":null},{"title":"Machine Learning","authors":null,"summary":null,"date":null,"references":null},{"title":null,"authors":null,"summary":null,"date":null,"references":null},{"title":"Time series","authors":null,"summary":null,"date":null,"references":null},{"title":"Graph","authors":null,"summary":null,"date":null,"references":null},{"title":"Graph Neural Networks","authors":null,"summary":null,"date":null,"references":null},{"title":"Matrix Completion","authors":null,"summary":null,"date":null,"references":null},{"title":"Neural Networks","authors":null,"summary":null,"date":null,"references":null},{"title":"PyTorch","authors":null,"summary":null,"date":null,"references":null},{"title":"Self-supervised Learning","authors":null,"summary":null,"date":null,"references":null},{"title":"Biological Neural Network","authors":null,"summary":null,"date":null,"references":null},{"title":"Boltzmann Machine","authors":null,"summary":null,"date":null,"references":null},{"title":"Energy-based Learning","authors":null,"summary":null,"date":null,"references":null},{"title":"Principle of Max Entropy","authors":null,"summary":null,"date":null,"references":null},{"title":"Energy-based model","authors":null,"summary":null,"date":null,"references":null},{"title":"Flow","authors":null,"summary":null,"date":null,"references":null},{"title":"Hebb's Rule","authors":null,"summary":null,"date":null,"references":null},{"title":"Hopfield Network","authors":null,"summary":null,"date":null,"references":null},{"title":"VAE","authors":null,"summary":null,"date":null,"references":null},{"title":"AutoRegressive","authors":null,"summary":null,"date":null,"references":null},{"title":"Normalizing Flow","authors":null,"summary":null,"date":null,"references":null},{"title":"Variational Inference","authors":null,"summary":null,"date":null,"references":null},{"title":"Gaussian Mixture","authors":null,"summary":null,"date":null,"references":null},{"title":"Maximum Likelihood","authors":null,"summary":null,"date":null,"references":null},{"title":"Sampling","authors":null,"summary":null,"date":null,"references":null},{"title":"Bootstrap","authors":null,"summary":null,"date":null,"references":null},{"title":"Least Squares","authors":null,"summary":null,"date":null,"references":null},{"title":"References for Probability Estimation Club","authors":null,"summary":"A list of references for our online discussions.","date":"2020-12-12T00:00:00Z","references":[{"link":"https://web.stanford.edu/~hastie/ElemStatLearn/","name":"Trevor Hastie, Robert Tibshirani, J. F. (2004). The Elements of Statistical Learning (Vol. 99, Issue 466). Springer Science \u0026 Business Media."},{"link":"https://doi.org/978-0387-31073-2","name":"Christpher M. Bishop. (2006). Pattern Recognition and Machine Learning."}]},{"title":"Bayes's Theorem","authors":null,"summary":null,"date":null,"references":null},{"title":"Conditional Probability","authors":null,"summary":null,"date":null,"references":null},{"title":"Conditional Probability Estimation","authors":null,"summary":"Understand models to estimate conditional probabilities","date":"2020-11-03T00:00:00Z","references":null},{"title":"Estimation Theory","authors":null,"summary":null,"date":null,"references":null},{"title":"Probability","authors":null,"summary":null,"date":null,"references":null},{"title":null,"authors":null,"summary":null,"date":null,"references":null},{"title":"Foundations of Machine Learning","authors":null,"summary":"Dive deep into the foundations of machine learning.","date":"2020-05-03T00:00:00Z","references":[{"link":"https://www.cse.huji.ac.il/~shais/UnderstandingMachineLearning/","name":"Shalev-Shwartz, S., \u0026 Ben-David, S. (2013). Understanding Machine Learning: From Theory to Algorithms"},{"link":"https://arxiv.org/abs/1803.08823","name":"Pankaj Mehta, Ching-Hao Wang, Alexandre G. R. Day, C. R. (2019). A high-bias, low-variance introduction to Machine Learning for physicists."},{"link":"http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf","name":"Lecun, Y. (2016). A Tutorial on Energy-Based Learning."}]},{"title":"Introduction","authors":null,"summary":null,"date":null,"references":null},{"title":"Neuroscience","authors":null,"summary":null,"date":null,"references":null},{"title":"Spiking Neuron Models","authors":null,"summary":"Reading club for the book Spiking Neuron Models","date":"2020-04-27T13:22:46+02:00","references":null},{"title":"The Elements of Statistical Learning Reading Club","authors":null,"summary":"Read the book","date":"2020-04-27T13:22:46+02:00","references":null},{"title":"Spiking Neuron Models Reading Club","authors":null,"summary":null,"date":null,"references":null},{"title":"05.Least Angle Regression","authors":null,"summary":"Least angle regression, aka, LAR","date":"2016-09-29T00:00:00Z","references":null},{"title":"The Elements of Statistical Learning","authors":null,"summary":null,"date":null,"references":null},{"title":"04.Shrinkage Methods","authors":null,"summary":"Shrinkage methods","date":"2016-09-23T00:00:00Z","references":null},{"title":"03.Guass-Markov Theorem and Multiple Regression","authors":null,"summary":"Gauss-Markov Theorem","date":"2016-09-01T00:00:00Z","references":null},{"title":"02.Linear Methods for Regression","authors":null,"summary":"Linear regression, least squares","date":"2016-08-18T00:00:00Z","references":null},{"title":"01.Introductions (Review) and Several Preliminary Statistical Methods","authors":null,"summary":"Some basics of statistical learning; least squares and k nearest neighbors; statistical decision theory; local methods in high dimensions","date":"2016-07-06T00:00:00Z","references":null},{"title":"ABOUT","authors":null,"summary":null,"date":"2016-03-24T00:00:00Z","references":null},{"title":null,"authors":null,"summary":null,"date":null,"references":null},{"title":"00.Spiking Neuron Models Reading Club","authors":null,"summary":"Introduction to reading club of spiking neuron models, schedule, and notice","date":"2016-03-18T00:00:00Z","references":null},{"title":"00.The Elements of Statistical Learning Reading Club","authors":null,"summary":"Introduction to reading club of The Elements of Statistical Learning, schedule, and notice","date":"2016-03-18T00:00:00Z","references":null},{"title":null,"authors":null,"summary":null,"date":null,"references":null},{"title":null,"authors":null,"summary":null,"date":null,"references":null},{"title":"cpe/21.gnn-basics.md","authors":null,"summary":null,"date":null,"references":null},{"title":"cpe/22.gnn-basics-2.md","authors":null,"summary":null,"date":null,"references":null},{"title":"cpe/23.gnn.md","authors":null,"summary":null,"date":null,"references":null},{"title":"cpe/24.gnn-pytorch.md","authors":null,"summary":null,"date":null,"references":null},{"title":"cpe/25.gnn-2.md","authors":null,"summary":null,"date":null,"references":null},{"title":"cpe/26.gnn-3.md","authors":null,"summary":null,"date":null,"references":null},{"title":"cpe/28.gnn-time-series-forecasting.md","authors":null,"summary":null,"date":null,"references":null},{"title":null,"authors":null,"summary":null,"date":null,"references":null}]}