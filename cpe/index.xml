<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:project="http://neuronstar.github.io/projects"><channel><title>Conditional Probability Estimation on NeuronStar</title><link>https://neuronstar.github.io/cpe/</link><description>Recent content in Conditional Probability Estimation on NeuronStar</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Sun, 03 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://neuronstar.github.io/cpe/index.xml" rel="self" type="application/rss+xml"/><item><title>Graph Neural Networks: Basics (2)</title><link>https://neuronstar.github.io/cpe/22.gnn-basics-2/</link><pubDate>Sun, 03 Oct 2021 00:00:00 +0000</pubDate><project:when_start>2021-10-09T14:30:00</project:when_start><project:when_end>2021-10-09T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/22.gnn-basics-2/</guid><description>(<time>2021-10-09T14:30:00</time> -<time>2021-10-09T16:00:00</time>)&lt;br/> We will continue the discussion on Graph Neural Networks.
Problems of using Graphs Graph Neural Networks Textbook: Hamilton1
Hamilton2020 Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046 &amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Graph Neural Networks: Basics</title><link>https://neuronstar.github.io/cpe/21.gnn-basics/</link><pubDate>Sat, 11 Sep 2021 00:00:00 +0000</pubDate><project:when_start>2021-09-25T14:30:00</project:when_start><project:when_end>2021-09-25T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/21.gnn-basics/</guid><description>(<time>2021-09-25T14:30:00</time> -<time>2021-09-25T16:00:00</time>)&lt;br/> This will be the beginning of a new topic: Graph Neural Networks. In this new series, we will use the textbook by Hamilton1. For the first episode, we will discuss some basics about graphs to make sure we are all on the same page.
@Steven will lead the discussion.
Hamilton2020 Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046 &amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Self-supervised Learning: Theories (Part 2)</title><link>https://neuronstar.github.io/cpe/20.self-supervised-learning-theories-2/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><project:when_start>2021-09-11T14:30:00</project:when_start><project:when_end>2021-09-11T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/20.self-supervised-learning-theories-2/</guid><description>(<time>2021-09-11T14:30:00</time> -<time>2021-09-11T16:00:00</time>)&lt;br/> We will dive deep into Section 6 of the paper arXiv:2006.08218. Here are a few topics to be explored.
InfoGAN objective; Positive and negative samples in loss function (InfoNCE); Uniformity in constrastive loss; JS-divergence.</description></item><item><title>Self-supervised Learning: Theories (Part 1)</title><link>https://neuronstar.github.io/cpe/19.self-supervised-learning-theories-1/</link><pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate><project:when_start>2021-08-28T14:30:00</project:when_start><project:when_end>2021-08-28T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/19.self-supervised-learning-theories-1/</guid><description>(<time>2021-08-28T14:30:00</time> -<time>2021-08-28T16:00:00</time>)&lt;br/> We will discuss Section 6 of the paper arXiv:2006.08218.</description></item><item><title>Self-supervised Learning: GAN</title><link>https://neuronstar.github.io/cpe/18.self-supervised-learning-gan/</link><pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate><project:when_start>2021-08-14T14:30:00</project:when_start><project:when_end>2021-08-14T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/18.self-supervised-learning-gan/</guid><description>(<time>2021-08-14T14:30:00</time> -<time>2021-08-14T16:00:00</time>)&lt;br/> We will discuss the reset of the paper arXiv:2006.08218.</description></item><item><title>Self-supervised Learning: Generative or Contrastive</title><link>https://neuronstar.github.io/cpe/17.self-supervised-learning/</link><pubDate>Sat, 03 Jul 2021 00:00:00 +0000</pubDate><project:when_start>2021-07-31T14:30:00</project:when_start><project:when_end>2021-07-31T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/17.self-supervised-learning/</guid><description>(<time>2021-07-31T14:30:00</time> -<time>2021-07-31T16:00:00</time>)&lt;br/> Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218
We have discussed many topics of generative models. This paper serves as a summary of the current season of the discussions.</description></item><item><title>LTD/LTP</title><link>https://neuronstar.github.io/cpe/16.ltd-ltp/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><project:when_start>2021-07-17T14:30:00</project:when_start><project:when_end>2021-07-17T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/16.ltd-ltp/</guid><description>(<time>2021-07-17T14:30:00</time> -<time>2021-07-17T16:00:00</time>)&lt;br/> In this meetup, we will discuss some key ideas related to biological neural network: LTP and LTD.</description></item><item><title>Predictive Coding Approximates Backprop along Arbitrary Computation Graphs</title><link>https://neuronstar.github.io/cpe/15.predictive-coding/</link><pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate><project:when_start>2021-07-03T14:30:00</project:when_start><project:when_end>2021-07-03T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/15.predictive-coding/</guid><description>(<time>2021-07-03T14:30:00</time> -<time>2021-07-03T16:00:00</time>)&lt;br/> In this meetup, we will discuss this paper: https://arxiv.org/abs/2006.04182
Why? Feedforward-backprop usually has a loss function that involves all the parameters. Backprop means we need this huge global loss $\mathcal L({w_{ij}})$. However, it is hard to imaging such global loss calculations in our brain. One of the alternatives is predictive coding, which only utilizes local connection information.
In this paper (2006.04182), the author proves the equivalence of backprop and predictive coding on arbitary graph.</description></item><item><title>Energy-based Models 5</title><link>https://neuronstar.github.io/cpe/14.energy-based-learning-5/</link><pubDate>Wed, 02 Jun 2021 00:00:00 +0000</pubDate><project:when_start>2021-06-19T14:30:00</project:when_start><project:when_end>2021-06-19T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/14.energy-based-learning-5/</guid><description>(<time>2021-06-19T14:30:00</time> -<time>2021-06-19T16:00:00</time>)&lt;br/> In this meetup, we will discuss Restricted Boltzmann Machine (RBM). We will cover the reason for introducing RBM and the training. At the end of the discussion, we will also cover some topics of Deep Boltzmann Machines.</description></item><item><title>Energy-based Models 4</title><link>https://neuronstar.github.io/cpe/13.energy-based-learning-4/</link><pubDate>Wed, 26 May 2021 00:00:00 +0000</pubDate><project:when_start>2021-05-28T14:30:00</project:when_start><project:when_end>2021-05-28T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/13.energy-based-learning-4/</guid><description>(<time>2021-05-28T14:30:00</time> -<time>2021-05-28T16:00:00</time>)&lt;br/> In this discussion, we will discuss the Pytorch Deep Learning Lectures by LeCun.</description></item><item><title>Energy-based Models 3</title><link>https://neuronstar.github.io/cpe/12.energy-based-learning-3/</link><pubDate>Sat, 24 Apr 2021 00:00:00 +0000</pubDate><project:when_start>2021-05-15T14:30:00</project:when_start><project:when_end>2021-05-15T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/12.energy-based-learning-3/</guid><description>(<time>2021-05-15T14:30:00</time> -<time>2021-05-15T16:00:00</time>)&lt;br/> In the past two meetups, we have been discussing EBM from a computer scientist&amp;rsquo;s perspective.
In this discussion, we will discuss chapter XV of Mehta P, Bukov M, Wang C-HH, Day AGRR, Richardson C, Fisher CK, et al. A high-bias, low-variance introduction to Machine Learning for physicists. Phys Rep. 2018;810: 122. doi:10.1016/j.physrep.2019.03.001</description></item><item><title>Energy-based Models 2</title><link>https://neuronstar.github.io/cpe/11.energy-based-learning-2/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><project:when_start>2021-04-24T14:30:00</project:when_start><project:when_end>2021-04-24T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/11.energy-based-learning-2/</guid><description>(<time>2021-04-24T14:30:00</time> -<time>2021-04-24T16:00:00</time>)&lt;br/> We will discuss energy-based learning in this session.
References:
Lecture notes: https://atcold.github.io/pytorch-Deep-Learning/[cid:90ae645c-415b-4c9f-8ea6-c78839a8e8d4] https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/ https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2/ https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view : if you can not access Google Drive, this file (007-ebm-01) has been attached to this calendar event too. Supplementary:
https://arxiv.org/pdf/1803.08823.pdf</description></item><item><title>Energy-based Models</title><link>https://neuronstar.github.io/cpe/10.energy-based-learning/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><project:when_start>2021-04-10T13:30:00</project:when_start><project:when_end>2021-04-10T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/10.energy-based-learning/</guid><description>(<time>2021-04-10T13:30:00</time> -<time>2021-04-10T16:00:00</time>)&lt;br/> We will discuss energy-based learning in this session.
References:
Lecture notes: https://atcold.github.io/pytorch-Deep-Learning/[cid:90ae645c-415b-4c9f-8ea6-c78839a8e8d4] https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/ https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-2/ https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view : if you can not access Google Drive, this file (007-ebm-01) has been attached to this calendar event too. Supplementary:
https://arxiv.org/pdf/1803.08823.pdf</description></item><item><title>Summary of Generative Models</title><link>https://neuronstar.github.io/cpe/09.summary-of-generative-models/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><project:when_start>2021-03-27T13:30:00</project:when_start><project:when_end>2021-03-27T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/09.summary-of-generative-models/</guid><description>(<time>2021-03-27T13:30:00</time> -<time>2021-03-27T16:00:00</time>)&lt;br/></description></item><item><title>MAF: how is MADE being used</title><link>https://neuronstar.github.io/cpe/08.maf/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><project:when_start>2021-03-13T13:30:00</project:when_start><project:when_end>2021-03-13T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/08.maf/</guid><description>(<time>2021-03-13T13:30:00</time> -<time>2021-03-13T16:00:00</time>)&lt;br/> We discussed MAF (arXiv:1705.07057v4) last time: The paper did not explain how exactly is MADE being used to update the shift and logscale.
We will use the tensorflow implementation of MAF to probe the above question. Here is the link to the relevant documentation: https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow
Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>MADE: Masked Autoencoder for Distribution Estimation</title><link>https://neuronstar.github.io/cpe/07.made/</link><pubDate>Sat, 27 Feb 2021 00:00:00 +0000</pubDate><project:when_start>2021-02-27T13:30:00</project:when_start><project:when_end>2021-02-27T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/07.made/</guid><description>(<time>2021-02-27T13:30:00</time> -<time>2021-02-27T16:00:00</time>)&lt;br/> Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>Deep AutoRegressive Networks</title><link>https://neuronstar.github.io/cpe/06.deep-autoregressive-networks/</link><pubDate>Sat, 13 Feb 2021 00:00:00 +0000</pubDate><project:when_start>2021-02-13T13:30:00</project:when_start><project:when_end>2021-02-13T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/06.deep-autoregressive-networks/</guid><description>(<time>2021-02-13T13:30:00</time> -<time>2021-02-13T16:00:00</time>)&lt;br/> Topics Refer to references.
Notes 1310.8499_notes.pdf</description></item><item><title>Review of Normalizing Flow</title><link>https://neuronstar.github.io/cpe/05.normalizing-flow-review/</link><pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate><project:when_start>2021-01-30T13:30:00</project:when_start><project:when_end>2021-01-30T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/05.normalizing-flow-review/</guid><description>(<time>2021-01-30T13:30:00</time> -<time>2021-01-30T16:00:00</time>)&lt;br/> Topics Normalizing flow Applications of normalizing flow Methods of normalizing flow Problems of normalizing flow</description></item><item><title>Variantional Inference Normalizing Flow</title><link>https://neuronstar.github.io/cpe/04.variational-inference-normalizing-flow/</link><pubDate>Sat, 16 Jan 2021 00:00:00 +0000</pubDate><project:when_start>2021-01-16T14:30:00</project:when_start><project:when_end>2021-01-16T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/04.variational-inference-normalizing-flow/</guid><description>(<time>2021-01-16T14:30:00</time> -<time>2021-01-16T16:00:00</time>)&lt;br/> Topics Variational Inference Normalizing Flow Variational Inference with Normalizing Flows</description></item><item><title>EM Methods</title><link>https://neuronstar.github.io/cpe/03.em-methods/</link><pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate><project:when_start>2021-01-02T14:30:00</project:when_start><project:when_end>2021-01-02T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/03.em-methods/</guid><description>(<time>2021-01-02T14:30:00</time> -<time>2021-01-02T16:00:00</time>)&lt;br/> Topics EM for Gaussian mixtures General EM algorithm Why does it work? Decomposition of log-likelihood into KL divergence and Relation between EM and Gibbs sampling</description></item><item><title>References for Probability Estimation Club</title><link>https://neuronstar.github.io/cpe/00.references/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><guid>https://neuronstar.github.io/cpe/00.references/</guid><description>( -
)&lt;br/> A list of references for our online discussions.</description></item><item><title>Least Squares, Bootstrap, Maximum Likelihood, and Bayesian</title><link>https://neuronstar.github.io/cpe/02.least-squares-bootstrap-maximum-likelihood-and-bayesian/</link><pubDate>Sat, 12 Dec 2020 00:00:00 +0000</pubDate><project:when_start>2020-12-12T14:30:00</project:when_start><project:when_end>2020-12-12T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/02.least-squares-bootstrap-maximum-likelihood-and-bayesian/</guid><description>(<time>2020-12-12T14:30:00</time> -<time>2020-12-12T16:00:00</time>)&lt;br/> Least squares, bootstrap, maximum likelihood, and maximum posterior leads to the same results in many cases.</description></item><item><title>Conditional Probability and Bayes</title><link>https://neuronstar.github.io/cpe/01.conditional-probability-and-bayes/</link><pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate><project:when_start>2020-11-28T14:30:00</project:when_start><project:when_end>2020-11-28T16:00:00</project:when_end><guid>https://neuronstar.github.io/cpe/01.conditional-probability-and-bayes/</guid><description>(<time>2020-11-28T14:30:00</time> -<time>2020-11-28T16:00:00</time>)&lt;br/> The Bayesian view of probability is quite objective and also more general than the frequentist&amp;rsquo;s view. It doesn&amp;rsquo;t rely on repeatition of events.</description></item></channel></rss>