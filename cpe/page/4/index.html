<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.68.3"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>Conditional Probability Estimation | NeuronStar | NeuronStar</title><meta name=description content="An online journal club on the topics Conditional Probability Estimation. Our cover topics on all sorts of probabilistic approach, such as VAE, normalizing, graph neural network, probabilistic time series forecasting."><meta name=robots content="noindex"><meta name=author content="NeuronStar"><meta property="og:title" content="Conditional Probability Estimation"><meta property="og:description" content="An online journal club on the topics Conditional Probability Estimation. Our cover topics on all sorts of probabilistic approach, such as VAE, normalizing, graph neural network, probabilistic time series forecasting."><meta property="og:type" content="website"><meta property="og:url" content="https://neuronstar.github.io/cpe/"><meta property="og:updated_time" content="2023-09-24T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-61051776-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link href=https://neuronstar.github.io/cpe/index.xml rel=alternate type=application/rss+xml title=NeuronStar><link rel=canonical href=https://neuronstar.github.io/cpe/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=https://neuronstar.github.io/css/bundle.css><link rel=stylesheet href=https://unpkg.com/bulmaswatch/united/bulmaswatch.min.css></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/images/logos/logo.png alt=NeuronStar height=28 style=margin-right:.5em> NeuronStar</a><div class="navbar-burger burger" style=color:#000 data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Groups</a><div class=navbar-dropdown><a href=https://neuronstar.github.io/cpe/ class=navbar-item>Conditional Probability Estimation</a>
<a href=https://neuronstar.github.io/snm/ class=navbar-item>Spiking Neuron Models Reading Club</a>
<a href=https://neuronstar.github.io/esl/ class=navbar-item>The Elements of Statistical Learning</a></div></div><div class="navbar-item has-dropdown is-hoverable"></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item href=/about/>About</a>
<a class=navbar-item href=https://dl.leima.is/>Forecasting with Deep Learning</a>
<a class=navbar-item href=https://github.com/orgs/neuronstar/discussions/categories/papers-please>Propose New Papers</a>
<a class=navbar-item href=https://github.com/orgs/neuronstar/discussions>Community</a>
<a class=navbar-item target=blank href=https://github.com/neuronstar><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);console.log("target",target)
console.log("$target",$target)
el.classList.toggle("is-active");console.log("active el",el)
console.log("active $target",$target)});});}});</script></header><main><section><div class="hero is-primary is-medium"><div class=hero-body><div class="container has-text-left"><h1 class="title is-1">Conditional Probability Estimation</h1><h2 class="subtitle is-4">An online journal club on the topics Conditional Probability Estimation. Our cover topics on all sorts of probabilistic approach, such as VAE, normalizing, graph neural network, probabilistic time series forecasting.</h2><h3 class="title is-5" style=margin-top:2em><a href=https://neuronstar.github.io/projects/conditional-probability-estimation/ class=has-text-warning><i class="fas fa-link"></i>Introduction: Conditional Probability Estimation</a></h3></div></div></div></section><div class="columns is-fullheight"><div class=column><section class="hero is-default is-bold"><div class=hero-body><div class=container><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=https://neuronstar.github.io/>NeuronStar</a></li><li class=active><a href=https://neuronstar.github.io/cpe/>Conditional Probability Estimation</a></li></ul></nav><div class="columns is-multiline is-variable is-1-mobile is-0-tablet is-3-desktop is-8-widescreen is-2-fullhd is-desktop"><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>24</sup> <a href=https://neuronstar.github.io/cpe/24.gnn-pytorch/ itemprop=headline>Graph Neural Networks: PyTorch</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-11-02T00:00:00+00:00>2021-11-02</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/graph style=margin-right:.5em><span class="tag is-warning is-small is-light">#Graph</span></a>
<a href=/tags/pytorch style=margin-right:.5em><span class="tag is-warning is-small is-light">#PyTorch</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html>Tutorial 7: Graph Neural Networks — UvA DL Notebooks v1.1 documentation. [cited 2 Nov 2021]. Available: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046>Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: PyTorch tutorials on GNN</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>23</sup> <a href=https://neuronstar.github.io/cpe/23.gnn/ itemprop=headline>Graph Neural Networks</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-10-19T00:00:00+00:00>2021-10-19</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/graph style=margin-right:.5em><span class="tag is-warning is-small is-light">#Graph</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046>Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Chapter 5 of Hamilton1.
Hamilton2020 Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046 &#8617;&#xfe0e;</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>22</sup> <a href=https://neuronstar.github.io/cpe/22.gnn-basics-2/ itemprop=headline>Graph Neural Networks: Basics (2)</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-10-03T00:00:00+00:00>2021-10-03</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/graph style=margin-right:.5em><span class="tag is-warning is-small is-light">#Graph</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046>Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: We will continue the discussion on Graph Neural Networks.
Problems of using Graphs Graph Neural Networks Textbook: Hamilton1
Hamilton2020 Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046 &#8617;&#xfe0e;</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>21</sup> <a href=https://neuronstar.github.io/cpe/21.gnn-basics/ itemprop=headline>Graph Neural Networks: Basics</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-09-11T00:00:00+00:00>2021-09-11</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/graph style=margin-right:.5em><span class="tag is-warning is-small is-light">#Graph</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://www.morganclaypool.com/doi/10.2200/S01045ED1V01Y202009AIM046>Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: This will be the beginning of a new topic: Graph Neural Networks. In this new series, we will use the textbook by Hamilton1. For the first episode, we will discuss some basics about graphs to make sure we are all on the same page.
@Steven will lead the discussion.
Hamilton2020 Hamilton WL. Graph representation learning. Synth lect artif intell mach learn. 2020;14: 1–159. doi:10.2200/s01045ed1v01y202009aim046 &#8617;&#xfe0e;</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>20</sup> <a href=https://neuronstar.github.io/cpe/20.self-supervised-learning-theories-2/ itemprop=headline>Self-supervised Learning: Theories (Part 2)</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-08-26T00:00:00+00:00>2021-08-26</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/self-supervised-learning style=margin-right:.5em><span class="tag is-warning is-small is-light">#Self-supervised Learning</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/2006.08218>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/2005.10242>Wang T, Isola P. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2005.10242</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/2003.14323>Newell A, Deng J. How Useful is Self-Supervised Pretraining for Visual Tasks? arXiv [cs.CV]. 2020. Available: http://arxiv.org/abs/2003.14323</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/1907.13625>Tschannen M, Djolonga J, Rubenstein PK, Gelly S, Lucic M. On Mutual Information Maximization for Representation Learning. arXiv [cs.LG]. 2019. Available: http://arxiv.org/abs/1907.13625</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/1807.03748>van den Oord A, Li Y, Vinyals O. Representation learning with Contrastive Predictive Coding. arXiv [cs.LG]. 2018. Available: http://arxiv.org/abs/1807.03748</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/1606.03657>Chen X, Duan Y, Houthooft R, Schulman J, Sutskever I, Abbeel P. InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1606.03657</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/1606.00709>Nowozin S, Cseke B, Tomioka R. f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. arXiv [stat.ML]. 2016. Available: http://arxiv.org/abs/1606.00709</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Theories of self-supervised learning</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>19</sup> <a href=https://neuronstar.github.io/cpe/19.self-supervised-learning-theories-1/ itemprop=headline>Self-supervised Learning: Theories (Part 1)</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-08-26T00:00:00+00:00>2021-08-26</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/self-supervised-learning style=margin-right:.5em><span class="tag is-warning is-small is-light">#Self-supervised Learning</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/2006.08218>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Theories of self-supervised learning</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>18</sup> <a href=https://neuronstar.github.io/cpe/18.self-supervised-learning-gan/ itemprop=headline>Self-supervised Learning: GAN</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-08-01T00:00:00+00:00>2021-08-01</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/self-supervised-learning style=margin-right:.5em><span class="tag is-warning is-small is-light">#Self-supervised Learning</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/1406.2661>Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, et al. Generative Adversarial Networks. arXiv [stat.ML]. 2014. Available: http://arxiv.org/abs/1406.2661</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/1411.1784>Mirza M, Osindero S. Conditional Generative Adversarial Nets. arXiv [cs.LG]. 2014. Available: http://arxiv.org/abs/1411.1784</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/1701.00160>Goodfellow I. NIPS 2016 Tutorial: Generative Adversarial Networks. arXiv [cs.LG]. 2016. Available: http://arxiv.org/abs/1701.00160</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://deeplizard.com/lesson/gaa1ilrazd>GAN Course Introduction - Intuitive Intro To Generative Adversarial Networks. [cited 1 Aug 2021]. Available: https://deeplizard.com/lesson/gaa1ilrazd</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/2006.08218>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Let's talk about GAN this time.</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>17</sup> <a href=https://neuronstar.github.io/cpe/17.self-supervised-learning/ itemprop=headline>Self-supervised Learning: Generative or Contrastive</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-07-03T00:00:00+00:00>2021-07-03</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/self-supervised-learning style=margin-right:.5em><span class="tag is-warning is-small is-light">#Self-supervised Learning</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=http://arxiv.org/abs/2006.08218>Liu X, Zhang F, Hou Z, Wang Z, Mian L, Zhang J, et al. Self-supervised Learning: Generative or Contrastive. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.08218</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Some biological neural network knowledge</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>16</sup> <a href=https://neuronstar.github.io/cpe/16.ltd-ltp/ itemprop=headline>LTD/LTP</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-06-21T00:00:00+00:00>2021-06-21</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/biological-neural-network style=margin-right:.5em><span class="tag is-warning is-small is-light">#Biological Neural Network</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://www.nature.com/articles/s41539-019-0048-y>Abraham WC, Jones OD, Glanzman DL. Is plasticity of synapses the mechanism of long-term memory storage? NPJ Sci Learn. 2019;4: 9. doi:10.1038/s41539-019-0048-y</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: Some biological neural network knowledge</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>15</sup> <a href=https://neuronstar.github.io/cpe/15.predictive-coding/ itemprop=headline>Predictive Coding Approximates Backprop along Arbitrary Computation Graphs</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-06-21T00:00:00+00:00>2021-06-21</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/biological-neural-network style=margin-right:.5em><span class="tag is-warning is-small is-light">#Biological Neural Network</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://arxiv.org/abs/2006.04182>Millidge B, Tschantz A, Buckley CL. Predictive Coding Approximates Backprop along Arbitrary Computation Graphs. arXiv [cs.LG]. 2020. Available: http://arxiv.org/abs/2006.04182</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: In this meetup, we will discuss this paper: https://arxiv.org/abs/2006.04182
Why? Feedforward-backprop usually has a loss function that involves all the parameters. Backprop means we need this huge global loss $\mathcal L({w_{ij}})$. However, it is hard to imaging such global loss calculations in our brain. One of the alternatives is predictive coding, which only utilizes local connection information.
In this paper (2006.04182), the author proves the equivalence of backprop and predictive coding on arbitary graph.</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>14</sup> <a href=https://neuronstar.github.io/cpe/14.energy-based-learning-5/ itemprop=headline>Energy-based Models 5</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-06-02T00:00:00+00:00>2021-06-02</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/energy-based-learning style=margin-right:.5em><span class="tag is-warning is-small is-light">#Energy-based Learning</span></a>
<a href=/tags/principle-of-max-entropy style=margin-right:.5em><span class="tag is-warning is-small is-light">#Principle of Max Entropy</span></a>
<a href=/tags/boltzmann-machine style=margin-right:.5em><span class="tag is-warning is-small is-light">#Boltzmann Machine</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/>Pytorch Deep Learning Lectures</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view>Pytorch Deep Learning Slides</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://arxiv.org/abs/1803.08823>A high-bias, low-variance introduction to Machine Learning for physicists</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: We will discuss RBM and training in this session.</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div><div class="column is-half is-multiline" itemscope itemtype=http://schema.org/CreativeWork><h1 class=blog-timestamp></h1><h1 class=title><sup>13</sup> <a href=https://neuronstar.github.io/cpe/13.energy-based-learning-4/ itemprop=headline>Energy-based Models 4</a></h1><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-calendar-alt"></i>Published: <time datetime=2021-05-26T00:00:00+00:00>2021-05-26</time></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-hashtag" aria-hidden=true></i>Tags:<div class="tags is-small" style=display:inline><a href=/tags/energy-based-learning style=margin-right:.5em><span class="tag is-warning is-small is-light">#Energy-based Learning</span></a>
<a href=/tags/principle-of-max-entropy style=margin-right:.5em><span class="tag is-warning is-small is-light">#Principle of Max Entropy</span></a>
<a href=/tags/boltzmann-machine style=margin-right:.5em><span class="tag is-warning is-small is-light">#Boltzmann Machine</span></a></div></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="fa fa-paperclip" aria-hidden=true></i>References:</span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://atcold.github.io/pytorch-Deep-Learning/en/week07/07-1/>Pytorch Deep Learning Lectures</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://drive.google.com/file/d/1z8Dz1YtkOEJpU-gh5RIjORs3GGqkYJQa/view>Pytorch Deep Learning Slides</a></span>
<span style=display:block;font-size:90%;padding-left:1em>- <a href=https://arxiv.org/abs/1803.08823>A high-bias, low-variance introduction to Machine Learning for physicists</a></span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-star"></i>Summary: We will discuss energy-based learning in this session.</span></div><div style=margin-left:2em><span style=display:block;font-size:90%><i class="far fa-list-alt"></i>Pages: 60</span></div></div><div class=is-divider style="border-top:.05rem dashed #dbdbdb!important;margin:1em 0!important"></div></div></div><br><div class=container><nav class="pagination has-text-centered" role=navigation aria-label=pagination><a class="button is-primary" href=/cpe/page/3/><span class="icon is-small"><i class="fa fa-angle-double-left"></i></span><span>Previous</span></a>
<a class="button is-outlined is-primary">Page 4 of 5</a>
<a class="button is-primary" href=/cpe/page/5/><span class="icon is-small"><i class="fa fa-angle-double-right"></i></span><span>Next</span></a></nav></div></div></section></div></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=https://neuronstar.github.io/>NeuronStar</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer></footer><script async type=text/javascript src=/js/bulma.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/js/all.min.js integrity="sha512-yFjZbTYRCJodnuyGlsKamNE/LlEaEAxSUDe5+u61mV8zzqJVFOH7TnULE2/PP/l5vKWpUNnF4VGVkXh3MjgLsg==" crossorigin=anonymous referrerpolicy=no-referrer></script><script type=application/json class=js-hypothesis-config>{"openSidebar":false,"theme":"clean","enableExperimentalNewNoteButton":true,"showHighlights":true}</script></body></html>