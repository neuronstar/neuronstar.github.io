<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.68.3"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><title>01.Introductions (Review) and Several Preliminary Statistical Methods | NeuronStar | NeuronStar</title><meta name=author content="OctoMiao"><meta property="og:title" content="01.Introductions (Review) and Several Preliminary Statistical Methods"><meta property="og:description" content="Some basics of statistical learning; least squares and k nearest neighbors; statistical decision theory; local methods in high dimensions"><meta property="og:type" content="article"><meta property="og:url" content="/esl/01.statistical-learning-theory/"><meta property="article:published_time" content="2016-07-06T00:00:00+00:00"><meta property="article:modified_time" content="2016-07-06T00:00:00+00:00"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-61051776-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><link rel=canonical href=/esl/01.statistical-learning-theory/><link rel="shortcut icon" type=image/png href=/logos/logo-square.png><link rel=stylesheet href=/css/bulma.css><link rel=stylesheet href=/css/bulma-divider.min.css><link rel=stylesheet href=/assets/css/bulma-ribbon.min.css><link rel=stylesheet href=/assets/css/tooltip.css><link rel=stylesheet href=https://jenil.github.io/bulmaswatch/united/bulmaswatch.min.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/css/blog-post.css><link rel=stylesheet href=/css/code-highlighting/dark.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=https://unpkg.com/applause-button/dist/applause-button.css><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],tags:'ams',processEscapes:true,processEnvironments:true},options:{skipHtmlTags:['script','noscript','style','textarea','pre']},svg:{fontCache:'global'}};window.addEventListener('load',(event)=>{document.querySelectorAll("mjx-container").forEach(function(x){x.parentElement.classList+='has-jax'})});</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script>mermaid.initialize({startOnLoad:true});</script><script src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/js/all.min.js integrity="sha256-KzZiKy0DWYsnwMF+X1DvQngQ2/FxF7MF3Ff72XcpuPs=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin=anonymous></head><body><header><nav class="navbar is-transparent"><div class=navbar-brand><a class=navbar-item href=/><img src=/images/logos/logo.png alt=NeuronStar height=28 style=margin-right:.5em> NeuronStar</a><div class="navbar-burger burger" style=color:#000 data-target=navMenu><span></span><span></span><span></span></div></div><div class=navbar-menu id=navMenu><div class=navbar-start><div class="navbar-item has-dropdown is-hoverable"><a href=/projects class=navbar-link>Groups</a><div class=navbar-dropdown><a href=/cpe/ class=navbar-item>Conditional Probability Estimation</a>
<a href=/snm/ class=navbar-item>Spiking Neuron Models Reading Club</a>
<a href=/esl/ class=navbar-item>The Elements of Statistical Learning</a></div></div></div><span class=navbar-burger><span></span><span></span><span></span></span><div class=navbar-end><div class=navbar-item><a class=navbar-item target=blank href=https://github.com/neuronstar><span class=icon><i class="fab fa-github"></i></span></a></div></div></div></nav><script>document.addEventListener('DOMContentLoaded',()=>{const $navbarBurgers=Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'),0);if($navbarBurgers.length>0){$navbarBurgers.forEach(el=>{el.addEventListener('click',()=>{const target=el.dataset.target;const $target=document.getElementById(target);el.classList.toggle('is-active');$target.classList.toggle('is-active');});});}});</script></header><main><div class=container itemscope itemtype=http://schema.org/BlogPosting><meta itemprop=name content="01.Introductions (Review) and Several Preliminary Statistical Methods"><meta itemprop=description content="Some basics of statistical learning; least squares and k nearest neighbors; statistical decision theory; local methods in high dimensions"><meta itemprop=datePublished content="2016-07-06T00:00:00+00:00"><meta itemprop=dateModified content="2016-07-06T00:00:00+00:00"><meta itemprop=wordCount content="818"><meta itemprop=keywords content><section class=section><div class=container><article class=post><header class=post-header><nav class="breadcrumb has-succeeds-separator is-small" aria-label=breadcrumbs><ul><li><a href=/>NeuronStar</a></li><li><a href=/esl/>The Elements of Statistical Learning</a></li><li class=active><a href=/esl/01.statistical-learning-theory/>01.Introductions (Review) and Several Preliminary Statistical Methods</a></li></ul></nav><h1 class="post-title has-text-centered is-size-1" itemprop="name headline">01.Introductions (Review) and Several Preliminary Statistical Methods</h1><h2 class="title is-6 has-text-centered"><i class="fas fa-tags" style=margin-right:.5em></i></h2></header><div class=columns><div class="column is-8"><div class=is-divider data-content=SUMMARY></div><div class="notification is-light"><p>Some basics of statistical learning; least squares and k nearest neighbors; statistical decision theory; local methods in high dimensions</p></div><div class=is-divider data-content=ARTICLE></div><div class="content blog-post section" itemprop=articleBody><h2 id=review>Review</h2><p><strong>Skeleton notes</strong></p><h3 id=abbreviations>Abbreviations</h3><ol><li>MSE: mean squared error</li><li>EPE: expected prediction error</li><li>RSS: sum of squares</li></ol><h3 id=notations>Notations</h3><p>Fonts:</p><ol><li>Vectors or scalars are denoted by italic math font $X$.</li><li>Components of vectors are denoted by subscripts $X_i$.</li><li>Matrix is denoted by math bold font $\mathbf X$.</li></ol><p>Symbols</p><ol><li>$X$ for input variables;</li><li>$Y$ for quantitative output;</li><li>$G$ for qualitative output;</li><li>$\hat {}$ for prediction.</li></ol><h3 id=least-squares-and-nearest-neighbors>Least Squares and Nearest Neighbors</h3><h4 id=least-squares>Least Squares</h4><p>Least square model:</p><p>$$
\hat Y = X^T \hat \beta.
$$</p><p>Residual sum of squares (RSS):</p><p>$$
\mathrm{RSS}(\beta) = (\mathbf y - \mathbf X \beta)^{\mathrm T} (\mathbf y - \mathbf X \beta).
$$</p><p>The parameters we need is the set that minimizes RSS, which requires</p><p>$$
\frac{d}{d\beta} \mathrm{RSS} = 0.
$$</p><p>So we can solve the parameters easily.</p><h4 id=nearest-neighbor>Nearest-Neighbor</h4><ol><li>For input data $x$, calculate the Euclidean distance between $x$ and other input data $x_j$.</li><li>Choose the $k$ nearest neighbors based on the distance.</li><li>Output prediction is determined by average of the corresponding outputs of the selected inputs.
$$
\hat Y = \frac{1}{k} \sum_{N_k} y_i.
$$</li></ol><p>For the calculation of distance, metric must be implemented. The book used examples of Euclidean metric. Another metric that can be inspiring is the hyperbolic space. I talked about this in <a href=https://reading-club.github.io/5weekplus/week2.html>our reading club</a>.
{: .notes&ndash;info}</p><h4 id=for-which-scenario>For Which Scenario</h4><ol><li>Least squares: Gaussian-like data set;</li><li>Nearest-Neighbor: mixture of Gaussians.</li></ol><p>Mixture of Gaussians can be described by generative model. I am not really sure what that is. It seems to me that the final data is basically generated from Gaussians of different parameters which are generated randomly.
{: .notes&ndash;warning}</p><h2 id=statistical-decision-theory>Statistical Decision Theory</h2><ol><li>Given input $X$ and output $Y$;</li><li>Following a joint distribution $\mathrm{Pr}(X,Y)$;</li><li>Based on input and output, we look for a function that predicts the behavior, i.e., $\hat Y = f(X)$;</li><li>How well the prediction is is defined by squared error loss $L(Y,\hat Y) = (Y-\hat Y)^2$.</li><li>With the distribution, we predict the expected prediction error (EPE) as
$$
\mathrm{EPE}(f) = E[ ( Y- \hat Y )^2 ] = \int (y - f(x))^2 \mathrm{Pr}(dx, dy).
$$</li><li>The book derived that the best prediction is $f(x) = E(Y\vert X=x)$.</li><li>Different loss functions lead to different EPE&rsquo;s.</li></ol><p>Question: Can we simply solve the probability distribution and find out the function of prediction? The conclusion says the best prediction of $Y$ is the conditional mean. Is it effectively solving $Y$ from the probability distribution?
{: .notes&ndash;warning}</p><h3 id=nearest-neighbor-1>Nearest-Neighbor</h3><ol><li>The best prediction based on EPE is conditional mean, Eq. 2.13;</li><li>Both $k$ nearest neighbor and linear regression fits into this framework;</li><li>Additive models: basically turn the linear $x^T\beta$ into a function of $f_j(X_j)$. The summation still holds.</li><li>The best prediction based on expectation only is conditional median.</li><li>Categorical variable $G$ also follows the same paradigm but with different loss function.</li><li>A choice of loss function for categorical case is a matrix. It has to be a matrix because we have to specify penalties a given prediction class compared to the output class. The dimension of this matrix should be the number of categories. It is rank 2.</li></ol><ol><li>0 neighbor indicates an exact classification for the sample data but without the implementation of expectation values at each point since there is only one value at that point in one set of sample data;</li><li>$k$ nearest neighbor assumed that expectation around a small patch of a point is identical to expectation at the exact point with the corresponding distribution.</li><li>In Monte Carlo method, calculation of volume in high dimension converges very slowly. The reason is that we need a very large number of sampling points since the dimension is high. The procedure is multiplicative. The same thing might happen here. $k$ nearest neighbor is basically some kind of averaging procedure of the volume density. It requires a large number of sample data points to perform an fairly accurate average.</li><li>The linear regression is basically a first order Taylor expansion of the approximator $f(x)$. $f(x) = x^T\beta$.</li></ol><h2 id=local-methods-in-high-dimensions>Local Methods in High Dimensions</h2><ol><li>Curse of high dimensions: edge length of a cube of volume $r$ is $e_p(r) = r^{1/p}$. An extreme example: $(10^{-10})^{1/10} =0.1$.</li><li>Small volume leads to high variance.</li><li>Homogeneous sampling doesn&rsquo;t work in high dimensions. Since most points will fall near the edges.</li><li>Requires huge number of sample points in high dimensions.</li></ol><h2 id=statistical-models-supervised-learning-and-function-approximation>Statistical Models, Supervised Learning and Function Approximation</h2><h3 id=joint-distribution>Joint Distribution</h3><p>I didn&rsquo;t not get the point of this subsection. It seems that the authors are talking about whether it is proper to assume the relation between input and output is deterministic.
{: .notes&ndash;warning}</p><h3 id=supervised-learning>Supervised Learning</h3><ol><li>learn by example.</li></ol><h3 id=function-approximation>Function Approximation</h3><ol><li>Linear model;</li><li>Function as basis (Eq. 2.30): $f_\theta(x) = \sum h_k(x)\theta_k$.</li><li>Examples of function bases are Fourier expansions, sigmoid, etc.</li><li>Learning through minimizing sum of squares (RSS), or maximum likelihood estimation, etc.</li><li>Maximum likelihood estimation:<ol><li>Likelihood: $L(\theta) = \sum_{i=1}^N \log \mathrm{Pr}_\theta (y_i)$;</li><li>Maximized it (&ldquo;probability of the observed sample is largest&rdquo;)</li><li>Minimizing RSS is equivalent to maximum likelihood estimation. Eq. 2.35.</li></ol></li></ol></div><p><div class="has-text-right is-size-7"><span class=icon><i class="fas fa-pencil-alt"></i></span>Published: <time datetime=2016-07-06T00:00:00+00:00>2016-07-06</time>
by <span itemprop=author>OctoMiao</span>;</div></p><div class=is-divider></div><nav class="pagination is-centered" role=navigation aria-label=pagination><a href="/esl/02.linear-methods-for-regresssion/?ref=footer" class=pagination-previous>« 02.Linear Methods for Regression</a>
<a class=pagination-next href="/esl/00.the-elements-of-statistical-learning/?ref=footer">00.The Elements of Statistical Learning Reading... »</a></nav></div><div class="column is-4"><style>#TableOfContents>ul{list-style-type:lower-greek;padding-left:0}#TableOfContents>ul>li ul{list-style-type:none;padding-left:1em}</style><div class=is-divider data-content=ToC></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><details><summary>Table of Contents</summary><div><div><nav id=TableOfContents><ul><li><a href=#review>Review</a><ul><li><a href=#abbreviations>Abbreviations</a></li><li><a href=#notations>Notations</a></li><li><a href=#least-squares-and-nearest-neighbors>Least Squares and Nearest Neighbors</a></li></ul></li><li><a href=#statistical-decision-theory>Statistical Decision Theory</a><ul><li><a href=#nearest-neighbor-1>Nearest-Neighbor</a></li></ul></li><li><a href=#local-methods-in-high-dimensions>Local Methods in High Dimensions</a></li><li><a href=#statistical-models-supervised-learning-and-function-approximation>Statistical Models, Supervised Learning and Function Approximation</a><ul><li><a href=#joint-distribution>Joint Distribution</a></li><li><a href=#supervised-learning>Supervised Learning</a></li><li><a href=#function-approximation>Function Approximation</a></li></ul></li></ul></nav></div></div></details></div></div></article></div><script>const el=document.querySelector('details summary')
el.onclick=()=>{(function(l,o,a,d,e,r){e=o.createElement(a),r=o.getElementsByTagName(a)[0];e.async=1;e.src=d;r.parentNode.insertBefore(e,r)})(window,document,'script','/js/smoothscroll.js');el.onclick=null}
document.querySelectorAll('#TableOfContents a').forEach(link=>{link.addEventListener('click',()=>{document.querySelector(link.href.slice(link.href.indexOf('#'))).scrollIntoView({behavior:'smooth'})})})</script><div class=is-divider data-content=CONNECTUME></div><div class="box is-size-7"><article class=media><div class=media-content><div class=content><p><strong>Current Ref:</strong><br><ul><li style=list-style:none><span class="tag is-primary is-light has-text-weight-bold">esl/01.statistical-learning-theory.md</span></li></ul></p></div></div></article></div><div id=comments class=is-divider data-content=COMMENTS></div><script src=https://utteranc.es/client.js repo=neuronstar/neuronstar.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></div></div></article></div></section></div><div class=navtools><a class="button is-primary is-light is-outlined" alt="Edit this page" href=https://github.com/neurnstar/neuronstar.github.io/edit/master/content/esl/01.statistical-learning-theory.md target=blank style=position:fixed;bottom:20px;right:10px;border-radius:9999px;width:35px;height:35px><i class="fas fa-pencil-alt"></i></a><a class="button is-primary is-light is-outlined" href=#comments alt=Comments style=position:fixed;bottom:60px;right:10px;border-radius:9999px;width:35px;height:35px><i class="far fa-comments"></i></a></div></main><footer><footer class=footer><div class=container><div class="content has-text-centered"><p>Created and maintained by <a href=/>NeuronStar</a>.
Acknowledgement: <a href=https://gohugo.io/>Hugo</a>,
<a href=https://themes.gohugo.io/bulma/>Bulma</a>, <a href=https://kausalflow.com>KausalFlow</a>.
<strong>love</strong>.<br><a class=tag href=/index.xml>Feed</a>
<a class=tag href=/data.json>JSON Data</a></p></div></div></footer><script src=https://unpkg.com/applause-button/dist/applause-button.js></script></footer><script async type=text/javascript src=/js/bulma.js></script></body></html>